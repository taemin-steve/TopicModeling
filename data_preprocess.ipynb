{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Publication Type  Authors  Book Authors  Book Editors  Book Group Authors   \n",
      "0                  J      NaN           NaN           NaN                 NaN  \\\n",
      "1                  C      NaN           NaN           NaN                 NaN   \n",
      "2                  C      NaN           NaN           NaN                 NaN   \n",
      "3                  C      NaN           NaN           NaN                 NaN   \n",
      "4                  C      NaN           NaN           NaN                 NaN   \n",
      "..               ...      ...           ...           ...                 ...   \n",
      "695                C      NaN           NaN           NaN                 NaN   \n",
      "696                J      NaN           NaN           NaN                 NaN   \n",
      "697                J      NaN           NaN           NaN                 NaN   \n",
      "698                C      NaN           NaN           NaN                 NaN   \n",
      "699                C      NaN           NaN           NaN                 NaN   \n",
      "\n",
      "     Author Full Names  Book Author Full Names  Group Authors   \n",
      "0                  NaN                     NaN            NaN  \\\n",
      "1                  NaN                     NaN            NaN   \n",
      "2                  NaN                     NaN            NaN   \n",
      "3                  NaN                     NaN            NaN   \n",
      "4                  NaN                     NaN            NaN   \n",
      "..                 ...                     ...            ...   \n",
      "695                NaN                     NaN            NaN   \n",
      "696                NaN                     NaN            NaN   \n",
      "697                NaN                     NaN            NaN   \n",
      "698                NaN                     NaN            NaN   \n",
      "699                NaN                     NaN            NaN   \n",
      "\n",
      "                                         Article Title  Source Title  ...   \n",
      "0    Are Object Detection Assessment Criteria Ready...           NaN  ...  \\\n",
      "1    Computer vision signal processing on graphics ...           NaN  ...   \n",
      "2    Human-Inspired Camera: A Novel Camera System f...           NaN  ...   \n",
      "3    Increasing the Accuracy for Computer Vision Sy...           NaN  ...   \n",
      "4    Leveraging Deep Learning for Computer Vision: ...           NaN  ...   \n",
      "..                                                 ...           ...  ...   \n",
      "695  Vision based Obstacle Detection and Collision ...           NaN  ...   \n",
      "696  Guest editors' introduction to the special sec...           NaN  ...   \n",
      "697  Computer vision assisted virtual reality calib...           NaN  ...   \n",
      "698  Design and Realization of Computer Aided Instr...           NaN  ...   \n",
      "699  Research on automatic indication values of poi...           NaN  ...   \n",
      "\n",
      "     Web of Science Index  Research Areas  IDS Number  Pubmed Id   \n",
      "0                     NaN             NaN         NaN        NaN  \\\n",
      "1                     NaN             NaN         NaN        NaN   \n",
      "2                     NaN             NaN         NaN        NaN   \n",
      "3                     NaN             NaN         NaN        NaN   \n",
      "4                     NaN             NaN         NaN        NaN   \n",
      "..                    ...             ...         ...        ...   \n",
      "695                   NaN             NaN         NaN        NaN   \n",
      "696                   NaN             NaN         NaN        NaN   \n",
      "697                   NaN             NaN         NaN        NaN   \n",
      "698                   NaN             NaN         NaN        NaN   \n",
      "699                   NaN             NaN         NaN        NaN   \n",
      "\n",
      "     Open Access Designations  Highly Cited Status  Hot Paper Status   \n",
      "0                         NaN                  NaN               NaN  \\\n",
      "1                         NaN                  NaN               NaN   \n",
      "2                         NaN                  NaN               NaN   \n",
      "3                         NaN                  NaN               NaN   \n",
      "4                         NaN                  NaN               NaN   \n",
      "..                        ...                  ...               ...   \n",
      "695                       NaN                  NaN               NaN   \n",
      "696                       NaN                  NaN               NaN   \n",
      "697                       NaN                  NaN               NaN   \n",
      "698                       NaN                  NaN               NaN   \n",
      "699                       NaN                  NaN               NaN   \n",
      "\n",
      "     Date of Export  UT (Unique WOS ID)  Web of Science Record  \n",
      "0               NaN                 NaN                      0  \n",
      "1               NaN                 NaN                      0  \n",
      "2               NaN                 NaN                      0  \n",
      "3               NaN                 NaN                      0  \n",
      "4               NaN                 NaN                      0  \n",
      "..              ...                 ...                    ...  \n",
      "695             NaN                 NaN                      0  \n",
      "696             NaN                 NaN                      0  \n",
      "697             NaN                 NaN                      0  \n",
      "698             NaN                 NaN                      0  \n",
      "699             NaN                 NaN                      0  \n",
      "\n",
      "[700 rows x 72 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "df = pd.read_excel(\"savedrecs.xls\")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[[\"Article Title\",\"Abstract\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Maritime vessels equipped with visible and infrared cameras can complement other conventional sensors for object detection. However, application of computer vision techniques in maritime domain received attention only recently. The maritime environment offers its own unique requirements and challenges. Assessment of the quality of detections is a fundamental need in computer vision. However, the conventional assessment metrics suitable for usual object detection are deficient in the maritime setting. Thus, a large body of related work in computer vision appears inapplicable to the maritime setting at the first sight. We discuss the problem of defining assessment metrics suitable for maritime computer vision. We consider new bottom edge proximity metrics as assessment metrics for maritime computer vision. These metrics indicate that existing computer vision approaches are indeed promising for maritime computer vision and can play a foundational role in the emerging field of maritime computer vision.',\n",
       " 'In some sense, computer graphics and computer vision are inverses of one another. Special purpose computer vision hardware is rarely found in typical mass-produced personal computers, but graphics processing units (GPUs) found on most personal computers, often exceed (in number of transistors as well as in compute power) the capabilities of the Central Processing Unit (CPU). This paper shows speedups attained by using computer graphics hardware for implementation of computer vision algorithms by efficiently mapping mathematical operations of computer vision onto modem computer graphics architecture. As an example computer vision algorithm, we implement a real-time projective camera motion tracking routine on modern, GeForce FX class GPUs. Algorithms are implemented using OpenGL and the nVIDIA Cg fragment shaders. Trade-offs between computer vision requirements and GPU resources are discussed. Algorithm implementation is examined closely, and hardware bottlenecks are addressed to examine the performance of GPU architecture for computer vision. It is shown that significant speedups can be achieved, while leaving the CPU free for other signal processing tasks. Applications of our work include wearable, computer mediated reality systems that use both computer vision and computer graphics, and require realtime processing with low-latency and high throughput provided by modern GPUs.',\n",
       " 'Computer vision models aim to emulate biological design so that systems can perform meaningful tasks. We believe that the underlying processes of the human visual system hold the keys to further improving the performance of such computer vision solutions. This exploratory paper investigates the swaying motion of human vision when walking to develop a novel camera system. We successfully demonstrate that this design is able to improve performance in computer vision tasks, such as monocular depth estimation.',\n",
       " 'Robots rely on the computer vision systems to obtain the environmental information. As a result, the accuracy of the computer vision systems is essential for the control of the robots. Many computer vision systems make use of markers of the well-designed patterns to calculate the system parameters. Undesirably, the noise exists universally, which decreases the calibration accuracy and consequently decreases the accuracy of the computer vision systems. In this paper, we propose a pattern modeling method to remove the noise by decreasing the degree of freedom of the total calibration markers to one. The theorem is proposed and proved. The proposed method can be readily adopted by different computer vision systems, e.g. structured light based computer vision systems and stereo vision based systems.',\n",
       " 'The usability of computer vision is everywhere, whereas deep learning revolutionized the concept of artificial intelligence including computer vision. This paper discusses the leverages of deep learning for computer vision. At first, the background details of computer vision and deep learning have been discussed. Important tasks of computer vision like image classification, object localization, object detection, segmentation are briefly explained. Various types of deep learning algorithms have been described. The architecture of the convolutional neural network has been explained briefly. Applications of deep learning in different fields of computer vision like image classifications, human activity recognition, scene text detection and recognition, object tracking, visual question answering, etc., have been discussed with the references of the recent state-of-the-art works.',\n",
       " 'As the development in quantum computation is on the rise, its potential application in various research areas has been investigated, including to artificial intelligence and machine learning (AI/ML). Computer vision, the fastest growing field in the AI/ML field, has also seen potential applications of quantum computing to solve computer vision tasks. In this paper, we discuss the current approach of utilizing quantum computing to computer vision. In particular, we firstly elaborate on classical approaches to solving several computer vision tasks, then briefly describe the background of different paradigms of quantum computers, i.e., the adiabatic quantum computer (quantum annealer), and the gate model, also known as the universal quantum computer. Subsequently, we summarize the problem formulation and quantum computing-based solutions to computer vision tasks that appeared in the major AI/ML conferences in the past years. Additionally, the strengths and drawbacks of each case are also discussed, providing beneficial insights into the current state of quantum computing usage for computer vision.',\n",
       " 'Computer vision is becoming a mainstream subject of study in computer science and engineering. With the rapid explosion of multimedia and the extensive use of video and image-based communications over the World Wide Web, there is a strong demand for educating students to, become knowledgeable in computer imaging and vision. The purpose of this paper is to review the status of computer vision education today.',\n",
       " 'The rise in popularity of products and interfaces which use computer vision has not been matched by a rise in usability of frameworks which present computer vision methods to end-users, hobbyists, general developers or researchers outside the field. This position paper presents a work-in-progress set of design guidelines geared towards developer-centred interfaces in order to help provide computer vision in an intuitive and accessible manner The guidelines were developed through examination of previous work in computer vision and human-computer interaction, analysis of vision problems and inspiration from successful abstractions in other fields, and are intended as a positive reflection on the current state of computer vision interfaces. Our key guideline states that developer interfaces to computer vision must hide details regarding specific algorithms, and we discuss the implications of frameworks which support this guideline.',\n",
       " 'This paper presents three-wheeled robot with computer vision system. The robot was developed in the Saint-Petersburg Electrotechnical University in the context of the computer vision algorithms research. The main research steps were the following: low level control system executing, layout of PCB for brush less direct current motor wheels control, writing a program for robot remote access, pattern recognition and obstacles avoiding using computer vision.',\n",
       " 'Computer Vision is a cross research field with the main purpose of understanding the surrounding environment as closely as possible to human perception. In addition to research on techniques and algorithms that enhance the capabilities of a Computer Vision system, research on the architecture and design of such systems is also important. In this paper we present an End-to-End Computer Vision Framework open-source solution, based on Python programming language, that aims to support researchers in this field. The main focus of the framework is the configurability and scalability of the system in the continuous need to add new Computer Vision algorithms or machine learning models for a day-to-day research activity.',\n",
       " 'The rapid growth of automation in manufacturing industry results demands better computer vision. Hence computer vision now plays an important role in product inspection, assembly, and design in reverse engineering. In this paper we discuss briefly the importance of certain graph theory techniques for developing a method for automatic sensor placement including the optimal sensor placements for improved computer vision.',\n",
       " 'Most computer vision applications are built from a combination of basic computer vision algorithms, such as filters, descriptors and matchers. The functionality of this computer vision toolbox is well understood and solid implementations exist. One of the leading and most often used implementations is the Open Source Computer Vision Library (OpenCV), which implements more than 500 computer vision algorithms. However, most of these algorithms have multiple parameters that have to be tuned to the specific vision application. Therefore, during development of new vision applications, some human-in-the loop iteration cannot be avoided. The problem is that OpenCV lacks support for this phase of development. This usually leads to a situation where each developer creates his own static ad-hoc debug solutions that output intermediate results in a printf-debugging manner. To remedy this situation, we developed reusable general-purpose interactive tools for visual debugging and development of OpenCV-based computer vision applications.',\n",
       " \"Ripple Down Rules (RDR)'s incremental knowledge acquisition provides computer vision applications with the ability to gradually adapt to the domain and circumvent some of its learning challenges. RDR use incremental exception-based theory revision and rely on the expert to provide the rule conditions. A computer vision expert whilst understanding their significance cannot always provide accurate rule conditions using numeric attributes. This work investigates the impact of the quasi-expertise of vision experts on the structure and performance of the acquired knowledge base. The findings provide insights into the design of features and strategies to facilitate the use of quasi-expertise for knowledge acquisition in computer vision.\",\n",
       " 'nan',\n",
       " 'Edutainment robots and computer vision techniques can be effectively used in both educational and practical purposes. We proved this by implementing computer vision techniques for a commercial edutainment robot with a visual programming language. We also devised three exemplary applications which showed good results on the robot.',\n",
       " 'There is a large growth in hardware and software systems capable of producing vast amounts of image and video data. These systems are rich sources of continuous image and video streams. This motivates researchers to build scalable computer vision systems that utilize data-streaming concepts for processing of visual data streams. However, several challenges exist in building large-scale computer vision systems. For example, computer vision algorithms have different accuracy and speed profiles depending on the content, type, and speed of incoming data. Also, it is not clear how to adaptively tune these algorithms in large-scale systems. These challenges exist because we lack formal frameworks for building and optimizing large-scale visual processing. This paper presents formal methods and algorithms that aim to overcome these challenges and improve building and optimizing large-scale computer vision systems. We describe a formal algebra framework for the mathematical description of computer vision pipelines for processing image and video streams. The algebra naturally describes feedback control and provides a formal and abstract method for optimizing computer vision pipelines. We then show that a general optimizer can be used with the feedback-control mechanisms of our stream algebra to provide a common online parameter optimization method for computer vision pipelines.',\n",
       " 'Vision-based algorithms have been widely used for detection of underwater objects that are long, flexible, and highly deformable - like cables, pipelines, ropes, wires, and long strands of fishing net. Most algorithms use a deep learning or a combination of deep learning and classical computer vision approach in order to detect these specific objects. This paper presents a pure classical computer vision approach that aims to solve this detection problem by combining several state-of-the-art computer vision techniques. The implemented algorithm is tested using real world data collected using a AUV.',\n",
       " 'this paper presents the implementation of underwater computer vision of the ZEARUS Autonomous Underwater Vehicle (AUV) at Kasetsart University. The purpose of the implementation is to augment previously used algorithm for the 2018 International RoboSub Competition and Singapore AUV 2018 Challenge. The result shows that new algorithm has higher precision level and can achieve better object detection.',\n",
       " \"Computer-vision-based Detect and Avoid (DAA) promises to enable large-scale unmanned BVLoS drone flights in the near future, which would have enormous economic benefits. However, quite a bit of work remains to be done before it can be certified. We describe AirMap's ongoing research into computer-vision-based DAA.\",\n",
       " \"The display system of computer information includes many characteristics, such as brightness, chromaticity, resolution and contrast ratio, which can all affect human's vison characteristic and psychology. In this paper, we have mainly studied human's vision characteristic and the characteristic about brightness, color, resolution and the relationship between human's vision persistence and computer display system. What all we have studied are significant to the study of computer display.\",\n",
       " \"With the development of the times, the progress of society, and the continuous improvement of science and technology, people's daily production and life have changed greatly compared with before. Internet, Internet of things, deep algorithm and other technologies are widely used in people's life, which makes big data technology trigger a new round of development trend. In this era of big data, many industries and technology research also ushered in great development opportunities. In order to better study the multimedia technology of computer vision image in the new era; this paper analyzes the application of big data technology, so as to better and more efficient research on computer vision image multimedia technology. The rapid development of the current era has brought great challenges to computer vision image multimedia technology. Therefore, this paper makes an in-depth study of computer vision image multimedia technology under the background of big data. In the research, this paper systematically describes the current computer vision image multimedia technology, as well as the future development direction of computer vision image multimedia technology. Through the analysis, the big data analysis method proposed in this paper plays a key role in the research of computer vision image multimedia technology.\",\n",
       " 'Object recognition and scene classification are among the main interests in computer vision which have been investigated for long. Automatic recognition and classification of objects and scenes is an important skill to be gained by computers, especially in the field of artificial intelligence. Merging this skill with the ever increasing computing power of the computers will help in the development of many applications that are yet to be resolved. In this article, we present a survey on contextual and semantic approaches for object recognition by reviewing both computer vision and human vision literatures',\n",
       " 'Activities of optical computing researches at Osaka University in this decade are described. As a new trend of optical computing research, vision optical computers have been studied. Concepts of vision optical computers are presented and their roles in the next century are discussed.',\n",
       " \"Computational modeling of the primate visual system yields insights of potential relevance to some of the challenges that computer vision is facing, such as object recognition and categorization, motion detection and activity recognition, or vision-based navigation and manipulation. This paper reviews some functional principles and structures that are generally thought to underlie the primate visual cortex, and attempts to extract biological principles that could further advance computer vision research. Organized for a computer vision audience, we present functional principles of the processing hierarchies present in the primate visual system considering recent discoveries in neurophysiology. The hierarchical processing in the primate visual system is characterized by a sequence of different levels of processing (on the order of 10) that constitute a deep hierarchy in contrast to the flat vision architectures predominantly used in today's mainstream computer vision. We hope that the functional description of the deep hierarchies realized in the primate visual system provides valuable insights for the design of computer vision algorithms, fostering increasingly productive interaction between biological and computer vision research.\",\n",
       " 'nan',\n",
       " 'Heavy vehicle weights need to be closely monitored for preventing fatigue-induced deterioration and critical fractures to highway infrastructure, among many other purposes, but development of a cost-effective weigh-in-motion (WIM) system remains challenging. This paper describes the creation and experimental validations of a computer vision-based non-contact WIM system. The underlining physics is that the force exerted by each tire onto the road is the product of the tire-road contact pressure and contact area. Computer vision is applied (1) to measure the tire deformation parameters so that the tire-roadway contact area can be accurately estimated; and (2) to recognize the marking texts on the tire sidewall so that the manufacturer-recommended tire inflation pressure can be found. In this research, a computer vision system is developed, which is comprised of a camera and computer vision software for measuring tire deformation parameters and recognizing the tire sidewall markings from images of individual tires of a moving vehicle. Computer vision techniques such as edge detection and optical character recognition are applied to enhance the measurement and recognition accuracy. Field experiments were conducted on fully loaded or empty concrete trucks and the truck weights estimated by this novel computer vision-based non-contact WIM system agreed well with the curb weights verified by static weighing. This research has demonstrated a novel application of the computer vision technology to solve a challenging vehicle WIM problem. Requiring no sensor installation on the roadway or the vehicle, this cost-effective non-contact computer vision system has demonstrated a great potential to be implemented.',\n",
       " \"Imaging channel's nonlinear characteristic is one of the factors of computer vision error. Common methods of distortion correction rely on a lot of prior knowledge and interactive information. As the imaging model is only related to the structure of the vision system, and the parameters of the model can be identified automatically, it has a prevalent significance to correct computer vision error using imaging model. This paper gives an example of micro robot soccer vision system, using imaging model, identifying parameters and gaining good result.\",\n",
       " 'This short paper introduces the scope of our special emphasis session on Computer Vision for Systems Biology. It attempts to define the needs for computer vision based readouts in systems biological research and to shed light on some of the challenges computer vision researchers should tackle for the systems biology community. Finally, it will give a short overview of the invited and contributed papers that will be presented in this session.',\n",
       " 'AI computer vision has advanced significantly in recent years. IoT and edge computing devices such as mobile phones have become the primary computing platform for many end users. Mobile devices such as robots and drones that rely on batteries demand for energy efficient computation. Since 2015, the IEEE Annual International Low-Power Computer Vision Challenge (LPCVC) was held to identify energy-efficient AI and computer vision solutions. The 2020 LPCVC includes three challenge tracks: (1) PyTorch UAV Video Track, (2) FPGA Image Track, and (3) On-device Visual Intelligence Competition (OVIC) Tenforflow Track. This paper summarizes the 2020 winning solutions from the three tracks of LPCVC competitions. Methods and future directions for energy-efficient AI and computer vision research are discussed.',\n",
       " 'nan',\n",
       " \"Partially sighted people face multiple challenges in their daily lives. For example, they are not confident about their interactions with other people, mainly because they are not sure of other people's expressions, feelings or intentions. In many cases, this situation results in social issues such as isolation and marginalisation among others. The field of computer vision has attempted to provide solutions to some of the problems people with visual impairment face. Currently, computer vision technologies are able to identify almost any element available in our environment. However, most of the recognition processes are trained based on image classification. The current proposal pushes the boundaries of computer vision (henceforth CV) powered by image categorisation by evolving it to the next stage, namely cognitive computer vision. We aim to develop a computer algorithm to enhance machine vision by incorporating a conversational analytic (henceforth CA) approach. This intelligence will be embedded into a hardware device with the objective of providing real-time feedback to visually impaired users via speech output, thus reducing the gap between technology and human-like augmented senses.\",\n",
       " 'This paper presents analysis of human vision features which affects the development of systems of computer vision. Human eyes have known limitations and defects. One of characteristics of vision, for example, the field of vision should be taken into account at the arrangement of cameras on an unmanned vehcile. This characteristic has a great influence on safety of driving and can considerably change depending on many factors. The authors analyzed the range and reasons of changes of the mentioned factor. Also, the paper presents parameters for assessment of the field of vision. Computer vision systems have to fulfill many requirements connected with operational conditions, for example, backlight, illumination overfalls, and great changes of brightness, motion in darkness. The variant of arrangement of two cameras for proving binocular vision is presented',\n",
       " 'nan',\n",
       " 'In this paper, we propose an FPGA-based emulation framework that can provide dynamic vulnerability analysis for hardware-accelerated computer vision applications. The framework can be integrated alongside the targeted application, to allow for run-time, in-field, dynamically adjusted vulnerability analysis in real-world conditions, taking into consideration the non-deterministic parameters of the computer vision algorithm computations. We evaluate the proposed framework in real-time using an FPGA platform, for an obstacle avoidance (OA) computer vision application and its disparity estimation kernel to study the impact of Single-Event Upsets (SEUs).',\n",
       " 'Along History, the gestures have been one of the most natural interaction methods of the Human Being. This work proposes a novel 3D interaction technique based on computer vision for human-computer gesture interaction. The main contribution to the interaction field is that this technique implements and improves on-the-edge computer vision algorithms offering a low cost solution providing robustness against scenario and user diversity. Thanks to these characteristics, this system allows the human-computer ubiquitous interaction in a robust way.',\n",
       " 'Text plays an important role in conveying information to users in a virtual reality (VR) environment. Both VR software and hardware are evolving rapidly to improve text display quality. However, evaluation of text readability still relies on human participants. In this study, cloud computer vision was used to evaluate text readability in VR. Human subjects were recruited to test the same text scenarios. The cloud computer vision-based approach produced results that were consistent with human vision-based recognition. The use of computer vision to automate text readability evaluation could significantly reduce the overall effort and time in developing readable text in VR.',\n",
       " 'Collaboration, extension, and reproduction of research is of great importance in computer vision. Scientific workflows offer a unique framework for distributed collaboration and sharing of experiments. They provide a structured, end-to-end analysis methodology that easily and automatically allows for standardized replication and testing of models, inter-operability of heterogeneous codebases, and incorporation of novel algorithms. In this paper, we introduce the use of scientific workflows in computer vision to aid collaboration.',\n",
       " 'Computer vision is a growing field of computer science that intends to extract some useful information from images, usually taken from cameras or scanners. The ability to recognize shapes in images is often necessary in computer vision programs. This article describes how to make a program able to recognize basic geometrical figures by using machine learning. This article shows the image processing stages until feature extraction; Giving to the reader an idea of how to apply computer vision for other problems that involve shape recognition.',\n",
       " 'As the deep learning exhibits strong advantages in the feature extraction, it has been widely used in the field of computer vision and among others, and gradually replaced traditional machine learning algorithms. This paper first reviews the main ideas of deep learning, and displays several related frequently-used algorithms for computer vision. Afterwards, the current research status of computer vision field is demonstrated in this paper, particularly the main applications of deep learning in the research field.',\n",
       " 'In an age which bears witness to a proliferation of Closed Circuit Television (CCTV) cameras for security and surveillance monitoring, the use of image processing and computer vision techniques which were provided as top end bespoke solutions can now be realised using desktop PC processing. Commercial Video Motion Detection (VMD) and Intelligent Scene Monitoring (ISM) systems are becoming increasingly sophisticated, aided, in no small way, by a technology transfer from previously exclusively military research sectors. Image processing is traditionally concerned with preprocessing operations such as Fourier filtering, edge detection and morphological operations. Computer vision extends the image processing paradigm to include understanding of scene content, tracking and object classification. Examples of computer vision applications include Automatic Number Plate Recognition (ANPR), people and vehicle tracking, crowd analysis and model based vision. Often image processing and computer vision techniques are developed with highly specific applications in mind and the goal of a more global understanding computer vision system remains, at least for now, outside the bounds of present technology. This paper will review some of the most recent developments in computer vision and image processing for challenging outdoor perimeter security applications. It also describes the efforts of development teams to integrate some of these advanced ideas into coherent prototype development systems.',\n",
       " 'Software engineering (SE) research has traditionally revolved around engineering the source code. However, novel approaches that analyze software through computer vision have been increasingly adopted in SE. These approaches allow analyzing the software from a different complementary perspective other than the source code, and they are used to either complement existing source code-based methods, or to overcome their limitations. The goal of this manuscript is to survey the use of computer vision techniques in SE with the aim of assessing their potential in advancing the field of SE research. We examined an extensive body of literature from top-tier SE venues, as well as venues from closely related fields (machine learning, computer vision, and human-computer interaction). Our inclusion criteria targeted papers applying computer vision techniques that address problems related to any area of SE. We collected an initial pool of 2,716 papers, from which we obtained 66 final relevant papers covering a variety of SE areas. We analyzed what computer vision techniques have been adopted or designed, for what reasons, how they are used, what benefits they provide, and how they are evaluated. Our findings highlight that visual approaches have been adopted in a wide variety of SE tasks, predominantly for effectively tackling software analysis and testing challenges in the web and mobile domains. The results also show a rapid growth trend of the use of computer vision techniques in SE research.',\n",
       " 'On IoT devices such as autonomous driving drones, computer vision jobs such as video recording, streaming and object detection use same camera frame. However, since these IoT devices are resource-constrained systems, they have two problems. First, these applications often do duplicated processing for the same camera raw frame. Second, scheduling between computer vision applications is difficult. In this paper, we propose a shareable camera framework that performs the tasks of computer vision applications. This framework converts the existing pipeline to a pipeline that does not have redundant processing based on the data flow whenever it receives a request from the applications. It also has a scheduling algorithm to guarantee quality-of-service of the applications in the resource-constrained systems. With the proposed framework, the IoT application developers can easily develop reliable computer vision applications that share a single camera simultaneously.',\n",
       " 'We demonstrate a concept of computer vision as a secure, live service on the Internet. We show a platform to distribute a real lime vision algorithm using simple widely available web technologies, such as Adobe Flash. We allow a user to access this service without downloading an executable or sharing the image stream with anyone. We support developers to publish without distribution complexity Finally the platform supports user-permitted aggregation of data for computer vision research or analysis. We describe results a simple distributed motion detection algorithm. We discuss future scenarios for organically extending the horizon of computer vision research.',\n",
       " 'Autonomous underwater vehicles have increasing demands for computer vision capabilities. As an example, marine ecosystem observation will strongly benefit from real-time analysis of acquired images. However, the complexity of computer vision algorithms and the vast amount of data from still images or video induces serious challenges for the limited energy budget of the vehicles. We propose the extensive employment of field programmable gate array for the energy efficient implementation of on-board computer vision tasks. Our case study considers dedicated implementations for particle image velocimetry and stereo depth map estimation. The results show a substantial improvement in energy efficiency while maintaining attainable accuracy compared to a software reference model.',\n",
       " 'Contribution: Open Source Computer Vision Library (OpenCV) Basics is an application designed with the purpose of facilitating the initiation of industrial engineering students in the field of Computer Vision, making the learning process easier, more dynamic and more direct. To this end, an application has been developed for the Android operating system with which users can make use of a wide variety of algorithms available in the OpenCV library. Background: Teaching topics related to Computer Vision can rely on the use of new technologies such as mobile applications. With this type of support, students can learn concepts that might otherwise be difficult to understand. Intended Outcomes: The objective is to facilitate the assimilation of concepts related to Computer Vision by taking advantage of the camera and the processing power of a mobile device to observe in real time the effects produced on an image by many of the image processing algorithms included in OpenCV. This application is currently available to be downloaded for free through the Google Play Store so that anyone interested in the field of Computer Vision can make use of it. Application Design: The proposed approach introduces students to concepts related to Computer Vision by making use of the developed application, complementing the theoretical contents taught by the teacher with specific examples. Findings: The degree of satisfaction of OpenCV Basics users has been evaluated within the framework of the course advanced robotized systems, taught in the industrial engineering degree at the University of La Laguna.',\n",
       " 'Deep learning has enabled the rapid expansion of computer vision tasks from image frames to video segments. This paper focuses on the review of the latest research in the field of computer vision tasks in general and on object localization and identification of their associated pixels in video frames in particular. After performing a systematic analysis of the existing methods, the challenges related to computer vision tasks are presented. In order to address the existing challenges, a hybrid framework is proposed, where deep learning methods are coupled with domain knowledge. An additional feature of this survey is that a review of the currently existing approaches integrating domain knowledge with deep learning techniques is presented. Finally, some conclusions on the implementation of hybrid architectures to perform computer vision tasks are discussed.',\n",
       " 'Computer vision on chip is critical for many emerging applications such as advanced driver assistance system (ADAS), which requires a low-power and real-time image data analytics. Therefore, designing a computer-vision accelerator on-chip to achieve high throughput as well as low power is greatly needed. This paper reviews how to have ASIC realization of standard computer vision algorithms such as SIFT/SURF. The first work is a feature-based recognition co-processor with peak power consumption of 31.5mW for real-time recognition of VGA images. The second work is a face recognition accelerator with 23mW for 5.5 frame/s HD images.',\n",
       " 'The research and algorithm development process in MATLAB for the multispectral computer vision system is described. There are a lot of ready-made tools for that in MATLAB. These tools save the development time. Then the well-tested computer vision algorithms are translated from MATLAB to C/C++ language for the further implementation.',\n",
       " 'The winners, as well as the organizers and sponsors of the IEEE Low-Power Computer Vision Challenge, share their insights into making computer vision (CV) more efficient for running on mobile or embedded systems. As CV (and more generally, artificial intelligence) is deployed widely on the Internet of Things, efficiency will become increasingly important.',\n",
       " \"Advancements in deep learning have ignited an explosion of research on efficient hardware for embedded computer vision. Hardware vision acceleration, however, does not address the cost of capturing and processing the image data that feeds these algorithms. We examine the role of the image signal processing (ISP) pipeline in computer vision to identify opportunities to reduce computation and save energy. The key insight is that imaging pipelines should be be configurable: to switch between a traditional photography mode and a low-power vision mode that produces lower-quality image data suitable only for computer vision. We use eight computer vision algorithms and a reversible pipeline simulation tool to study the imaging system's impact on vision performance. For both CNN-based and classical vision algorithms, we observe that only two ISP stages, demosaicing and gamma compression, are critical for task performance. We propose a new image sensor design that can compensate for these stages. The sensor design features an adjustable resolution and tunable analog-to-digital converters (ADCs). Our proposed imaging system's vision mode disables the ISP entirely and configures the sensor to produce subsampled, low-erprecision image data. This vision mode can save similar to 75% of the average energy of a baseline photography mode with only a small impact on vision task accuracy.\",\n",
       " 'Graphics and vision are approximate inverses of each other: ordinarily Graphics Processing Units (GPUs) are used to convert numbers into pictures (i.e. computer graphics). In this paper, we discus the use of GPUs in approximately the reverse way: to assist in converting pictures into numbers (i.e. computer vision). For graphical operations, GPUs currently provide many hundreds of gigaflops of processing power. This paper discusses how this processing power is being harnessed for Image Processing and Computer Vision, thereby providing dramatic speedups on commodity, readily available graphics hardware. A brief review of algorithms mapped to the GPU by using the graphics API for vision is presented. The recent NVIDIA CUDA programming model is then introduced as a way of expressing program parallelism without the need for graphics expertise.',\n",
       " 'Traditional image signal processors (ISPs) are primarily designed and optimized to improve the image quality perceived by humans. However, optimal perceptual image quality does not always translate into optimal performance for computer vision applications. We propose a set of methods, which we collectively call VisionISP, to repurpose the ISP for machine consumption. VisionISP significantly reduces data transmission needs by reducing the bit-depth and resolution while preserving the relevant information. The blocks in VisionISP are simple, content-aware, and trainable. Experimental results show that VisionISP boosts the performance of a subsequent computer vision system trained to detect objects in an autonomous driving setting. The results demonstrate the potential and the practicality of VisionISP for computer vision applications.',\n",
       " \"In this paper, our goal is to understand what needs to be done to enable computer vision algorithms running on uncompressed image sequences to run as well on image sequences that have undergone compression and then decompression. The central conflict of context based computer vision algorithms versus the structured block based approach of today's codecs means that more has to be done than to simply create a divide between coding foreground preferentially and giving less importance to background. We take as example, a single computer vision algorithm, the mean shift tracker and see that its performance can be improved substantially in low bit rate scenarios, albeit some tradeoffs.\",\n",
       " 'Accurate sensor noise propagation is critical for many computer vision and robotic applications. Several probabilistic computer vision techniques require estimates of sensor noise after it has been propagated through one or many non-linear transformations. We investigate the unscented transform as an alternative to the standard linearisation technique for uncertainty propagation in a computer vision framework. An evaluation is performed using synthetic data for two common computer vision sensors, an RGB-D sensor and stereo camera pair. The unscented transform is shown to outperform linearisation when used to estimate distributions of reconstructed, 3D points from image features. Experimental results also indicate that the unscented transform is a viable replacement for linearisation when used in a probabilistic visual odometry framework.',\n",
       " \"Artificial Intelligence (AI) has found many applications in today's world, such as computer vision for self-driving cars, speech recognition for personal assistants, and algorithm design for strategy gaming systems. Although enjoying the convenience that AI has brought to our daily lives, people may be wondering when and how it started and evolved.\",\n",
       " 'Artificial intelligence is a branch of research that allows machines to solve intuitive problems by imitating the way the human brain works. Meaningful information can be extracted from image and video analysis using computer vision methods. Computer vision studies in areas such as digital image processing, object recognition, object tracking have been strengthened with artificial intelligence-based components developed in recent years. Thus, faster and more precise results can be obtained. The aim of this study is to reveal the artificial intelligence methods used in the field of computer vision, the principles and constraints of the methods in the light of the related literature.',\n",
       " 'nan',\n",
       " 'Pattern recognition and computer vision tasks are computationally intensive, repetitive, and often exceed the capabilities of the CPU, leaving little time for higher level tasks. We present a novel computer architecture which uses multiple, commodity computer graphics devices to perform pattern recognition and computer vision tasks many times faster than the CPU. This is a parallel computing architecture that is quickly and easily constructed from readily available hardware. It is based on parallel processing done on multiple Graphics Processing Units (GPUs). An eigenspace image recognition approach is implemented on this parallel graphics architecture. This paper discusses methods of mapping computer vision algorithms to run efficiently on multiple graphics devices to maximally utilize the underlying graphics hardware. The additional memory and memory bandwidth provided by the graphics hardware provided for significant speedup of the eigenspace approach. We show that graphics devices parallelize well and provide significant speedup over a CPU implementation, providing an immediately constructible low cost architecture well suited for pattern recognition and computer vision.',\n",
       " \"AS/RS (Automated Storage and Retrieval System) is the important component of warehousing system. How to improve the efficiency of the goods' recognition and picking is the pivotal problem in AS/RS. This paper introduces the study of computer vision and robot technology application in AS/RS. With the application of computer vision and robot control technology, automatic-recognition and automatic-positioning of goods can come true. An all-automated store and picking system in AS/RS had been put forward in the paper.\",\n",
       " 'With the increasing complexity of machine vision algorithms and growing applications of image processing, how do computers without a dedicated graphics processor perform? This research discusses the computational abilities of two low-cost single board computers (SBCs) by subjecting them to various Visual Inertial Odometry (VIO) algorithms. The end goal of this research is to identify a SBC which meets the requirements of being employed on an Unmanned Aerial System for autonomous navigation.',\n",
       " 'Automated data acquisition and analysis using scripting and Computer Vision (CV) allow one to perform repetitive tasks faster with higher accuracy. In this paper, we discuss techniques to automate analytical tools and several examples of advanced image processing and data analysis to provide accurate IC diagnostics.',\n",
       " 'This paper presents a technique to develop a vision based interface system for controlling and performing various computer functions with the aim of making the human-computer interaction. The main aim of human computer interaction is to develop simpler ways for users to interact with computers. One of the main areas of research in human machine interaction is hand gesture recognition. It makes interaction with machines intelligible and effortless. In this investigation, with the use of a camera and computer vision technology such as image segmentation and feature extraction, a technique is developed which can be used for computer control using hand gestures.',\n",
       " 'Computer vision applications have a large disparity in operations, data representation and memory access patterns from the early vision stages to the final classification and recognition stages. A hardware system for computer vision has to provide high flexibility without compromising performance, exploiting massively spatial-parallel operations but also keeping a high throughput on data-dependent and complex program flows. Furthermore, the architecture must be modular, scalable and easy to adapt to the needs of different applications. Keeping this in mind, a hybrid SIMD/MIMD architecture for embedded computer vision is proposed. It consists of a coprocessor designed to provide fast and flexible computation of demanding image processing tasks of vision applications. A 32-bit 128-unit device was prototyped on a Virtex-6 FPGA which delivers a peak performance of 19.6 GOP/s and 7.2 W of power dissipation.',\n",
       " 'nan',\n",
       " \"In the context of 5G networks, the possibility to fusion radio vision with computer vision is a must-have asset, enabler of building the extensive navigation maps through the so-called MirrorWorld. This demo will showcase an essential building block for merging the two environments: matching a user equipment's identity in video stream and in radio measurements. We demonstrate the integration of a computer vision system with a radio access network and showcase the identification of the true radio transmitter between two equipment existing in a video feed.\",\n",
       " 'The problem of computer vision is to automatically characterize the contents of digitized images. Applications include factory automation, navigation, digital libraries, and medicine. Not only is recognition an inverse problem with no single mathematical solution, but it is also complicated by external sources of uncertainty such as the conditions of image formation. Thus, the need for dealing with uncertainty in computer vision is well accepted. However, the majority of work in this area has used fixed thresholds or probabilistic approaches, from surface reconstruction to object recognition. This paper will survey current approaches to uncertainty in computer vision, paying particular attention to the attitudes toward fuzzy systems. Although fuzzy systems are out of the mainstream of computer vision, they pose great promise for addressing uncertainty issues that are nor adequately dealt with by current methods.',\n",
       " 'The increasing availability of open source implementations of computer vision algorithms promises to commoditize these technologies as reusable community maintained building blocks. Using OpenCV and the cross-platform Qt development environment, we investigate the suitability of an open building block approach for rapid concept prototyping of computer vision based user interaction designs in CE-orientated networked multimedia home environments.',\n",
       " 'This frame work is a solution for two global issues. First is, hand amputated people cannot control machines by using fingers. The second is unavailability of a low cost human motion controlled system for gaming. A computer vision based solution is given to both these problems by generating six different signals from six different hand movements.',\n",
       " 'Computer applications have considerably shifted from single data processing to machine learning in recent years due to the accessibility and availability of massive volumes of data obtained through the internet and various sources. Machine learning is automating human assistance by training an algorithm on relevant data. Supervised, Unsupervised, and Reinforcement Learning are the three fundamental categories of machine learning techniques. In this paper, we have discussed the different learning styles used in the field of Computer vision, Deep Learning, Neural networks, and machine learning. Some of the most recent applications of machine learning in computer vision include object identification, object classification, and extracting usable information from images, graphic documents, and videos. Some machine learning techniques frequently include zero-shot learning, active learning, contrastive learning, self-supervised learning, life-long learning, semi-supervised learning, ensemble learning, sequential learning, and multi-view learning used in computer vision until now. There is a lack of systematic reviews about all learning styles. This paper presents literature analysis of how different machine learning styles evolved in the field of Artificial Intelligence (AI) for computer vision. This research examines and evaluates machine learning applications in computer vision and future forecasting. This paper will be helpful for researchers working with learning styles as it gives a deep insight into future directions.',\n",
       " 'As the world is getting more connected and processing power increases, the need for undergraduate students to have a solid foundation in machine learning and computer vision is becoming standard [1] [2]. In this paper, we develop a foundation for future computer vision and machine learning work through use of open-source software that can be used on personal and embedded devices. This foundation helps prepare students entering the workforce who will be working in autonomous vehicle safety and car control features.',\n",
       " 'Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent neural networks. Given its high performance and less need for vision-specific inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these vision transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also include efficient transformer methods for pushing transformer into real device-based applications. Furthermore, we also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Toward the end of this paper, we discuss the challenges and provide several further research directions for vision transformers.',\n",
       " 'In this paper we describe, without a pretense of completeness, some modeling and identification techniques which have been proposed recently for applications to computer vision. The emphasis is on methods which, although sometimes still in development, attempt to address specific issues of the particular application area.',\n",
       " 'Shrimp is one of the most important aquaculture products in Vietnam and it is desired to continuously estimate shrimp population density and size This paper presents our initial steps in supervising aqua farming based on computer vision techniques. The computer vision system includes U-net segmentation, marker-controlled watershed segmentation, thresholding, contour tracing, etc. Experiment results show that the proposed method can obtain good results in counting shrimps (mean absolute error of 0.093) and estimating shrimp lengths (RMS error of 0.293 cm) when they are separately located.',\n",
       " 'This study aims at classifying the quality of food products based on the computer vision systems. In the present paper a review based on quality evaluation of few food materials like cauliflower, cereals, spinach leaves, tea, mango and various steps to find their quality based on their defects are described.',\n",
       " 'The purpose of this paper is to describe the design and implementation of a computer vision based analysis system for dendrochronology. The issues involved in the detection and analysis of tree rings are not unique to the application, but are likely of interest to anyone developing automated image analysis systems.',\n",
       " \"This paper proposes a scheme of terrain aided navigation based on principle of computer vision. Being different from the conventional terrain matching technique, the scheme uses CCD camera rather than barometer and radio altimeter as sensing element. Terrain elevation information within an area rather than along the course of vehicle's flight is drawn from CCD images according to the principle of computer vision. Shorter flight time is needed to gather sufficient information for successful terrain matching, hence, the scheme provides estimation and compensation for errors of inertial navigation system more rapidly.\",\n",
       " 'Deformable models are a useful tool in computer vision and computer graphics. A deformable model is a curve (in two dimensions) or a surface (in three dimensions), whose shape, position, and orientation are controlled through a set of parameters. Deformable models can represent manufactured objects, human faces and skeletons, and even bodies of fluid. In computer graphics we use deformable models for animations and simulations, whereas in computer vision applications, such as tracking and fitting, deformable models help to restrict the family of possible solutions. In this paper, we introduce the use of a directed acyclic graph (DAG) to describe the position and Jacobian of each point on the surface of deformable models. This data structure, combined with a topological description of the points, is simple, powerful, and extremely useful for both computer vision and computer graphics applications. We show a computer vision application, 3D deformable face tracking, and a computer graphics application, cyberylove data visualization and calibration.',\n",
       " 'nan',\n",
       " 'Computer vision detection technology is one of the most popular topics in the field of computer vision. With the continuous improvement of the relevant algorithm and the performance-price ratio of the corresponding imaging equipment, the corresponding computer vision detection algorithm is also constantly upgraded and deepened. Computer vision detection technology is mainly used in transportation, public security, national defense and military fields, but the pixel accuracy of traditional computer vision detection technology has been unable to meet today & x2019;s accuracy requirements. In this paper, firstly, the quantum denoising algorithm based on dual-tree and dual-density wavelet transform is used to realize the combination of quantum image coding expression and wavelet transform, and finally achieve a more detailed and accurate description of the image and realize the noise reduction of the image. In order to further realize sub-pixel image processing, cubic spline interpolation edge detection algorithm will be added to wavelet transform, which mainly calculates the zeros of the second-order function corresponding to the cubic spline function on both sides of the image edge points, so as to realize sub-pixel location of the image edge points. Finally, by comparing with the traditional pixel accuracy detection algorithms, it can be found that the proposed sub-pixel computer vision detection algorithm based on wavelet transform has good robustness, and its computing time is relatively faster, so it will have better adaptability in practical applications.',\n",
       " 'The success of deep learning has led to intense growth and interest in computer vision, along with concerns about its potential impact on society. Yet we know little about how these changes have affected the people that research and practice computer vision: we as a community spend so much effort trying to replicate the abilities of humans, but so little time considering the impact of this work on ourselves. In this paper, we report on a study in which we asked computer vision researchers and practitioners to write stories about emotionally-salient events that happened to them. Our analysis of over 50 responses found tremendous affective (emotional) strain in the computer vision community. While many describe excitement and success, we found strikingly frequent feelings of isolation, cynicism, apathy, and exasperation over the state of the field. This is especially true among people who do not share the unbridled enthusiasm for normative standards for computer vision research and who do not see themselves as part of the incrowd. Our findings suggest that these feelings are closely tied to the kinds of research and professional practices now expected in computer vision. We argue that as a community with significant stature, we need to work towards an inclusive culture that makes transparent and addresses the real emotional toil of its members.',\n",
       " 'Wheeled robot soccer is a soccer robot that moves with wheels with several capabilities, such as detecting, catching, dribbling, and kicking a ball. A robot soccer requires a system or sensor to detect a ball. The development of computer vision technology enables a computer system to detect a ball using computer vision algorithm. In this research, the system consisting of a camera and a single board computer. The camera is used to capture an image and the single board computer is used to process the image to get ball position on the image. The ball position information is then processed to map the position of the ball. After that, the information of mapping is sent to the robot controller and process it into a movement. Experiments result show that the robot with the designed system is function as designed.',\n",
       " 'ARIEL, a newly developed small-sized, low-cost, and modulus hovering type autonomous underwater vehicle, is designed to perform computer-vision-driven intervention tasks on a small single-board computer. The working prototype has demonstrated an underwater intervention mission and proved a functional system at the Underwater Robot Convention in JAMSTEC 2021. In the experiment trials, ARIEL performed the mission in the testing water tank. ARIEL is a reference design of an AUV for lightweight shoreline intervention tasks.',\n",
       " 'The rehabilitation robot Manus is an assistive device for severely motor handicapped users. The executing of all day living tasks with the Manus, can be very complex and a vision-based controller can simplify this. The lack of existing vision-based controlled systems, is the poor reliability of the computer vision in unstructured environments. In this paper, a computer vision solution is presented, which can estimate real-time the pose of an object and co-operate with a vision-based controller. The computer vision is robust to illumination changes, a varying scale and rotation and is robust to occlusion. The computer vision is mainly based on the SIFT-algorithm and the usage of a 3D-model of an object. Important steps to create this 3D-model are discussed. The detection and recognition of the required SIFT-keypoints, has become real-time, by exchanging redundancy against calculation time. With a position-based visual servoing controller, the Manus can be positioned with respect to an object.',\n",
       " 'Computer vision enables amongst others detection and tracking of static and moving objects, as well as identification of events and actions. Nevertheless the applicability and adoption of computer vision approaches in large-scale industrial environments is limited mainly due to their computation requirements when focusing on real-time objects tracking or events identification. In this paper we present the experimentation outcomes of a computer vision application that has been deployed on a large-scale multi-cloud facility. Effective monitoring and workflow management mechanisms are also presented as the enablers for meeting the real-time requirements of the computer vision application. We evaluate the effectiveness of these mechanisms through a set of experiments that demonstrate their value for allowing cloud infrastructures to provide real-time guarantees.',\n",
       " 'This paper proposes that despite the success of deep learning methods in computer vision, the dominance we see would not have been possible by the methods of deep learning alone: the tacit change has been the evolution of empirical practice in computer vision. We demonstrate this by examining the distribution of sensor settings in vision datasets, only one potential dataset bias, and performance of both classic and deep learning algorithms under various camera settings. This reveals a strong mismatch between optimal performance ranges of theory-driven algorithms and sensor setting distributions in common vision datasets.',\n",
       " \"This paper presents a method based on computer vision, which allows the gesture recognition of a pianist's right hand. The choice of computer vision as a processing method was made due to the lack of a satisfying fingering detection system addressing musicians. On the other hand, Hidden Markov Models (HMM) are an advisable mathematic theory managing stochastic processes such as human gestures. Both HMM and computer vision co-operate to achieve content-based video retrieval in Music Interaction, as presented in this paper. Thus, we developed a method of hand extraction, based on the special position assumed by the hand of the pianist. The study of the pianist fingering retrieval has been effectuated on four aspects: preparation and capture of video signal, hand segmentation, finger extraction, feature vector exportation and HMM classifier.\",\n",
       " 'nan',\n",
       " 'nan',\n",
       " \"This paper presents an automated system to assess human vision to identify early signs of vision disorders such as amblyopia (lazy eye), so that potential problems can be addressed as early as possible by having the system refer children to a specialist (pediatric ophthalmologist). The system does not require extensive operator training or patient cooperation. This paper explores the application of photoscreening, computer vision and artificial intelligence techniques for diagnosing vision disorders by processing video images taken of patients' eyes, computing important eye features, and determining the referral decisions. Extensive experiments and analysis indicate that the system has an accuracy of 77% when evaluated using the referral decisions, which are recommended by a specialist.\",\n",
       " 'Computer vision systems are increasingly used in industry for inspection or process control. The more demanding requirements observed today make the implementation of this type of systems a technological challenge. Many of the computational architectures available allow us to meet the main functional requirements related to the use case and also the non-functional ones, such as processing time restrictions and connectivity. However, the requirement for adaptability so that such systems can be easily modified to meet different use cases or even accommodate environmental changes remains a challenge. This work proposes a flexible architecture for computer vision systems using FPGA. This architecture combines components of the processing flow of the vision system implemented in hardware and software to obtain advantages associated with both approaches. The proposed solution is validated against different real use cases existing in the industry. The results obtained allow us to affirm that such architecture brings an interesting advantages since it meets the operational requirements present in industrial applications, demands less development effort and can be easily adapted to new usage scenarios.',\n",
       " 'Reproducibility of research is an area of growing concern in computer vision. Scientific workflows provide a structured methodology for standardized replication and testing of state-of-the-art models, open publication of datasets and software together, and ease of analysis by re-using pre-existing components. In this paper, we present initial work in developing a framework that will allow reuse and extension of many computer vision methods, as well as allowing easy reproducibility of analytical results, by publishing datasets and workflows packaged together as linked data. Our approach uses the WINGS semantic workflow system which validates semantic constraints of the computer vision algorithms, making it easy for non-experts to correctly apply state-of-the-art image processing methods to their data. We show the ease of use of semantic workflows for reproducibility in computer vision by both utilizing pre-developed workflow fragments and developing novel computer vision workflow fragments for a video activity recognition task, analysis of multimedia web content, and the analysis of artistic style in paintings using convolutional neural networks.',\n",
       " 'In this paper, a 4 degree-of-freedom (DOF) chess playing robotic manipulator and computer vision based chess recognition system are presented. The robotic system is capable of playing chess game autonomously against human or another robotic system. The logical system consists of anti-glare camera mounted on the chessboard, which acts as an eye of robot, personal computer for run-time implementation of computer vision algorithm, an open-source chess engine used as chess brain of robotic manipulator, which plays chess algorithmic ally on behalf of robot, and of course robot itself. On development side, a simple image segmentation based computer vision algorithm was developed to find the legitimate chess move and human hand motion detection, a software system was developed that enables the robot to pick and drop chess pieces from prescribed chess boxes, lastly communication channel was exploited for interfacing computer vision algorithm with chess engine. The whole robotic system is formulated from recycled machine parts and open source tools. The convincing performance of computer vision algorithm and robotic manipulator testifies by its 1st position in IEEE Final Year Project Competition Lahore 2012',\n",
       " \"This paper presents a project that allows the Baxter humanoid robot to play chess against human players autonomously. The complete solution uses three main subsystems: computer vision based on a single camera embedded in Baxter's arm to perceive the game state, an open-source chess engine to compute the next move, and a mechatronics subsystem with a 7-DOF arm to manipulate the pieces. Baxter can play chess successfully in unconstrained environments by dynamically responding to changes in the environment. This implementation demonstrates Baxter's capabilities of vision-based adaptive control and small-scale manipulation, which can be applicable to numerous applications, while also contributing to the computer vision chess analysis literature.\",\n",
       " \"In this paper the phases of the mechanical design and fuzzy logic based control system implementation for the autonomous robot with computer vision system are considered. The robot was designed on the department of automatic control systems in the Saint Petersburg Electrotechnical University LETI in relation to the study of the autonomous robot's control systems.\",\n",
       " 'This article is devoted to the application development for Android OS using computer vision technology. It demonstrates the implementation process step by step and also describes working with such development tools like: Java, ML Kit, Android Studio. The result of the study is application that the user interacts with by turning their head and blinking their eyes.',\n",
       " 'A quadrotor Micro Aerial Vehicle (MAV) is designed to navigate a track using neural network approach to identify the direction of the path from a stream of monocular images received from a downward-facing camera mounted on the vehicle. Current autonomous MAVs mainly employ computer vision techniques based on image processing and feature tracking for vision-based navigation tasks. It requires expensive onboard computation and can create latency in the real-time system when working with low-powered computers. By using a supervised image classifier, we shift the costly computational task of training a neural network to classify the direction of the track to an off board computer. We make use of the learned weights obtained after training to perform simple mathematical operations to predict the class of the image on the onboard computer. We compare the computer vision based tracking approach with the proposed approach to navigate a track using a quadrotor and show that the processing rates of the latter is faster. This allows low-cost, low-powered computers such as the Raspberry Pi to be used efficiently as onboard companion computers for flying vision-based autonomous missions with MAVs.',\n",
       " 'The COVID-19 pandemic has triggered an urgent call to contribute to the fight against an immense threat to the human population. Computer Vision, as a subfield of artificial intelligence, has enjoyed recent success in solving various complex problems in health care and has the potential to contribute to the fight of controlling COVID-19. In response to this call, computer vision researchers are putting their knowledge base at test to devise effective ways to counter COVID-19 challenge and serve the global community. New contributions are being shared with every passing day. It motivated us to review the recent work, collect information about available research resources, and an indication of future research directions. We want to make it possible for computer vision researchers to find existing and future research directions. This survey article presents a preliminary review of the literature on research community efforts against COVID-19 pandemic.',\n",
       " 'nan',\n",
       " 'Automatic inspections of overhead contact lines (OCLs) are developed to implement anomaly detection during normal operation. It is an essential prerequisite for efficient maintenance of railway electrification system. This paper presents a comprehensive survey on the inspections of OCLs with an emphasis on computer vision technology, which has developed rapidly due to its ability to understand images. Our survey begins with a brief introduction on anomalies in OCL inspections and generic procedures of computer vision inspection for anomaly detection. Subsequently, for detecting deviations of parameters and defective components during OCL inspections, the existing techniques involving stereo vision and object vision, especially convolution neural networks are described in detail from two aspects: measurement of OCL parameters and identification of OCL conditions. Some interference factors in OCL inspection are analyzed. Actual cases of the inspections are also briefly shown. Challenges and suggestions for further research on OCL inspection are drawn toward the end of the paper.',\n",
       " \"Computer Vision would complement the autonomous robotics goals of accuracy and precision by adding eyes to the performance of the robot. We will be researching on computer vision techniques and experimenting their benefits with autonomous robotics. We investigated both Arduino MegaTM Controller Board and Raspberry Pi for analyzing the flexibility of chipset for computer vision using the 4-wire dual data bus Pixi Camera (CMU's 5th version). Both C/C++ and Python libraries were implemented to test the vision camera using 12C and USB communication. Pixi camera scans at a rate of 50 frames per second. We investigated the object recognition, object classification and object tracking capabilities using various algorithms for detecting the specific target doors in a hallway. The autonomous robot is going to maneuver in the hall way by avoiding the obstacles and stop precisely at a door-front of the assigned door number. This is possible by associating the door numbers with color codes.\",\n",
       " 'Wearable, camera based, head-tracking systems use spatial image registration algorithms to align images taken as the wearer gazes around their environment. This allows for computer-generated information to appear to the user as though it was anchored in the real world. Often, these algorithms require creation of a multiscale Gaussian pyramid or repetitive re-projection of the images. Such operations, however can be computationally expensive, and such head-tracking algorithms are desired to run in real-time on a body borne computer In this paper we present a method of using the 3D computer graphics hardware that is available in a typical wearable computer to accelerate the repetitive image projections required in many computer vision algorithms. We apply this graphics for vision technique to a wearable camera based head-tracking algorithm, implemented on a wearable computer with 3D graphics hardware. We perform an analysis of the acceleration achieved by applying graphics hardware to computer vision to create a Mediated Reality.',\n",
       " 'With the rapid development of range image acquisition techniques, 3D computer vision has became a popular research area. It has numerous applications in various domains including robotics, biometrics, remote sensing, entertainment, civil construction, and medical treatment. Recently, a large number of algorithms have been proposed to address specific problems in the area of 3D computer vision. Meanwhile, several benchmark datasets have also been released to stimulate the research in this area. The availability of benchmark datasets plays an significant role in the process of technological progress. In this paper, we first introduce several major 3D acquisition techniques. We also present an overview on various popular topics in 3D computer vision including 3D object modeling, 3D model retrieval, 3D object recognition, 3D face recognition, RGB-D vision, and 3D remote sensing. Moreover, we present a contemporary summary of the existing benchmark datasets in 3D computer vision. This paper can therefore, serve as a handbook for those who are working in the related areas.',\n",
       " 'Extracting and indexing relevant information with computer vision algorithms in very large solar image archives allows investigating solar activity from a new perspective. Using computer vision algorithms, we have developed methods that work with very compact and concise descriptions of images. We apply our method to images from the Solar Dynamics Observatory (SDO) and present a proof-of-concept Query by Example (QBE) system. In addition we introduce a benchmark dataset, on one hand to evaluate our system, and on the other hand to allow comparisons of our results with other QBE systems in this domain.',\n",
       " \"Online teaching-learning platforms have become an integral part of life, during and post Covid-19 pandemic. In this regard, many teaching-learning accessories have been developed for use. AirPad, the work presented in this article is an application that helps draw one's imagination on screen by just capturing the motion of object of interest with a camera in air. Computer vision is concerned with the extraction of meaningful information from image data. Continued explorations on computer vision are often concerned with the development of computer algorithms for specific applications. Computer vision is a field of artificial intelligence that works on enabling computers to see, identify and process images in the same way as human vision does, and then provide the appropriate output. Three important tasks involved during computer vision processing are: 1) Detection 2) Tracking 3) Recognition. Computer vision algorithms are utilized to perform the task. The preferred language used is Python due to its exhaustive libraries and easy to use syntax, but can be implemented in any OpenCV supported language. Present work uses PyQt5 which is a python interface for Qt library. It is one of the most used modules in building GUI apps in Python, and that's due to its simplicity. Following features have been implemented in the present work: i. Functions to draw square, rectangle and circle. ii. Testing Tab to ensure proper traction of object of interest in each frame. iii. Export the work in multiple formats like images and pdf. The user requires a laptop with webcam and a virtual pen which could be a finger to use the application and enjoy its features.\",\n",
       " 'A vehicle detection and counting system plays an important role in an intelligent transportation system, especially for traffic management. This paper proposes a video-based method for vehicle detection and counting system based on computer vision technology. The proposed method uses background subtraction technique to find foreground objects in a video sequence. In order to detect moving vehicles more accurately, several computer vision techniques, including thresholding, hole filling and adaptive morphology operations, are then applied. Finally, vehicle counting is done by using a virtual detection zone. Experimental results show that the accuracy of the proposed vehicle counting system is around 96%.',\n",
       " 'Assistive robotic devices have great potential to improve the quality of life for individuals suffering with movement disorders. One such device is a robot-arm which helps people with upper body mobility to perform daily tasks. Manual control of robot arms can be challenging for wheelchair users with upper extremity disorders. This research presents an autonomous wheelchair mounted robotic arm built using a computer vision interface. The design utilizes a robotic arm with six degrees of freedom, an electric wheelchair, computer system and two vision sensors. One vision sensor detects the coarse position of the colored objects placed randomly on a shelf located in front of the wheelchair by using a computer vision algorithm. The other vision sensor provides fine localization by ensuring the object is correctly positioned in front of the gripper. The arm is then controlled automatically to pick up the object and return it to the user. Tests have been conducted by placing objects at different locations and the performance of the robotic arm is tabulated. An average task completion time of 37.52 seconds is achieved.',\n",
       " 'nan',\n",
       " 'Data fusion procedure is presented to enhance classical Advanced Driver Assistance Systems (ADAS). The novel vehicle safety approach, combines two classical sensors: computer vision and laser scanner. Laser scanner algorithm performs detection of vehicles and pedestrians based on pattern matching algorithms. Computer vision approach is based on Haar-Like features for vehicles and Histogram of Oriented Gradients (HOG) features for pedestrians. The high level fusion procedure uses Kalman Filter and Joint Probabilistic Data Association (JPDA) algorithm to provide high level detection. Results proved that by means of data fusion, the performance of the system is enhanced.',\n",
       " 'nan',\n",
       " 'This paper addresses the problem of migrating large and complex computer vision code bases that have been developed off-line, into efficient real-time implementations avoiding the need for rewriting the software, and the associated costs. Creative linking strategies based on Linux loadable kernel modules are presented to create a simultaneous realization of real-time and off-line frame rate computer vision systems from a single code base. In this approach, systemic predictability is achieved by inserting time-critical components of a user-level executable directly into the kernel as a virtual device driver. This effectively emulates a single process space model that is nonpreemptable, nonpageable, and that has direct access to a powerful set of system-level services. This overall approach is shown to provide the basis for building a predictable frame-rate vision system using commercial off-the-shelf hardware and a standard uniprocessor Linux operating system. Experiments on a frame-rate vision system designed for computer-assisted laser retinal surgery show that this method reduces the variance of observed per-frame central processing unit cycle counts by two orders of magnitude. The conclusion is that when predictable application algorithms are used, it is possible to efficiently migrate to a predictable frame-rate computer vision system.',\n",
       " 'To measure the height and wear of the diamond abrasive, a new technique based on computer vision is presented in this paper. Firstly, the computer vision system is designed. The height evaluation of abrasives and the calibration of the imaging set-up are introduced in detail. Secondly, we adopt the sequential similarity detection algorithm to get corresponding pixels in the left and right image. Finally, the heights and wear of the grain are obtained by calculating their parallax on left and right images. The experiment of standard block shows that the relative error is less than 5%.',\n",
       " \"Numerical dynamic range analysis combines with architectural simulation to help designers explore algorithmic and architectural tradeoffs that can meet embedded computer vision's demand for precision and high-memory bandwidth.\",\n",
       " 'Real hands-on experience can help students gain a better understanding of theoretical problems in image analysis and computer vision and allows them to put in practice and improve their knowledge in digital signal processing, mathematics, statistics, perception and psychophysics. However, important efforts are necessary to enable students to develop a computer vision application because of the lack of extensively tested and well documented software platforms. In this paper, we describe our experience with an open source library addressed to researchers and developers in computer vision, the OpenCV library, its limits when used by students, and how we adapted it for teaching purposes by producing a set of appropriate tutorials. These tutorials help the students reduce the average time for installation and setup from I week to 4 hours and help them design an end-to-end image analysis and computer vision project. Finally, we discuss our experience of using this framework for undergraduate as well postgraduate student projects.',\n",
       " 'Object tracking is one of the most important and fundamental disciplines of Computer Vision. Many Computer Vision applications require specific object tracking capabilities, including autonomous and smart vehicles, video surveillance, medical treatments, and many others. The OpenCV as one of the most popular libraries for Computer Vision includes several hundred Computer Vision algorithms. Object tracking tasks in the library can be roughly clustered in single and multiple object trackers. The library is widely used for real-time applications, but there are a lot of unanswered questions such as when to use a specific tracker, how to evaluate its performance, and for what kind of objects will the tracker yield the best results? In this paper, we evaluate 7 trackers implemented in OpenCV against the MOT20 dataset. The results are shown based on Multiple Object Tracking Accuracy (MOTA) and Multiple Object Tracking Precision (MOTP) metrics.',\n",
       " \"Computer vision has achieved impressive progress in recent years. Meanwhile, mobile phones have become the primary computing platforms for millions of people. In addition to mobile phones, many autonomous systems rely on visual data for making decisions, and some of these systems have limited energy (such as unmanned aerial vehicles also called drones and mobile robots). These systems rely on batteries, and energy efficiency is critical. This paper serves the following two main purposes. First, examine the state of the art for low-power solutions to detect objects in images. Since 2015, the IEEE Annual International Low-Power Image Recognition Challenge (LPIRC) has been held to identify the most energy-efficient computer vision solutions. This paper summarizes the 2018 winners' solutions. Second, suggest directions for research as well as opportunities for low-power computer vision.\",\n",
       " 'This paper contains development of methods and algorithms of computer vision for image recognition of mineral spices. It is described CASE-technology for automated systems development was used to create computer vision system for assessing the qualitative composition of mineral rocks. Methods and algorithms of computer vision of mineral rocks, in particular problems of the algorithm for automatic segmentation of colour images of ores, using the methods of cluster analysis are considered. Results of studies are demonstrated different colour spaces by k-means clustering. It was supposed the technique of pre- computing the values of the centroids. There is formulas translation metrics colour space HSV. The effectiveness of the proposed method lies in the automatic identification of interest objects on the total image, tuning parameters of the algorithm is a number that indicates the amount allocated to the segments.',\n",
       " 'Real-time tracking is an important problem in computer vision in which most methods are based on the conventional cameras. Neuromorphic vision is a concept defined by incorporating neuromorphic vision sensors such as silicon retinas in vision processing system. With the development of the silicon technology, asynchronous event-based silicon retinas that mimic neuro-biological architectures has been developed in recent years. In this work, we combine the vision tracking algorithm of computer vision with the information encoding mechanism of event-based sensors which is inspired from the neural rate coding mechanism. The real-time tracking of single object with the advantage of high speed of 100 time bins per second is successfully realized. Our method demonstrates that the computer vision methods could be used for the neuromorphic vision processing and we can realize fast real-time tracking using neuromorphic vision sensors compare to the conventional camera.',\n",
       " 'This paper explores the possibility of using computer vision for detecting deep ocean macro-organisms, based on their bioluminescent features. We use Yolo and RGB datasets to train the machine learning approach, and validate and test its efficiency. To the best of our knowledge, this is the first paper to introduce a single-stage object detection algorithm for the problem of bioluminescent macroorganism detection.',\n",
       " 'Nail painting machine is getting prevailing attention in consumer devices. A smart technique is proposed in this paper to automatically define nail printing area. This study develops a computer vision method to mark the area of nails and merge the user selected patterns. The segmented and merged result is then sent to the nail painting machine for nail printing.',\n",
       " 'The computer vision approach involves a lot of modeling problems in preventing noise caused by sensing units such as cameras and projectors. In order to improve computer vision modeling performance, a robust modeling technique must be developed for essential models in the system. The RANSAC and least median of squares (LMedS) algorithms have been widely applied in such issues. However, the performance deteriorates as the noise ratio increases and the modeling time for algorithms tends to increase in actual applications. In this study, we propose a new LMedS method based on fuzzy reinforcement learning concept for modeling of computer vision applications. The performance of the algorithm is evaluated by modeling synthetic data and camera homography experiments. Their results found the method to be effective in improving calculation time, model optimality, and robustness in modeling performance.',\n",
       " 'Computer science is involved to the greater extent in agricultural and food science these days. Many Artificial Intelligence and soft computing techniques and technologies are used for classification and defect detection of various products and thus helps in Better quality product for the end users. In this paper we focus on the standing of Arecanut in global and Indian market and usage of computer vision and image processing in an Arecanut classification and grading system. It is essential to take into consideration cultural and economic importance of Arecanut to determine the importance of computer vision technology for Arecanut. There are so many challenges to face in order to develop a system for automatic classification of Arecanut using images. Depending on the category and the region they are grown; several varieties of Arecanut are subject to significant difference in color, texture and shape. Various methods are used to process Arecanut mainly focusing on the external appearance of the product. Solution for classifying/grading Arecanut can be developed using its color, size and texture. We have also quoted the important work accomplished in respect of Arecanut from the Computer vision perspective and on some other fruits as well. The main motto of this article is to provide in-depth introduction to Arecanut, Computer Vision, need and applications of vision based technology in classification and grading of Arecanut.',\n",
       " 'Computer vision enables in-situ monitoring of animal populations at a lower cost and with less ecosystem disturbance than with human observers. However, computer vision uncertainty may not be fully understood by end-users, and the uncertainty assessments performed by technology experts may not fully address end-user needs. This knowledge gap can yield misinterpretations of computer vision data, and trust issues impeding the transfer of valuable technologies. We bridge this gap with a user-centered analysis of the uncertainty issues. Key uncertainty factors, and their interactions, are identified from the perspective of a core task in ecology research and beyond: counting individuals from different classes. We highlight factors for which uncertainty assessment methods are currently unavailable. The remaining uncertainty assessment methods are not interoperable. Hence it is currently difficult to assess the combined results of multiple uncertainty factors, and their impact on end-user counting tasks. We propose a framework for assessing the multifactorial uncertainty propagation along the data processing pipeline. It integrates methods from both computer vision and ecology domains, and aims at supporting the statistical analysis of abundance trends for population monitoring. Our typology of uncertainty factors and our assessment methods were drawn from interviews with marine ecology and computer vision experts, and from prior work for a fish monitoring application. Our findings contribute to enabling scientific research based on computer vision.',\n",
       " 'Development of autonomic chess-playing robots creates several interesting computer vision problems, including plane calibration and object recognition. Various solutions have been attempted, but most either require a modified chess set or place unreasonable constraints on board conditions and camera angles. A more general solution uses computer vision to automatically determine arbitrary chessboard location and identify chessmen on a standard, unmodified chess set. Although much work has been devoted to probabilistic image recognition in general, this paper presents a novel solution to the specific chessboard location problem that is accurate, less restrictive, and relatively time efficient.',\n",
       " 'Computer vision is not just about breaking down images or videos into constituent pixels, but also about making sense of those pixels and comprehending what they represent. Researchers have developed some brilliant neural networks and algorithms for modern computer vision. Tremendous developments have been observed in deep learning as computational power is getting cheaper. But data-driven deep learning and cloud computing based systems face some serious limitations at edge devices in real-world scenarios. Since we cannot bring edge devices to the data-centers, so we bring AI to the edge devices with AI on the Edge. OpenVINO toolkit is a powerful tool that facilitates deployment of high-performance computer vision applications to the edge devices. It converts existing applications into hardware friendly and inference-optimized deployable runtime packages that operate seamlessly at the edge. The goals of this paper are to describe an in-depth survey of problems faced in existing computer vision applications and to present AI on the Edge along with OpenVINO toolkit as the solution to those problems. We redefine the workflow for deploying computer vision systems and provide an efficient approach for development and deployment of edge applications. Furthermore, we summarize the possible works and applications of AI on the Edge in future in regard to security and privacy.',\n",
       " 'Small unmanned aerial vehicles (UAVs) have become increasingly popular in the last several years. This paper explores numerous methods to detect and track small UAVs using computer vision.',\n",
       " 'Vision-based, Human-Computer Interaction technology is mature to substitute legacy physical devices such as mouse and joystick for a number of different applications. Yet, some sort of mistrust prevents them from being used mainly because - in the past - the main focus has been on the computer vision side and not on usability, reliability and efficiency of the interface. This paper introduces a vision-based Perceptual User Interface (PUI) applied to radio-controlled (RC) mobile units. Its main effort is on shifting the research focus from basic computer vision and image processing issues to ease and naturalness of use, integrability and compatibility with the existing systems, portability and efficiency. We believe that current technologies can easily support vision-based PUIs and that PUIs are strongly needed by modern applications, but more effort is needed to merge the knowledge from HCI and computer vision communities to develop realistic and industrially appealing products. A proof-of-concept application is described and experimental results are provided and discussed.',\n",
       " 'nan',\n",
       " 'Network security has become an area of significant importance more than ever as highlighted by the eye-opening numbers of data breaches, attacks on critical infrastructure, and malware/ransomware/cryptojacker attacks that are reported almost every day. Increasingly, we are relying on networked infrastructure and with the advent of IoT, billions of devices will be connected to the Internet, providing attackers with more opportunities to exploit. Traditional machine learning methods have been frequently used in the context of network security. However, such methods are more based on statistical features extracted from sources such as binaries, emails, and packet flows. On the other hand, recent years witnessed a phenomenal growth in computer vision mainly driven by the advances in the area of convolutional neural networks. At a glance, it is not trivial to see how computer vision methods are related to network security. Nonetheless, there is a significant amount of work that highlighted how methods from computer vision can be applied in network security for detecting attacks or building security solutions. In this paper, we provide a comprehensive survey of such work under three topics; i) phishing attempt detection, ii) malware detection, and iii) traffic anomaly detection. We also discuss existing research gaps and future research directions, especially focusing on how network security research community and the industry can leverage the exponential growth of computer vision methods to build much secure networked systems. Finally, we review a set of such commercial products for which public information is available and explore how computer vision methods are effectively used in those products and conclude with a brief overview of commonly used computer vision methods in this domain.',\n",
       " 'Recent interest in developing online computer vision algorithms is spurred in part by a growth of applications capable of generating large volumes of images and videos. These applications are rich sources of images and video streams. Online vision algorithms for managing, processing and analyzing these streams need to rely upon streaming concepts, such as pipelines, to ensure timely and incremental processing of data. This paper is a first attempt at defining a formal stream algebra that provides a mathematical description of vision pipelines and describes the distributed manipulation of image and video streams. We also show how our algebra can effectively describe the vision pipelines of two state of the art techniques.',\n",
       " 'Depth inpainting is a crucial task for working with augmented reality. In previous works missing depth values are completed by convolutional encoder-decoder networks, which is a kind of bottleneck. But nowadays vision transformers showed very good quality in various tasks of computer vision and some of them became state of the art. In this study, we presented a supervised method for depth inpainting by RGB images and sparse depth maps via vision transformers. The proposed model was trained and evaluated on the NYUv2 dataset. Experiments showed that a vision transformer with a restrictive convolutional tokenization model can improve the quality of the inpainted depth map.',\n",
       " 'The paper describes an application of Computer Vision in the autonomous guidance of a traditional forklift truck, ROBOLIFT(TM), which is a product of Elsag Bailey Telerobot and FIAT OM. Computer Vision represents the main sensory system for both navigation and load recognition. The system is now shifting from the prototype stage to production and commercialization. Field tests have been carried out and results are reported.',\n",
       " 'This paper demonstrates how discrete exterior calculus tools may be useful in computer vision and graphics. A variational approach provides a link with mechanics.',\n",
       " 'In this industry-specific research topic, we foresee creating a system that would deliver a 360 degrees perspective of non-verbal and behavioural aspects into insightful contexts by recording kinesics like facial expressions, emotions, gestures, and spatial attributes throughout interview/meeting room processes conducted digitally or in person. In this research, we investigate the transfer learning training accuracy and performance measures of various pre-trained Computer Vision model architectures with newly devised hybrid end-to-end Quantum Neural Network (QNN) and evaluate hybrid QNN model inference performance for Computer Vision applications deployed in the real world.',\n",
       " 'In recent years, graphic processing units (GPUs) have emerged as an attractive alternative to CPUs for implementing algorithms in a wide range of applications. The focus of this work is to give an overview about the current state on using GPUs for computer vision. We describe briefly tools like CUDA, OpenCL and OpenACC used for GPU programming and their respective advantages / disadvantages. We give information about the current state of the art for implementing important computer vision algorithms like optical flow, KLT feature point tracking and SIFT descriptor extraction efficiently on the CPU. Finally, we describe open source frameworks which either provide CPU-accelerated computer vision algorithms or which are helpful for porting algorithms to the GPU.',\n",
       " 'The ability to automatically measure the image quality of a television display is a valuable resource in display manufacturing, competitive analysis and engineering review. In the past two decades, the advent of microcomputers has made automatic inspection feasible through the use of computer vision. Most of the approaches developed to date can be divided into two groups:fixtured systems, with fixed or movable cameras, and position independent systems, with one or more cameras. This paper will describe the motivation for using computer vision in television manufacturing, give a history of its use, classify the common methods used, and describe an experimental implementation built by the authors.',\n",
       " 'This paper delineates a vision based interface for regulating a computer mouse via 2D hand gestures. The evolution of Human Computer Interaction (HCI) has diverted the interest of researchers towards natural interaction techniques in recent years. Numerous applications of real time hand gesture based recognition in the real world have been deployed where we interact with computers. Hand gestures rely upon camera based color detection technique. This method mainly focuses on the use of a Web Camera to develop a virtual HCI device in a cost effective manner. This paper proposes a vision based system to control various mouse activities such as left and right clicking using hand gestures to make the interaction more efficient and reliable.',\n",
       " 'Take-A-Break Notification is a software which runs on Windows operating system designed for office workers who have the highest tendency on prolonged computer screens use, in order to reduce Computer Vision Syndrome (CVS). The purpose of this study is to prevent computer users from looking in front of a computer screen for a long period of time. Rapid Application Development (RAD) methodology has been used for the project development phase. The software will dim the computer, disabling the mouse and keyboard functions which will force the employees to take a 5 minutes break after 2 hours working in front of the computer screens. This software will encourage office workers to apply the ergonomic practices and to be able to reduce the increasing rate of Computer Vision Syndrome (CVS).',\n",
       " 'We present WiCV 2018 - Women in Computer Vision Workshop to increase the visibility and inclusion of women researchers in computer vision field, organized in conjunction with CVPR 2018. Computer vision and machine learning have made incredible progress over the past years, yet the number of female researchers is still low both in academia and industry. WiCV is organized to raise visibility of female researchers, to increase the collaboration, and to provide mentorship and give opportunities to female-identifying junior researchers in the field. In its fourth year, we are proud to present the changes and improvements over the past years, summary of statistics for presenters and attendees, followed by expectations from future generations.',\n",
       " 'This paper introduces an algorithm to carry out the functions of a mouse by providing a hands-free interaction between humans and computers. It provides an alternative to the traditional mouse computer. By using different expressions of a face using computer vision and matching it with already stored expression and execute actions as per the move. This algorithm will help physically disabled people to perform the functions of a mouse using their face and eye movement. It allows them to left-click, right-click, scroll up and down, move the cursor up, down, left, right. The system has a very basic need like webcam, NumPy, dlib, and a few other basic libraries.',\n",
       " 'Color and color differences are critical aspects in many image processing and computer vision applications. A paradigmatic example is object segmentation, where color distances can greatly influence the performance of the algorithms. Metrics for color difference have been proposed in the literature, including the definition of standards such as CIEDE2000, which quantifies the change in visual perception of two given colors. This standard has been recommended for industrial computer vision applications, but the benefits of its application have been impaired by the complexity of the formula. This paper proposes a new strategy that improves the usability of the CIEDE2000 metric when a maximum acceptable distance can be imposed. We argue that, for applications where a maximum value, above which colors are considered to be different, can be established, then it is possible to reduce the amount of calculations of the metric, by preemptively analyzing the color features. This methodology encompasses the benefits of the metric while overcoming its computational limitations, thus broadening the range of applications of CIEDE2000 in both the computer vision algorithms and computational resource requirements.',\n",
       " 'Lane detection is one of the most challenging problems in machine vision and still has not been fully accomplished because of the highly sensitive nature of computer vision methods. Computer vision depends on various ambient factors. External illumination conditions, camera and captured image quality etc. effect machine vision performance. Lane detection faces all these challenges as well as those due to loss of visibility, types of roads, road structure, road texture and other obstacles like trees, passing vehicles and their shadows. There are several lane detection methods having their own working principles and backgrounds, merits and demerits. We have used Receiver Operating Characteristic curve (referred to as ROC hereafter) and Detection Error Trade-off curve (referred to as DET hereafter) which establish the accuracy of computer vision methods. We have studied and analyzed several lane detection methods. The performance of two methods has been analyzed and compared using standard computer vision performance evaluation methods and it was found that method based on Canny edge detection was better than the other one based on Sobel operator. Its performance was established and presented better results when plotted using DET and ROC.',\n",
       " 'There is increasing interest in using computer vision and machine learning to enhance human decision making with computer-mediated assistive vision systems. In particular, retinal implants are a rapidly advancing technology offering individuals suffering vision loss due to retinal dystrophies, an opportunity to restore partial vision. However, the visual representations achievable with current and near-term implants are severely limited in resolution and contrast, placing high importance on the selection of visual features to convey via the implant. Using vision processing algorithms on camera-captured input, functional outcomes can be enhanced with such devices. To this end, we propose a novel end-to-end vision processing pipeline for prosthetic vision that learns task-salient visual filters in simulation offline via deep reinforcement learning (DRL). Once learnt, these filters are deployable on a prosthetic vision device to process camera-captured images and produce task-guiding scene representations in real-time. We show how a set of learnt visual features enabling a virtual agent to optimally perform the task of navigation in a 3-D environment can be extracted and applied to enhance the same features in real world images. We evaluate and validate our proposed approach quantitatively and qualitatively using simulated prosthetic vision. To our knowledge, this is the first application of DRL to the derivation of scene representations for human-centric computer-mediated displays such as prosthetic vision devices.',\n",
       " 'We present a wide area human tracking method using distributed computer vision systems. Each vision system consists of a camera and an image processor and they are all connected through a computer network. In this paper, we propose a method for human tracking and for coordination of all the vision systems. The human tracking method works on each vision system and uses a type of model based template matching to track moving people at 15 frame/sec on a standard personal computer. Coordination between the vision systems is necessary to achieve consistant wide area tracking. We use a state transition map and several action rules to synchronize the image processing between systems. All the vision systems share the state transition map jointly and decide their own actions according to the action rules. We describe experimental results that show the validity of our approach.',\n",
       " 'With the invention of the low-cost Microsoft Kinect sensor, high-resolution depth and visual (RGB) sensing has become available for widespread use. The complementary nature of the depth and visual information provided by the Kinect sensor opens up new opportunities to solve fundamental problems in computer vision. This paper presents a comprehensive review of recent Kinect-based computer vision algorithms and applications. The reviewed approaches are classified according to the type of vision problems that can be addressed or enhanced by means of the Kinect sensor. The covered topics include preprocessing, object tracking and recognition, human activity analysis, hand gesture analysis, and indoor 3-D mapping. For each category of methods, we outline their main algorithmic contributions and summarize their advantages/differences compared to their RGB counterparts. Finally, we give an overview of the challenges in this field and future research trends. This paper is expected to serve as a tutorial and source of references for Kinect-based computer vision researchers.',\n",
       " 'We describe a novel computer vision application: vision-based human sensing for a Smart Kiosk interface. A Smart kiosk is a free-standing information dispensing computer appliance capable of engaging in public interactions with multiple people. Vision sensing is a critical component of the Kiosk interface, where it is used to determine the context for the interaction. WE present a taxonomy of vision problems for a kiosk interface and describe a prototype kiosk which uses color stereo tracking and graphical output to interact with several users.',\n",
       " 'In evolutionary computer vision, algorithms are usually evolved which address one particular computer vision problem. Quite often, a set of training images is used to evolve an algorithm. Another set of images is then used to evaluate the performance of those algorithms. In contrast of this standard form of algorithm evolution, it is proposed to develop a vision system which continuously, evolves algorithms based on the task at hand. This adaptation of computer vision algorithms would happen on-line for every image which is presented to the system. Such a system would continuously adapt to new environmental conditions.',\n",
       " \"This paper discusses the role of computer vision to bridge the experiential gap between the cultural and emotional experience of the visitors in museums or cultural heritage sites. We don't argue against the use of multiple sensors to provide a more complete cultural experience but claim the primary role of computer vision for such a task. Although many research challenges are still far to be solved effectively, especially for detection, re-identification, tracking and recognition, we believe that technology can be deployed already in real contexts and support concrete applications with interesting results that will open the door to valuable future applications\",\n",
       " 'Tensors, or multidimensional arrays, are data structures that can naturally represent visual data of multiple dimensions. Inherently able to efficiently capture structured, latent semantic spaces and high-order interactions, tensors have a long history of applications in a wide span of computer vision problems. With the advent of the deep learning paradigm shift in computer vision, tensors have become even more fundamental. Indeed, essential ingredients in modern deep learning architectures, such as convolutions and attention mechanisms, can readily be considered as tensor mappings. In effect, tensor methods are increasingly finding significant applications in deep learning, including the design of memory and compute efficient network architectures, improving robustness to random noise and adversarial attacks, and aiding the theoretical understanding of deep networks. This article provides an in-depth and practical review of tensors and tensor methods in the context of representation learning and deep learning, with a particular focus on visual data analysis and computer vision applications. Concretely, besides fundamental work in tensor-based visual data analysis methods, we focus on recent developments that have brought on a gradual increase in tensor methods, especially in deep learning architectures and their implications in computer vision applications. To further enable the newcomer to grasp such concepts quickly, we provide companion Python notebooks, covering key aspects of this article and implementing them, step-by-step with TensorLy.',\n",
       " 'In this work a control system is presented for the autonomous navigation of vehicles using a monocular vision system to detect reference patterns in a controlled environment. This control system is based on the data extraction from the vision system camera feed to generate position vectors, which enable the localization of the vehicle in the environment, without having to depend purely on odometry avoiding the error that it generates. The infrastructure developed for this paper is a low cost 4-wheeled vehicle with a minimal quantity of sensors and a VGA camera for the computer vision system, all components commercially available.',\n",
       " 'Nowadays, the vision chip bridging sensing and processing has been extensively employed in high-speed image processing, owing to its excellent performance, low power consumption, and economical cost. However, there is a dilemma in designing processors to support conventional computer vision algorithms and neural networks since the two algorithms have a non-trivial trade-off in proposing a unified architecture. By analyzing computation properties, we propose a novel hierarchical parallel vision processor (ViP) for hybrid vision chips to accelerate both traditional computer vision (CV) and neural network (NN). The ViP architecture includes three parallelism levels: PE for pixel-centric, computing core (CC) for block, and vision core (VC) for global. PEs contain dedicated computing units and data paths for convolution operations without degrading its flexibility. Each CC is driven by customized SIMD instructions and can be dynamically connected for meeting block parallelism requirements. ViP is fabricated in 65nm CMOS technology and achieves a peak performance of 614.4 GOPS and an energy efficiency of 640 GOPS/W at 200 MHz clock frequency. Notably, several experiments on CV and NN are performed, illustrating an ultra-low latency in executing hybrid algorithms.',\n",
       " 'Releasing human beings from hand working has always been an target of science and technology. With the booming of Computer Vision, how to combine it with automation Artificial Intelligence better becomes a valuable topic nowadays. This thesis is written to promote the realization of automated industrial robots by means of Computer Vision and Deep Learning. And in the area of Computer Vision, Object Detection is a core issue that cannot be circumvented. How to recognize specific targets such as small elements in the vision of robots and how to distinguish their features drove the birth of Edge Detection Algorithm, which is wildly used in the area of Computer Vision out of its ability to lessen work load by efficiently sieving out irrelevant information and accurately saving those valuable information to process with. The method for Edge Detection Algorithm to work more efficiently and better has been an study heat spot. Structured Forest is used to accelerate Edge Detection Algorithm and increase the precision of it to reduce the delay time to and satisfy the industrial standard in this thesis. Besides, graphical user interface development and design is a basic skill of a computer practitioner. In order to display the whole operation result more conveniently and apply it to all kinds of platforms, the user interface named demo system was created by the prospering development tool QT. The develop process and experience of QT will be attached to this thesis.',\n",
       " \"Our society's information technology advancements have resulted in the increasingly problematic issue of information overload-i.e., we have more access to information than we can possibly process. This is nowhere more apparent than in the volume of imagery and video that we can access on a daily basis-for the general public, availability of YouTube video and Google Images, or for the image analysis professional tasked with searching security video or satellite reconnaissance. Which images to look at and how to ensure we see the images that are of most interest to us, begs the question of whether there are smart ways to triage this volume of imagery. Over the past decade, computer vision research has focused on the issue of ranking and indexing imagery. However, computer vision is limited in its ability to identify interesting imagery, particularly as interesting might be defined by an individual. In this paper we describe our efforts in developing brain-computer interfaces (BCIs) which synergistically integrate computer vision and human vision so as to construct a system for image triage. Our approach exploits machine learning for real-time decoding of brain signals which are recorded non-invasively via electroencephalography (EEG). The signals we decode are specific for events related to imagery attracting a user's attention. We describe two architectures we have developed for this type of cortically coupled computer vision and discuss potential applications and challenges for the future.\",\n",
       " \"Research into computer vision techniques has far out-paced the development of interfaces (such as APIs) to support the techniques' accessibility, especially to developers who are not experts in the field. We present a new description-based interface designed to be mainstream-developer-friendly while retaining sufficient power and flexibility to solve a wide variety of computer vision problems. The interface presents vision at the task level (hiding algorithmic detail) and uses a task-based description derived from definitions of vision problems. We show that after interpretation, the description can be used to invoke an appropriate method to provide the developer's requested result. Our implementation interprets the description and invokes various vision methods with automatically derived parameters, which we demonstrate on a range of tasks.\",\n",
       " 'Automated cars benefit greatly from millimeter wave broadband communication links. Also, computer vision is becoming more and more used in automotive applications. In this scenario, we propose a system capable to track the position of a given car on a road by fusing data from a camera system and a wireless radio link at 60 GHz. A Gaussian mixture model and epipolar geometry have been used for computer vision. From a V-band wireless link a Doppler shift estimate has been obtained. The effectiveness of our method is shown on measurement data.',\n",
       " 'We describe the development of a computer vision-based workflow for normalizing images of the legacy punchcard data format (IBM 029 - 80 column punchcard standard) and then reading the encoded data. We show the role of a newly developed Punchcard Extractor Tool within the Brown Dog service API. We also point to our showcase of these same computer vision techniques in a Jupyter notebook system.',\n",
       " 'One of the most dangerous threats on the internet nowadays is phishing attacks. This type of attack can lead to data breaches, and with it to image and financial loss in a company. The most common technique to exploit this type of attack is by sending emails to the target users to trick them to send their credentials to the attacker servers. If the user clicks on the link from the email, then good detection is needed to protect the user credentials. Many papers presented Computer Vision as a good detection technique, but we will explain why this solution can generate lots of false positives in some important environments. This paper focuses on challenges of the Computer Vision detection technique and proposes a combination of multiple techniques together with Computer Vision technique in order to solve the challenges we have shown. We also will present a methodology to detect phishing attacks that will work with the proposed combination techniques.',\n",
       " 'Speech impairment is a disability which affects an individuals ability to communicate using speech and hearing. People who are affected by this use other media of communication such as sign language. Although sign language is ubiquitous in recent times, there remains a challenge for non-sign language speakers to communicate with sign language speakers or signers. With recent advances in deep learning and computer vision there has been promising progress in the fields of motion and gesture recognition using deep learning and computer vision based techniques. The focus of this work is to create a vision based application which offers sign language translation to text thus aiding communication between signers and non-signers. The proposed model takes video sequences and extracts temporal and spatial features from them. We then use Inception, a CNN (Convolutional Neural Network) for recognizing spatial features. We then use a RNN (Recurrent Neural Network) to train on temporal features. The dataset used is the American Sign Language Dataset.',\n",
       " 'The control method of the mccanum wheel robotic car is accomplished by computer vision. The camera attached to the computer is for the purpose of image processing with open source computer vision (OpenCV). It can capture the locations of the wheel robot and the target by the use of open source computer vision as pattern recognition scheme. By using the Oriented FAST and Rotated Brief (ORB) feature, the program finds the patterns. The locations of the target and the robot are obtained by K-Nlean Clustering method. And then, the programming method is used to calculate their location difference to generate a movement command. The movement command is then sent to the microcontroller of wheel robot system by means of wireless communication using NRF radio modules. By doing so, the robotic car goes to target with the command of computer vision program. The experimental results confirmed that the performance of the control algorithm could be utilized for the real-time condition.',\n",
       " 'Detection of buried explosive threats is a challenging problem. GPR has recently become a powerful tool for achieving robust subsurface target detection, but novel target types, and large numbers of subsurface objects in rural environments significantly complicate accurate discrimination of explosive threats from harmless false alarms. Significant research in feature extraction from GPR data has previously shown the capability for improved performance. Similarly, many techniques from the computer vision literature have made significant strides in recent years in for improvements in object class recognition. This work studies the relationships between and application of feature descriptor techniques from the computer vision community in application to target detection in GPR data. Relationships between a very successful computer vision technique (Histogram of Oriented Gradients) and a related powerful technique from subsurface sensing (Edge Histogram Descriptors) are explored, and preliminary results suggest that techniques from the computer vision literature may provide robust target detection performance in GPR.',\n",
       " \"In this study we present the design of an autonomous mobile robot that navigates indoor and outdoor environments using computer vision. System's hardware, software and simulation infrastructures are explained and autonomous navigation algorithms used to reach a given target are described. Successful experimental results are obtained for computer vision algorithms like sidewalk billowing, visual localization and sidewalk detection from satellite maps, crosswalk detection and path planning. We envision that in the future such a system can be a basis for designing advanced robot systems that help people in their daily lives.\",\n",
       " 'This research presents an autonomous robotic frame work for academic, vocational and training purpose. The platform is centred on a 6 Degree Of Freedom (DOF) serial robotic arm. Two on-board cameras develop a computer vision system for detection and autonomous object/target manipulation placed randomly on a target surface and controls an educational robotic arm to pick it up and move it to a predefined destination. The computer vision is initially used to identify the colour and shape of the object. The system applies Centre-Of-Mass based computation, filtering and colour segmentation algorithm to locate the target and the position of the robotic arm. The proposed platform finds its potential to teach technical courses (like Robotics, Control, Electronics, Image-processing and Computer vision) and to implement and validate advanced algorithms for object manipulation and grasping, trajectory generation, path planning, etc. Experimental results demonstrated the effectiveness and robustness of the system.',\n",
       " 'A computer vision-based approach in identifying the pH level of a substance requires the use of multiple computer or machine vision techniques which include image processing, and object/contour detection, counting, or tracking. This paper proposes a pH level indicator system with knowledge-based systems (KBS) which has the capability of detecting, tracking, and identifying the level of a pH level indicator image. Moreover, the data that the system utilizes derives from KBS instead of being explicitly programmed. Furthermore, the study has a graphical user interface which allows the operator to easily use the system. The user may also add data to the knowledge-base, which means that the system improves over time. Compared with the traditional pH monitoring setup, the results in this study show that a computer vision-based approach is viable in determining the pH level of a substance.',\n",
       " 'This paper introduces the common methods of aero-engine defect detection, especially the detection method based on computer vision. As a hot subject, computer vision has a wide range of applications. Deep learning, as a branch of it, is also popular among scholars at home and abroad. In the future, in terms of aero-engine defects, developing automatic detection systems by deep learning will become more and more fashionable.',\n",
       " 'Smart Camera Networks (SCNs) is nowadays an emerging research field which represents the natural evolution of centralized computer vision applications towards full distributed and pervasive systems. In such a scenario, one of the biggest effort is in the definition of a flexible and reconfigurable SCN node architecture able to remotely support the possibility of updating the application parameters and changing the running computer vision applications at run-time. In this respect, this paper presents a novel SCN node architecture based on a device in which a microcontroller manages all the network functionality as well as the remote configuration, while an FPGA implements all the necessary module of a full computer vision pipeline. In the paper the envisioned architecture is first detailed in general terms, then a real implementation is presented to show the feasibility and the benefits of the proposed solution. Finally, performance evaluation results prove the potential of hardware software codesign in reaching flexibility and reduced latency time.',\n",
       " 'nan',\n",
       " 'No-Reference image quality assessment is a challenging problem of great interest to computer vision research community. This paper proposes to find a general solution to measure image quality for both human and computer vision system. A supervised deep neural network, called Deep Algorithm Quality (DAQ), is designed to blindly measure the human visual quality of benchmark images as well as to predict the performance of a computer vision algorithm on distorted images. The performance of the proposed DAQ is evaluated using two sets of experiments. The first experiment formulates DAQ as an image quality estimator, and evaluates the performance on general image quality assessment benchmarks. DAQ has achieved highest LCC and SROCC scores compare to five state-of-the-art image quality assessment methods. In the second experiment, DAQ is trained to work as a failure detector to predict the detection performance of another predefined computer vision algorithm working on distorted images.',\n",
       " 'In this paper we present the Women in Computer Vision Workshop - WiCV 2019, organized in conjunction with CVPR 2019. This event is meant for increasing the visibility and inclusion of women researchers in computer vision field. Computer vision and machine learning have made incredible progress over the past years, but the number of female researchers is still low both in the academia and in the industry. WiCV is organized especially for this reason: to raise visibility of female researchers, to increase collaborations between them, and to provide mentorship to female junior researchers in the field. In this paper, we present a report of trends over the past years, along with a summary of statistics regarding presenters, attendees, and sponsorship for the current workshop.',\n",
       " \"This paper describes the development of efficient computer vision techniques for human-computer interaction. Our approach combines range and color information to achieve efficient, robust tracking using consumer-level computer hardware and cameras. In this paper, we present the design of the Human Oriented Tracking (HOT) library, present our initial results, and describe our current efforts to improve HOT's performance through model-based tracking.\",\n",
       " 'Teaching System Integration in Engineering Curricula at Universities via Popular and Effective Robot-Design Competitions.',\n",
       " 'This paper presents a novel method for simultaneous human detection and 3D shape reconstruction from a single RGB image. It offers a low-cost alternative to existing motion capture solutions, allowing to reconstruct realistic human 3D shapes and poses by leveraging the speed of an object-detection based architecture and the extended applicability of a parametric human mesh model. Evaluation results using a synthetic dataset show that our approach is on-par with conventional 3D reconstruction methods in terms of accuracy, and outperforms them in terms of inference speed, particularly in the case of multi-person images.',\n",
       " 'nan',\n",
       " 'This paper presents a system to monitor the wear process in machines using computer vision and image processing techniques applied to wear particle analysis. Particles are classified using their visual attributes to predict wear failure modes in engines and other machinery. The aim of the current work is to develop an automated system to classify wear particles and thereby predict wear failure modes in engines and other machinery, such that it obviates the need for specialists and reliance on human visual inspection techniques. The paper describes an interactive control system CAVE (Computer Aided Vision Engineering) in terms of the stages involved in processing data to acquire morphological features of wear particles from microscopic images and their automatic classification.',\n",
       " 'Hardware support for deep convolutional neural networks (CNNs) is critical to advanced computer vision in mobile and embedded devices. Current designs, however, accelerate generic CNNs; they do not exploit the unique characteristics of real-time vision. We propose to use the temporal redundancy in natural video to avoid unnecessary computation on most frames. A new algorithm, activation motion compensation, detects changes in the visual input and incrementally updates a previously-computed activation. The technique takes inspiration from video compression and applies well-known motion estimation techniques to adapt to visual changes. We use an adaptive key frame rate to control the trade-off between efficiency and vision quality as the input changes. We implement the technique in hardware as an extension to state-of-the-art CNN accelerator designs. The new unit reduces the average energy per frame by 54%, 62%, and 87% for three CNNs with less than 1% loss in vision accuracy.',\n",
       " 'Computer vision has historically been taught as a graduate subject since few examples of the discipline were being practiced in mainstream engineering. In recent years, the incorporation of multimedia into embedded devices has drawn some vision topics into mainstream attention. Examples of consumer products include digital video recorders, cellular phones, and automobile collision-avoidance systems. This paper describes the development of an undergraduate course that. incorporates some vision topics into the larger context of embedded computing. Traditional topics, such as processor types, dynamic power management, and real-time scheduling, are taught alongside relevant vision topics, such as codecs, concurrent interfaces, and multimedia signal acquisition, storage, and rendering. In lab work, the students program hardware to operate as a digital video camera. While the primary goal for the course is to teach embedded computing, a secondary goal for the course is to entice students into graduate study in computer vision. However, a major developmental point was to justify the vision content in the context of how it serves the needs of students not opting for graduate study, as well as how the course would impact students working in other related graduate research areas.',\n",
       " 'This paper introduces Embedded Vision Engine (EVE) - a fully programmable, specialized vector processor architecture aimed at solving challenging Computer Vision applications encountered in Advanced Driver Assistance Systems (ADAS). The paper outlines the complexity of automotive vision applications, establishes why specialized architecture (like EVE) is needed and outlines the EVE architecture, its components and programming model. We present comparative benchmarks and provide an overview of many carefully crafted features of EVE for power management, inter processor communication, functional safety and software debug that helps in building a scalable, area-power efficient System-on-Chip (SoC) solutions for the cost, power and safety sensitive automotive vision space.',\n",
       " 'Roadbed settlement, we generally used method of ceramic micro pressure sensor or silicon micro pressure sensor measuring the pressure difference of the water produced in the settling tube. In order to overcome the deficiency of existing techniques, the Paper proposed a novel effective method using laser and embedded computer vision to measure the deviation from the laser spot center. Compared to traditional techniques, it has advantages of simple, low-cost and easy deployment.',\n",
       " 'The autonomous robotic system accurate localization is a challenging step in robot navigation field once the mobile device should avoid dangerous situations, such as unsafe conditions and collisions. In this context, the present paper proposes a localization method using the Extended Kalman Filter (EKF) to fuse the information coming from two different sensors (i.e. odometry and computer vision). The localization results present with known and unknown starting points and are tested in a simulated environment.',\n",
       " 'Deep learning has been successfully used for computer vision tasks, but its high computational cost limits the adoption in lightweight devices such as camera sensors. For this reason, many low-latency vision systems offload the inference computation to a local server, requiring fast (de)compression of the source images. Texture compression is a compelling alternative to existing compression schemes, such as JPEG or HEVC, due to its low decoding overhead, straightforward parallelization, robustness, and a fixed compression ratio. In this paper, we study the impact of lightweight bounding box-based texture compression algorithms, BC1 and YCoCg-BC3, on the accuracy of two computer vision tasks: object detection and semantic segmentation. While JPEG achieves superior per-pixel error rate, the YCoCg-BC3 encoding can provide comparable vision accuracy. The BC1 encoding results in significant degradation of vision performance. However, by retraining the FasterSeg teacher network with a BC1-compressed dataset, we reduced its segmentation mIoU loss from 2.7 to 0.5 percent. Thus, both BC1 and YCoCg-BC3 encoders are suitable for use in low latency vision systems, since they both achieve significantly higher encoding speed than JPEG and their decoding overhead is negligible.',\n",
       " 'With the rapid development of computer vision technology and the wide use of cameras, vision-based positioning has become a new research hot spot. Higher accuracy, lower cost, wider range of applications and some other advantages make vision-based positioning technology more promising than traditional positioning methods. This paper gives a briefly introduction of different methods of positioning firstly. Then we discuss different technologies of vision-based positioning techniques and their core algorithms. In addition, we compared the advantages and disadvantages of different algorithms. Finally, we investigate different applications of vision-based positioning and analyze the existing problems and possible development trends.',\n",
       " 'As one of the most basic and challenging core tasks in computer vision, target detection has been a hot research field in the world. Firstly, this paper summarizes the appearance and development of obstacle perception research methods based on vision at home and abroad. And the historical development process and branches of target detection are introduced in detail. Finally, summarize the current development.',\n",
       " 'The current investigation of aphid population dynamics mostly adopts manual counting, which causes the enormous workload. The accuracy is also hard to guarantee. But the computer vision technology used in this experimentation can automatically count aphids. In the experimentation, the aphids can be apart from soybean leaves by proposing the images with the HSI color system. Meanwhile the algorithm is given. The results showed that this method counts aphids accurately, rapidly and effectively.',\n",
       " 'Face as a biometric identification in computer vision is an important medium, in areas such as video surveillance, animation games, security anti-terrorist has a very wide range of applications, creating vivid, strong visibility of 3d face model, now has become a challenging in the field of computer vision is one of the important topics. At first, this paper used the zhongxing-micro ZC301P cameras to build a binocular stereo vision system for recording images. After the camera calibration and binocular calibration, the three-dimensional data of facial images were extracted using the functions of OpenCV computer vision library, and then 3d face model were reconstructed preliminary by DirectX. According the reconstruction process, the human face three-dimensional reconstruction software was designed and developed. The paper laid the foundation for the next step work that is to obtain more clear and strong visibility of 3d face.',\n",
       " 'Extracting good representations from POLSAR images is essential for many computer vision tasks. In this paper, we propose hierarchical matching pursuit (HMP), which builds a feature hierarchy layer-by-layer using an efficient matching pursuit encoder. The algorithm proposed in this paper, was applied on a POLSAR image and the result demonstrates that the extracted features using the proposed computer vision algorithm is an effective and useful method for classification of POLSAR images.',\n",
       " 'This paper presents a device that will implement scanning technology in a new way. The device will utilize techniques used in computer vision algorithms for object scanning and replication. The device uses edge detection, triangulation, and clustering to create a point cloud which will form a precise model for 3D printing. The data organization and file creation will be completed through a developed encoder which converts raw output into a desired file format.',\n",
       " 'This paper introduces a method to compensate for shadow in a computer vision system for a robot soccer team. For a non-uniform lighting, a simple color classification based on a fixed threshold will fail. The shadow compensation method uses a lighting model to perform the color classification based on the albedo ratio. The system has been tested on the robot soccer team that participates in Robocup 2005. The system performed well with improving percentage of the correct classification.',\n",
       " 'In this paper, design and implementation of an interactive open architecture computer vision software package called Ch OpenCV is presented. Benefiting from both Ch and OpenCV, Ch OpenCV has many salient features. It is interactive, capable of interface with binary static or dynamical C/C++ libraries, integrated with advanced numerical features and embeddable. It is especially suitable for rapid prototyping, web-based applications, and teaching and learning about computer vision. Applications of Ch OpenCV including Web-based image processing are illustrated with examples.',\n",
       " 'Human-autonomy sensor fusion is an emerging technology with a wide range of applications, including object detection/recognition, surveillance, collaborative control, and prosthetics. For object detection, humans and computer-vision-based systems employ different strategies to locate targets, likely providing complementary information. However, little effort has been made in combining the outputs of multiple autonomous detectors and multiple human-generated responses. This paper presents a method for integrating several sources of human- and autonomy-generated information for rapid object detection tasks. Human electroencephalography (EEG) and button-press responses from rapid serial visual presentation (RSVP) experiments are fused with outputs from trained object detection algorithms. Three fusion methods-Bayesian, Dempster-Shafer, and Dynamic Dempster-Shafer-are implemented for comparison. Results demonstrate that fusion of these human classifiers with computer-vision-based detectors improves object detection accuracy over purely computer-vision-based detection (5% relative increase in mean average precision) and the best individual computer vision algorithm (28% relative increase in mean average precision). Computer vision fused with button press response and/or the XDAWN + Bayesian Linear Discriminant Analysis neural classifier provides considerable improvement, while computer vision fused with other neural classifiers provides little or no improvement. Of the three fusion methods, Dynamic Dempster-Shafer Theory (DDST) Fusion exhibits the greatest performance in this application.',\n",
       " 'This paper examines how human emotion, which is often expressed by face expression, could be recognized using computer vision. The study is performed by analyzing journals and researches related to this topic, ranging from psychological to technological journals. A number of algorithms and techniques have been reviewed, and at the end of this paper, a summary of recommendation for performing facial emotion recognition based on the reviews of the techniques/methods is given.',\n",
       " 'Quality control is a key step in any manufacturing process in the industry. However, it is still an unexplored area in soft robotics, especially in the production of soft actuators. This article proposes a quality control method based on a series of specific tests and analyzes using computer vision to verify the performance of these actuators and identify possible manufacturing defects. Facilitating automation and faster implementation of quality control will help bring this technology closer to industrialization.',\n",
       " 'Coin detection and recognition applications play an important role in computer vision and in industry. Many applications have been developed to detect coins and estimate its corresponding values either by camera picture or mobile devices. This paper proposes a system that uses number of computer vision techniques to detect and recognize coins applied to Saudi riyal currency and returns its estimated values. The main goal goes around differentiating between different division of the same currency.',\n",
       " 'This paper describes a global framework that enables contactless human machine interaction using computer vision and machine learning techniques. The main originality of our framework is that only a very simple image acquisition device, as a computer camera, is sufficient to establish a rich human machine interaction as traditional devices such as mouse or keyboard. This framework is based on well known computer vision techniques and efficient machine learning techniques are used to detect and track user hand gestures so the end user can control his computer using virtual interfaces with very simple gestures.',\n",
       " 'Iris localization is an active area and complicated task in computer vision systems. This article presents a description of an eye tracking algorithm based on Hough transform.',\n",
       " 'The appearance of generative adversarial networks (GAN) provides a new approach and framework for computer vision. Compared with traditional machine learning algorithms, GAN works via adversarial training concept and is more powerful in both feature learning and representation. GAN also exhibits some problems, such as non-convergence, model collapse, and uncontrollability due to high degree of freedom. How to improve the theory of GAN and apply it to computer-vision-related tasks have now attracted much research efforts. In this paper, recently proposed GAN models and their applications in computer vision are systematically reviewed. In particular, we firstly survey the history and development of generative algorithms, the mechanism of GAN, its fundamental network structures, and theoretical analysis of the original GAN. Classical GAN algorithms are then compared comprehensively in terms of the mechanism, visual results of generated samples, and Frechet Inception Distance. These networks are further evaluated from network construction, performance, and applicability aspects by extensive experiments conducted over public datasets. After that, several typical applications of GAN in computer vision, including high-quality samples generation, style transfer, and image translation, are examined. Finally, some existing problems of GAN are summarized and discussed and potential future research topics are forecasted.',\n",
       " 'Potential of neuromorphic circuits on FDSOI technology for computer vision applications is demonstrated in this paper. Computer vision systems based on conventional Von Neumann architecture consume large area and energy. The FDSOI inverter-based circuits proposed in this work, require only 11 transistors per pixel for colour detection, and only 59 transistors per pixel for erosion and dilation operations, whereas the CMOS-based Boolean circuit requires more than 300 transistors per pixel, and 2700 transistors per pixel, respectively, for these operations.',\n",
       " 'Binocular stereo vision is an important aspect in computer vision field. In this paper, it is made a systematic research, which is the system of human-computer interaction based on bincular stereo vision. In order to improve the anti-interference performance the hardware system worked in the condition of near infrared ; to inhance the real time operation , the level of difficulty with stereo matching algorithm is simplified through special hardware design; based on the ICP algorithm for tracking tools, the flexibility and convenience of system are increasingly. The application in surgical navigation system is discussed. In the prototype system, how to unified virtual coordinates and real coordinates has been presented; the problem on how to tracking patient undergoing in real-time is solved. The result has higher precision and practicability, which provide a good platform for virtual reality.',\n",
       " \"The productivity and profitability of poultry farming are crucial to support its affordability issues in food security. Criteria in productivity measurement, including Feed Conversion Ratio (FCR) calculation, whereas economic management is essential for profitability. Hence, best management practices need to be implemented throughout the growth period for optimizing the poultry performance. This review provides a comprehensive overview of computer vision technology for poultry industry research. This review relies on the use of several online databases to identify key works in the area of computer vision in a poultry farm. We recommend our search by focusing on four keywords, 'computer vision' and 'poultry' or 'chicken' or 'broiler' that had been published between 2010 and early 2020 with open access provided by University Teknologi Malaysia only. All the selected papers were manually examined and sorted to determine their relevance to computer vision in a poultry farm. We focus on the latest developments by focusing on the hardware and software parts used to analyze the poultry data with some examples of various representative studies on poultry farming. Notably, hardware parts can be classified into camera types, lighting units and camera position, whereas software parts can be categorized into data acquisition and analysis software types as well as data processing and analysis methods that can be implemented into the software types. This paper concludes by highlighting the future works and key challenges that needed to be addressed to assure the quality of this technology prior to the successful implementation of the poultry industry.\",\n",
       " 'Event-based image processing is a relatively new domain in the field of computer vision. Much research has been carried out on adapting event-based data to comply with established techniques from frame-based computer vision. On the contrary, this paper presents a descriptor which is designed specifically for direct use with event-based data and therefore can be considered to be a pure event-based vision descriptor as it only uses events emitted from event-based vision devices without transforming the data to accommodate frame-based vision techniques. This novel descriptor is known as the Post-stimulus Time-dependent Event Descriptor (P-TED). P-TED is comprised of two features extracted from event data which describe motion and the underlying pattern of transmission respectively. Furthermore a framework is presented which leverages the P-TED descriptor to classify motions within event data. This framework is compared against another state-of-the-art event-based vision descriptor as well as an established frame-based approach.',\n",
       " 'With the rapid development of computer technology, image processing technology is significantly improved. In this paper, a method of moving target tracking based on computer vison is studied. By combining with the results of the Marr image research, we first introduce the framework of computer vision theory via a bottom-up visual tracking processing method. To be specific, we take the rocket as an example. Consecutively applying the segmentation algorithm and the tracking algorithm, we find that the accuracy of separation and the instantaneity of image can be enhanced, which makes a significant contribution to the target tracking technology.',\n",
       " 'In everyday life human gestures are often used to communicate or enhance speech. They can be also used to enable or improve the communication between human and machine. Among the contactless human-computer interfaces (HCI) the vision-based solutions enabling for face and hand gesture recognition are the most promising ones. Haar-like object detection algorithm developed by Viola and Jones allows for rapid detection of human faces or hands in image sequences. This paper presents the overview of vision-based Human-Computer interfaces employing methods based on Haar-like features and proposes a Human-Computer Interaction system controlled by mouth shape change.',\n",
       " \"This paper describes the research, development and evaluation process of a solution based on computer vision for the detection and prevention of Computer Vision Syndrome, a type of eye fatigue characterized by the appearance of ocular symptoms during or after prolonged periods watching digital screens. The system developed targets users of computers and mobile devices, detecting and warning users to the occurrence of eye fatigue situations and suggesting corrective behaviours in order to prevent more complicated health consequences. The implementation resorts to machine learning techniques, using eye images datasets for training the eye state detection algorithm. OpenCV Lib was used for eye's segmentation and subsequent fatigue analysis. The final goal of the system is to provide users and health professionals with quality data analysis of eye fatigue levels, in order to raise awareness over accumulated stress and promote behaviour change.\",\n",
       " 'The complexity of hand function is such that most existing upper limb rehabilitation robotic devices use only simplified hand interfaces. This is in contrast to the importance of the hand in regaining function after neurological injury. Computer vision technology has been used to identify hand posture in the field of Human Computer Interaction, but this approach has not been translated to the rehabilitation context. We describe a computer vision-based classifier that can be used to discriminate rehabilitation-relevant hand postures, and could be integrated into a virtual reality-based upper limb rehabilitation system. The proposed system was tested on a set of video recordings from able-bodied individuals performing cylindrical grasps, lateral key grips, and tip-to-tip pinches. The overall classification success rate was 91.2%, and was above 98% for 6 out of the 10 subjects.',\n",
       " 'An approach of reading analog multimeter based on computer vision technique is proposed in this paper. This method can detect not only the angle of needle-type pointer but also that of selector switch for identifying the measured value of the selected function from the analog multimeter image. A vision system of reading the linear scale of DCV, ACV, DCmA, and the nonlinear scale of resistance from an analog multimeter were developed for experiments and validating the proposed approach under various scenarios of different illuminations and shifted/tilted situations. Our experiments confirm the feasibility of the proposed approach.',\n",
       " '3D measurement in computer vision is to determine attitude of an object with respect to camera. Environments exist a lot of straight line features. New 3D measurement method of single camera from four coplanar line correspondences is presented in this paper. This algorithm is linear to solve. Simulation results show this method is feasible.',\n",
       " 'New programmable hardware architectures and much more efficient algorithm development environment tools are needed to meet the explosive growth and diverse application scenarios anticipated for computer vision technology over the next 5 years in both the military and commercial application arenas. Needed are programmable computer architectures that provide much higher cost effectiveness through scalability of all hardware resources (i.e., I/O bandwidth, throughput, memory) and through exploitation of the unique requirements and characteristics of image processing algorithms and applications. In addition, a new class of software development tools is needed that can streamline and improve the efficiency of the algorithm development process, as well as provide a concise, platform-independent, very-high-level notation for computer vision algorithms that encourages iterative and collaborative developments. Progress in developing new tools and technology for military and commercial computer vision applications by the Parallel Algebraic Logic (PAL) consortium is described.',\n",
       " 'Experience with the application of a low cost vision system to an elective course in mechatronics is discussed. The course is structured around a series of laboratories involving a mobile robot. The course ends with a team based design project competition. A camera with embedded color tracking capabilities was added to the course to provide non-electrical engineering students with exposure to computer vision technology.',\n",
       " 'Conventional input devices such as the mouse and keyboard are unnatural and limited for various forms of human-computer interaction. We have created a tangible user interface that mixes the physical world with a virtual reality billiards game. Our system uses computer vision techniques to analyze images acquired from a single inexpensive digital video camera in real-time in order to passively sense the full 3D pose of a markerless cue stick. A physically-modeled billiards game updates the virtual cue stick to mimic detected poses and shots. Our system affords users with a convenient, noninvasive, inexpensive, and natural interface for entertainment or training without requiring a billiards table installation. Moreover, our system is robust to changes in illumination and occlusion, and is noteworthy for its use of computer vision as the sole means of sensing user input.',\n",
       " 'Computer Based Perimetry system has been designed and manufactured which features the patient gaze fixation system. The results obtained from diagnosing various patients were compared to those obtained by internationally approved perimetery systems. This shows that our system is reliable as a standard perimetry.',\n",
       " 'Computer vision based spectroscopy is feasible due to the advent of faster cameras and computers. Here, spectroscopy means detecting elements based on the intensity patterns of laser induced breakdown spectra seen by a camera. This is conventionally done by using specific instruments-spectroscopes-which too may use CCD devices as the sensors. Using a commercial camera reduces the cost of the product, while enabling the testing of new detection algorithms. A simulation setup is helpful in this context. The computer vision based spectroscopy simulation, which we propose here, involves displaying patterns on a computer screen corresponding to a reduced standard database of spectra, observing the screen using a camera, and detecting the elements, preferably on a different computer. We show that, using markers and computing homographies, correct alignment of the scene and image planes can be achieved to detect-the element. This set up can be used to test detection algorithms, and we apply a simple correlation based algorithm for pattern detection.',\n",
       " 'To date, computer vision systems are limited to extract the digital data of what the cameras see. However, the meaning of what they observe could be greatly enhanced by considering the environment and common-sense knowledge. A new approach to combine computer vision with semantic modeling has been developed. This approach extracts the knowledge from images and uses it to perform real-time reasoning according to the contextual information, events of interest and logic rules. The reasoning with image knowledge allows protecting the privacy of the users, to overcome some problems of computer vision such as occlusion and missed detections and to offer services such as people guidance and people counting. This approach is the first step to develop an all-seeing smart building that can automatically react according to its evolving information.',\n",
       " 'Distributed smart cameras (DSCs) are real-time distributed embedded systems that perform computer vision using multiple cameras. This new approach has emerged thanks to a confluence of simultaneous advances in four key disciplines: computer vision, image sensors, embedded computing, and sensor networks. Processing images in a network of distributed smart cameras introduces several complications. However, we believe that the problems DSCs solve are much more important than the challenges of designing and building a distributed video system. We argue that distributed smart cameras represent key components for future embedded computer vision systems and that smart cameras will become an enabling technology for many new applications. We summarize smart camera technology and applications, discuss current trends, and identify important research challenges.',\n",
       " \"Computer vision systems have become a key element of modern companies. However, its implantation not only requires specialized hardware components but also software to control the correct performance of all these components. The purpose of this article is to present the design and deployment of a software created to manage the different computer vision systems of any company. First, using our experience in mechanization and industrialization of company processes, a general company specification will be proposed as well as the main software requirements that have to be satisfied. Then, a three-level modular design composed of the core, a configuration module, and user interfaces with functionalities capable of satisfying the defined requirements will be presented. Special attention will be given to time restrictions and components' synchronization. In addition, the different tests that have been carried out to control the correct performance of the software will be shown. The development process will end with a generic, modular, and scalable software able to fit different industrial scenarios by simply modifying a set of input parameters. To illustrate the correct performance of the proposal, the details of its installation in four real companies with different needs will be presented. The proposed work has a practical use in industry and it also provides a thorough description of the main components involved in computer vision systems of real company environments and how to manage them.\",\n",
       " 'The paper describes an application of Computer Vision in the autonomous guidance of a traditional forklift truck, ROBOLIFT(TM) which is a product of Elsag Bailey Telerobot and FIAT OM. Computer Vision represents the main sensory system for both navigation and load recognition. Artificial H-shaped landmarks placed along the truck path are used for the vehicle pose detection. Standard Europallets, supporting the loads to be transported, are recognized by the Vision system in order to provide their estimated pose to the automatic fork control subsystem. The system is now tested in real industrial applications and preliminary results are reported.',\n",
       " 'As computer vision based systems like. lane tracking, face tracking and obstacle detection mature an enhanced range of driver assistance systems are becoming feasible. This paper introduces a list of core competencies required for a driver assistance system, the issue of building in robustness is highlighted in contrast to leaving such considerations to a later Product development phase. We then demonstrate how these issues may be addressed in driver assistance systems based primarily on computer vision. The underlying computer vision systems are discussed followed by an example of a driver support application for lane keeping based on force-feedback through the steering wheel.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'This article develops a prototype quadcopter drone-based system for inspection of power-line ceramic insulators. The drone uses its onboard cameras and Raspberry Pi single-board computer to monitor the health condition of outdoor ceramic insulators. The main contribution of this article is the development of a complete quadcopter-based system prototype for overhead power-line ceramic insulators inspection. The system is capable of performing the required computer vision routines for insulator health monitoring, either onboard or on an onshore ground station computer. In the onshore mode of operation, the drone captures images as it flies and simultaneously sends them to the onshore ground station. The developed system is tested in real life on a small-scale model frame, on which insulators are mounted. The results presented in this article show that quadcopter-based insulator inspection can be carried out successfully using both onshore and onboard computer vision techniques, with acceptable quality in terms of precision and computer vision time.',\n",
       " \"The commercial drone market is booming. However ubiquitous drones have raised great concern about people's privacy. Computer vision algorithms may disclose details of human activities on the ground from the videos taken by drones. We may mitigate the threats from adversarial computer vision by exploring the limitations of computer vision techniques. In this paper, we propose to mitigate the drone security and privacy threats through regulating the drones altitude and its on-board camera capability. We systematically study the factors that can affect the effectiveness of drone attacks given the camera specification, drone altitude and drone dynamics. We validate our theoretical analysis by evaluating two scenarios, password inference and facial recognition. It can be observed that regulating drone altitude and camera capability shall preserve human privacy on the ground.\",\n",
       " 'nan',\n",
       " 'A graphical integrated development environment (IDE) for computer vision applications allows developing solutions by composing graphical widgets that represent operators of a computer vision library. A challenge in developing such IDE is the development of a graphical interface for each operator in the library, which is a slow and repetitive task. In this paper, we propose to generate a specific graphical widget editor for the input parameters of each operator, based directly on the library documentation. Our approach allows reducing significantly the development time of an IDE. The only assumption of the proposed approach is that the documentation has a structured format. We validated our approach by integrating the computer vision library Halcon in an IDE, using only its HTML documentation.',\n",
       " \"As video-based sensor networks continue to scale and become more ubiquitous, it is becoming increasingly important to focus systems research on techniques that support content-based decisions in real-time towards the edge of the network. While some prior work has focused on high-level image and video quality's effect on computer vision (e.g., object recognition). We are unaware of any work that focuses on the low-level details of why. This paper explores the impact of compression on underlying computer vision techniques. Specifically, this paper focuses on understanding the fundamental impact of compression on SIFT feature detection and matching. We show how reduced resolution or frame quality can negatively impact feature detection and tracking.\",\n",
       " 'Neural networks are used for many real world applications, but often they have problems estimating their own confidence. This is particularly problematic for computer vision applications aimed at making high stakes decisions with humans and their lives. In this paper we make a meta-analysis of the literature, showing that most if not all computer vision applications do not use proper epistemic uncertainty quantification, which means that these models ignore their own limitations. We describe the consequences of using models without proper uncertainty quantification, and motivate the community to adopt versions of the models they use that have proper calibrated epistemic uncertainty, in order to enable out of distribution detection. We close the paper with a summary of challenges on estimating uncertainty for computer vision applications and recommendations.',\n",
       " \"In this paper, we introduce an interactive whiteboard system using image processing and computer vision techniques. This system is more portable than other similar interactive systems, because the system only needs a computer, laser pointer and a camera to communicate with the user. Using a standard laser pointer, user can send his/her commands to the whiteboard, and the system tracks the light and interpret the commands using a video camera and a software based on machine vision algorithms, then user's commands transmitted to the computer and system executes necessary operations. User's command are the normal mouse activities including clicking on the various menu items and executing certain operations such as zoom or rotate of pictures and so on.\",\n",
       " 'Computer vision allows for computer systems to see, react and interact with the environment in which they are deployed. Current computer vision systems are based upon aging identification techniques such as blob and feature detection which can no longer be used in isolation to reliably identify actors within a given environment due to their two-dimensional limitations. Stereovision is becoming a viable computer vision technology for access control systems due to the growing processing power available for image processing that enables the use of multiple cameras to compute the third dimension of depth to aid in detection of objects within physical spaces. This paper evaluates the current state of computer vision with respect to access control for physical spaces and draws up a set of requirements for effective access control to be enforced using stereovision as a basis for the computer vision aspects of subject identification on the African continent. This paper then proposes an access control model for physical spaces based upon the three-dimensionality of stereovision to provide a more robust and reliable form of identification for actors within a system whereby access to physical locations should be limited.',\n",
       " 'nan',\n",
       " 'The application of PLC and computer vision technology was introduced in steel-tape automatic measurement system. The content of this paper was discussed by following: the theory of the system, the technology of automatic aiming at zero position, the servo motor controlling based on PLC, automatic image recognition, detection and the communication between PLC and computer. Practices strongly proved that the system not only meet the National steel-tape measurement Standard(JJG 4-1999), but also realized the measurement of multi-target and retain the higher measurement accuracy rapidly.',\n",
       " 'Computer vision systems are increasingly used for the early detection of skin diseases, such as malignant melanoma. The various proposals of computer vision systems are characterized by some fundamental common phases: image acquisition, pre-processing, segmentation, features extraction and selection and finally classification. In most of the related papers dealing with this topic, many features are extracted in order to feed classifiers from the simplest to the most sophisticated. Features are typically extracted using digital image processing methods (i.e., segmentation, edge detection and color and structure processing), and an open discussion about the meaning of these features and the objective ways of measuring them is ongoing. Therefore, the need to investigate this topic in order to find a guideline to support new researchers on these issues arises. The present work is a not exhaustive review of the most frequently used features in the elaboration of computer vision systems. The shortcomings in some of the existing studies are highlighted and suggestions for future research are provided.',\n",
       " 'Real time computer vision applications like video streaming on cell phones, remote surveillance and virtual reality have stringent performance requirements but can be severely restrained by limited resources. The use of optimized algorithms is vital to meet real-time requirements especially on popular mobile platforms. This paper presents work on performance optimization of common computer vision algorithms such as correlation on such embedded systems. The correlation algorithm which is popular for face recognition, can be implemented using convolution or the Discrete Fourier Transform (DFT). The algorithms are benchmarked on the Intel Pentium processor and Beagleboard, which is a new low-cost low-power platform based on the Texas Instruments (TI) OMAP 3530 processor architecture. The OMAP processor consists of an asymmetric dual-core architecture, including an ARM and a DSP supported by shared memory. OpenCV, which is a computer vision library developed by Intel corporation was utilized for some of the algorithms. Comparative results for the various approaches are presented and discussed with an emphasis on real-time implementation.',\n",
       " 'nan',\n",
       " \"Computing advances and increased smartphone use gives technology system designers greater flexibility in exploiting computer vision to support visually impaired users. Understanding these users' needs will certainly provide insight for the development of improved usability of computing devices.\",\n",
       " \"The paper presents a state-of-the-art analysis of modern drowsiness detection algorithms based on computer vision technologies as well as considers the problem of yawning detection for the vehicle driver. Based on the analysis of the literature we classic' drowsiness detection techniques into three groups: the driving pattern of the vehicle; psychophysiological characteristics of drivers; and computer vision techniques for driver monitoring. The computer vision methods look most promising since they are non-intrusive for the driver. The importance of the driver drowsiness monitoring system rises from the number of drowsiness-related accidents. Yawning is an important identifier of drowsiness, even though it is not the most reliable drowsiness indicator. Some of the methods that are based on computer vision are presented and discussed in the paper. We developed and evaluated a yawning detection model. We analyzed available datasets for yawning detection and conclude that the existing datasets have to be enhanced by pictures taken in real driving conditions. We propose yawning detection dataset-preparation as well as detection model development and evaluation.\",\n",
       " 'This article discusses the development of a technology of computer vision designed to assist medical practitioners diagnosing breast cancer in the 1980s and 1990s. A CAD system was designed to augment human vision by digitizing mammograms, enhancing computer-selected regions of interest and offering a protocol to recommend a course of action (follow up examination, biopsy, etc.). One issue that emerged following the introduction of CAD was that human vision-previously the gold standard for diagnostic accuracy-was influenced by the prompts the computer provided the interpreter, illuminating the paradoxes of de-skilling and the problems mediating visualization and expert decision making.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Deep Learning methods are currently the state-of-the-art in many Computer Vision and Image Processing problems, in particular image classification. After years of intensive investigation, a few models matured and became important tools, including Convolutional Neural Networks (CNNs), Siamese and Triplet Networks, Auto-Encoders (AEs) and Generative Adversarial Networks (GANs). The field is fast-paced and there is a lot of terminologies to catch up for those who want to adventure in Deep Learning waters. This paper has the objective to introduce the most fundamental concepts of Deep Learning for Computer Vision in particular CNNs, AEs and GANs, including architectures, inner workings and optimization. We offer an updated description of the theoretical and practical knowledge of working with those models. After that, we describe Siamese and Triplet Networks, not often covered in tutorial papers, as well as review the literature on recent and exciting topics such as visual stylization, pixel-wise prediction and video processing. Finally, we discuss the limitations of Deep Learning for Computer Vision.',\n",
       " 'This paper describes a framework which uses augmented reality for evaluating the performance of mobile computer vision systems. Computer vision systems use primarily image data to interpret the surrounding world, e. g to detect, classify and track objects. The performance of mobile computer vision systems acting in unknown environments is inherently difficult to evaluate since, often, obtaining ground truth data is problematic. The proposed novel framework exploits the possibility to add virtual agents into a real data sequence collected in an unknown environment, thus making it possible to efficiently create augmented data sequences, including ground truth, to be used for performance evaluation. Varying the content in the data sequence by adding different virtual agents is straightforward, making the proposed framework very flexible. The method has been implemented and tested on a pedestrian detection system used for automotive collision avoidance. Preliminary results show that the method has potential to replace and complement physical testing, for instance by creating collision scenarios, which are difficult to test in reality.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'This work presents Kornia - an open source computer vision library which consists of a set of differentiable routines and modules to solve generic computer vision problems. The package uses PyTorch as its main backend both for efficiency and to take advantage of the reverse-mode auto-differentiation to define and compute the gradient of complex functions. Inspired by OpenCV, Kornia is composed of a set of modules containing operators that can be inserted inside neural networks to train models to perform image transformations, camera calibration, epipolar geometry, and low level image processing techniques, such as filtering and edge detection that operate directly on high dimensional tensor representations. Examples of classical vision problems implemented using our framework are provided including a benchmark comparing to existing vision libraries.',\n",
       " 'The extensive use of modern intelligent transportation system is inseparable from computer technology, especially stereo vision technology, which has become an important technology for 3D reconstruction and detection of mixed traffic scenes. On the basis of the comprehensive description of the technology used in the traffic environment acquisition based on computer stereo vision, this paper analyzes the related object segmentation, shadow detection and 3D reconstruction and model recognition related technology in complex background traffic scene, and the 3D reconstruction technology in stereo vision matching. The development trend and difficulties of 3D reconstruction technology in stereo vision matching are also prospected.',\n",
       " \"Computer vision is often performed using Convolutional Neural Networks (CNNs). CNNs are compute-intensive and challenging to deploy on power-contrained systems such as mobile and Internet-of-Things (IoT) devices. CNNs are compute-intensive because they indiscriminately compute many features on all pixels of the input image. We observe that, given a computer vision task, images often contain pixels that are irrelevant to the task. For example, if the task is looking for cars, pixels in the sky are not very useful. Therefore, we propose that a CNN be modified to only operate on relevant pixels to save computation and energy. We propose a method to study three popular computer vision datasets, finding that 48% of pixels are irrelevant. We also propose the focused convolution to modify a CNN's convolutional layers to reject the pixels that are marked irrelevant. On an embedded device, we observe no loss in accuracy, while inference latency, energy consumption, and multiply-add count are all reduced by about 45%.\",\n",
       " 'Computer vision is a quite mature technology nowadays and it is very useful for power system monitoring applications. In this paper, some computer vision applications in a power substation will be reported. The system implemented is able to conduct automatically intruder detection, fire alarm zone detection and substation meter reading. The proposed system requires neither spare contact nor additional transducer. It can be used to prevent theft as well. Moreover, the proposed system is also very useful for short-term and ad hoc power substation monitoring because the system requires neither spare contact nor additional transducer.',\n",
       " \"The technical foundation for many applications of computer vision to multimedia applications is efficient and robust image motion estimation. These algorithms enable the creation of algorithms for mosaic construction, registration of video to a database, multi-sensor registration, 3D estimation and representation, and video content indexing and retrieval. Demonstrations on the above topics will be shown at WACV'98 by the Media Vision Group of Sarnoff Corporation.\",\n",
       " \"Cortical visual prostheses produce bionic vision by translating data from a headworn sensor into spatialtemporal patterns of electrical stimulation of a patient's Primary Visual Cortex (V1). The resulting bionic vision has low resolution, poor dynamic range and other limitations. These limitations are unlikely to change in the next decade due to the combined constraints of technology and biology as well as the slow process of medical device certification. This paper discusses ongoing research on Wearable Computer Vision Systems (WCVS) designed for two purposes: Improving the utility of bionic vision and non-invasive evaluation of visual prosthesis on sighted subjects using Simulated Prosthetic Vision (SPV).\",\n",
       " 'In this paper we propose a real-time traffic surveillance system using computer vision for tracking multiple vehicles from roadway images. We design an application program to monitor increasing congestion on roadway and problems. The application program is developed based on OpenCV library functions for the computer vision of the proposed monitoring algorithm. Our system sends traffic surveillance status to driver using GSM network. Additionally, we obtained the implementation of traffic monitoring system by using LED (Light-Emitting Diode) matrix display.',\n",
       " 'This paper covers basics of image generation mainly used for computer vision purposes to test new algorithms. It deals with visualization in 3D space in common Win32 environment with use of OpenGL and C++. The main idea is to obtain proper visualization image data which can be used as an image source for computer vision algorithms within simulation. In addition this concept allows that the tested computer vision algorithm and the image generator are booth in the same project. This significantly speeds up developing and testing of new algorithms.',\n",
       " 'Based on the study of developments in many fields of computer vision, a novel computer vision navigation system for mobile tracking robot is presented. Three irrelevant technologies, pattern recognition, binocular vision and motion estimation, make up of the basic technologies of our robot. The non-negative matrix factorization (NMF) algorithm is applied to detect the target. The application method of NMF in our robot is demonstrated. Interesting observations on distance measurement and motion capture are discussed in detail. The reasons resulting in error of distance measurement are analyzed. According to the models and formulas of distance measurement error, the error type could be found, which is helpful to decrease the distance error. Based on the diamond search (DS) technology applied in MPEG-4, an improved DS algorithm is developed to meet the special requirement of mobile tracking robot.',\n",
       " 'In recent years, lidar has been used as primary sensors for self-driving cars, however, due to their high expense, it becomes infeasible for mass production. Hence, we present the working of a self-driving car prototype that relies upon a cheaper alternative, viz. cameras. The primary objective of our prototype is to navigate safely, quickly, efficiently and comfortably through our virtual environment using computer vision. We have performed detection of lanes, traffic cars, obstacles, signals, etc. and have used the concept of stereo vision for depth calculation. Trajectory planning and steering control have also been implemented. Experimental results show that camera-based self-driving cars are viable and thus our paper can provide a foundation for all future real-world implementations.',\n",
       " 'Object detection is a challenge in the computer vision area. Traditional methods build from the extraction of their characteristics, to complex deep neural networks that combine object detection and classification. Currently, there are web services that allow the creation of models or methods that support the training and validation of the classifier to be built. In this article, we present the use of the web service Custom Vision by Azure, which is based on convolutional neural network architectures and helps developers in the construction of models that meet their requirements in the area of computer vision. The images to be evaluated are of roads in the rural area, where there is a geater complexity to detect objects. The Custom Vision service can be used to carry out classification or image detection tasks with some precision these will be presented in the results of the investigation.',\n",
       " 'Over the past twenty years, the computer vision and natural language processing groups have achieved great success in their respective fields. But their communities have rarely interacted. Recently, automatic image description generation has gathered a lot of attention in computer vision and natural language processing communities. The automatic image description generation associates computer vision with natural language processing. In the last five years, a large of literatures about image description generation have appeared. In this survey, we give a comprehensive overview of approaches and datasets used for image description generation that exist in the literatures.',\n",
       " 'Advances in machine learning have led to a rapid pace of innovation in Computer vision and deep learning classification algorithms. Deep learning classification models are often limited in flexibility due to their fixed pre-processing steps embedded into the algorithm and lack ways to easily iterate, debug, and analyze developed algorithms without programming knowledge. The lack of high-level tools for developing vision algorithms leads to longer development times that require significant knowledge of underlying algorithms. What about individuals without this deep knowledge of machine learning and vision yet wish to develop algorithms to prototype ideas? What about non-programmers such as designers and artists that wish to utilize the state-of-the-art in computer vision in their work? To address this under-served community, we propose a visual-programming solution akin to those found in modern game engines geared towards computer vision algorithm development. These results in a new prototyping tool to empower researchers and non-programmers to easily iterate algorithm development, use pre-trained classification models, and provide statistical post-analysis tools.',\n",
       " 'Applying computer vision for control the school of fish robots (SFR) using swarm model is a new direction. In this paper, the computer vision was studied for control a SFR using swarm model. The computer vision will return the framework and an algorithm was used to take the real position for every member. In the swarm model, every member has to solve a problem and combination together to get the target of the swarm. Combination is a complex problem, because the school want to solve a problem together, they have to combine the velocity, acceleration, force, direction. Moreover, the swarm fish robot will using computer vision to solve a problem, and the proposed problem is moved the chunk of wood (CW) on the straight line from point to point. At every time, to depend on CW position in the framework, every member will have suitable actions to get the target. Every member has to swim combination of direction, velocity, acceleration and force together to give a total of forces on the CW, and this total of forces have a straight direction to take the CW flowing on the straight line. With the proposed theory and algorithms, the scholar have designed a school of three fish to research and confirmed by experimental results.',\n",
       " 'Ball-on-Plate system, an example of nonprehensile robotic manipulation, constitutes a formidable control problem as well as an excellent testbench for complex control algorithms. The authors have developed a prototype of a linkage based Ballon-Plate system with Computer Vision Sensing, and an accompanying GUI based controller implemented on MATLAB, with the capability to implement various control schemes. Position control was tested using a PD Controller, and a novel fuzzy inference control scheme has been developed.',\n",
       " 'In this paper, we describe a PC cluster system for real-time computer vision. For easy construction of real-time distributed computer vision on PG cluster, we have developed a programming environment, in which a programmer have to describe only data flow between PCs and processing algorithms on each PG. And we also describe a real-time human motion capture system using multiple cameras as an prototypical application on the PG cluster system, which shows that the system works in real-time.',\n",
       " \"In this paper, a locating ceramics system based on computer vision is introduced. The system is used to indirectly measure the ceramics' poses on the automatic product line of the robot spraying unburned ceramics. The structured locating principle and method of the system are described and some application results are given. The locating principle of the system adopts the camera calibration, image segmentation through finding the valley at an image saturation histogram, and the image morphological operation. The Locating Ceramics System based on computer vision has been applied in practice.\",\n",
       " 'Metric learning refers to the task of learning a distance function among objects. Recently, metric learning has drawn wide attention in the field of computer vision and pattern recognition. This paper seeks to examine metric learning from two different perspectives: conventional metric learning and deep metric learning. The formulation and principal of different metric learning are delineated and the advantages and disadvantages are then presented. The paper concludes with the discussion on the future direction of the development of metric learning.',\n",
       " \"Computer vision systems and algorithms are designed to process digital images and extract necessary information from it. In this research we propose computer vision system consisting of several spherical mobile devices with digital camera and microcomputer inside. Device's design, basic characteristics and current results, as well as steps for further actions to improve the system are described below in the paper. At the core of image processing OpenCV library and modern convolutional neural networks such as YOLOv3, MobileNETSSDv2 are used.\",\n",
       " 'Moving object detection has achieved a noticeable attention in many computer vision applications. The research community have contributed lot of works for dealing with major challenges of moving object detection in real-world scenarios. The paper presents a comprehensive review on different moving object detection techniques classified into four categories: Background Modeling Based techniques; Frame Difference Based techniques; Optical Flow Based techniques and Deep Learning Based techniques. Moreover, detailed descriptions of various methods in each of this category are also provided.',\n",
       " 'The paper presents a prototype of a system which can be used as a therapeutic and educational tool for children with developmental problems. Natural body movements and gestures are used in the system to interact with virtual objects displayed on the screen. Nowadays such systems can he easily built with the use of widely available free software tools for both graphical and vision applications. Such tools are also shortly presented in the paper.',\n",
       " 'nan',\n",
       " 'Unmanned vehicles (Smart cars, drones, robots) need to understand and respond to the surrounding environment in order to perform their tasks. Therefore, they must be equipped with vision capabilities (vision sensors) to let them to detect the presence of objects in their paths or measure how far away objects are from the sensor depending of the type of application. Several technologies have been used to deal with this computer vision application such stereovision, LIDAR and RADAR but each of them still has advantages and disadvantages in term of limitations and price. In this paper, we developed one of the functions of unmanned vehicles to detect the presence of objects and measure how far they are using Kinect depth sensor, a low-cost range sensor along with its higher depth fidelity and attractive alternative in computer vision. The results showed that Kinect sensor can detect along with segmentation techniques the presence of objects in its field of view and measure how far they are from the sensor. Based on results, Kinect can be mounted on unmanned vehicles like a vision sensor for obstacle avoidance application or other applications which require a vision sensor to measure how far detected objects are from it as well for also manned vehicles to alert drivers but mostly computer vision has a crucial role in vehicles without human intervention.',\n",
       " 'In work, a real-time speed limit warning assistant system for drivers is presented. The proposed approach is able to produce optical and acoustic warnings by detecting speed limit signs simply processing captured images via a camera. The system developed can process by up to 7 frame per second on Raspberry Pi 2 single board computer.',\n",
       " 'It is a big challenge to realize accurate security detection of blast furnace bearing at the same time so as to guarantee the security of equipment. To end this problem, this paper proposed a computer vision technology based on sensor data and hybrid deep learning method for the solution. We use Variational Mode Decomposition (VMD) algorithm which is a new time-frequency analysis method, which can decompose multi-component signals into multiple single-component amplitude-modulated signals at one time to decompose and deal with the sensor data of bearing fault, so as to realize the effective stripping of fault components and original components from sensor data. Using the artificial intelligence mentioned above, the features can be quickly and accurately extracted. By combining the advantages of deep learning, we improve the coupling mechanism and implement a hybrid deep learning-based computer vision method which greatly improves the calculation speed and accuracy of bearing fault diagnosis. It can be fully connected with the feature extraction algorithm VMD, which overcomes the problem that the bearing feature component is easy to be submerged and difficult to extract under the condition of high temperature and strong noise. The results show that the optimal selection of parameters of computer vision technology based on sensor data and hybrid deep learning can be realized through training the sensor data obtained from the experiment. The optimized hybrid deep learning-based computer vision algorithm can achieve 97.4% bearing fault diagnosis hit rate, which is an advanced application of deep learning algorithm in the engineering field.',\n",
       " 'This paper gives two contributions to the state-of-the-art for viticulture technology research. First, we present a comprehensive review of computer vision, image processing, and machine learning techniques in viticulture. We summarize the latest developments in vision systems and techniques with examples from various representative studies, including, harvest yield estimation, vineyard management and monitoring, grape disease detection, quality evaluation, and grape phenology. We focus on how computer vision and machine learning techniques can be integrated into current vineyard management and vinification processes to achieve industry relevant outcomes. The second component of the paper presents the new GrapeCS-ML database which consists of images of grape varieties at different stages of development together with the corresponding ground truth data (e.g., pH and Brix) obtained from chemical analysis. One of the objectives of this database is to motivate computer vision and machine learning researchers to develop practical solutions for deployment in smart vineyards. We illustrate the usefulness of the database for a color-based berry detection application for white and red cultivars and give baseline comparisons using various machine learning approaches and color spaces. This paper concludes by highlighting future challenges that need to be addressed prior to successful implementation of this technology in the viticulture industry.',\n",
       " 'nan',\n",
       " 'Children with visual impairments face disproportionate challenges in learning mathematics. In this paper, we explore how to apply computer vision techniques to enhance mathematics learning by fusion of multisensory information for children with visually impairments. The multisensory information includes visual and audio information. Our research focuses on the mathematics education for children with visual impairments in early ages (Kindergarten - Grade 3) by using arithmetic racks. A computer vision-based algorithm is developed to detect the numbers and positions of arithmetic rack beads and the detection results are then provided to students with visual impairments as speech guidance. Preliminary results demonstrate the effectiveness and efficiency of the proposed method for detecting the number, color, position, and moving directions of the arithmetic rack beads under different situations.',\n",
       " 'In this paper, we present a new algorithm for finding all intersections of three quadrics. The proposed method is algebraic in nature and it is considerably more efficient than the Grobner basis and resultant-based solutions previously used in computer vision applications. We identify several computer vision problems that are formulated and solved as systems of three quadratic equations and for which our algorithm readily delivers considerably faster results. Also, we propose new formulations of three important vision problems: absolute camera pose with unknown focal length, generalized pose-and-scale, and hand-eye calibration with known translation. These new formulations allow our algorithm to significantly outperform the state-of-the-art in speed.',\n",
       " 'Virtual metrology of visualizing copper microstructure featured with computer vision and artificial neural network is reported. The focused ion beam images of copper microstructures are modeled. The input is the process variables of fabricating thin copper film and output is the microstructure image corresponding to the process variables. Computer vision and artificial neural network are used to generate model parameters. Images are generated using the model. Nucleation, grains, grain boundaries and twins are all visualized. The features of nucleation and grain growth are quantitatively analyzed in the images. The results of the predicted images support that the inference of microstructure images is proven. The technique is adaptive to most imaging systems and one of the exploratory applications of virtual metrology.',\n",
       " 'Computer hardware is becoming ubiquitous, from kitchen appliances to research machines to cell phones. This availability of computing capable circuits has driven its cost down, thus making several applications possible on commercial hardware. Computer vision is one of such tasks, it is now possible to create computer based helping devices for the visually impaired. In recent years there has been an explotion of computer vision hardware for many applications, 3D scanners in particular which are used to interact with regular operative systems or games. This paper presents an apparatus which uses one of such devices (Kinect) developed by Microsoft that senses the world in 3D. This information is then processed with a low-power computer called the BeagleBone whose output will be sent to the user via mechanical pulses.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'We present a method for recognizing intended grasp type based on data from the Microsoft Kinect. A computer vision algorithm estimates the vertical and the transversal distance of the hand from the center of the object and the hand orientation from the Kinect depth images. Based on this set of features in the reaching phase of grasp artificial neural network recognizes the intended grasp type. This is demonstrated with an example of a coffee cup on a working desk. Trained neural network classified the grasp with accuracy above 85%. By adding this feature to the existing computer vision system for control of the functional electrical stimulation assisted grasping we facilitate the compliance between the applied electrical stimulation and the user intentions.',\n",
       " 'We present a simple yet effective framework Transmitting What Matters (TWM) - to generate compressed videos containing only relevant objects targeted to specific computer vision tasks, such as faces for the task of face expression recognition, license plates for the task of optical character recognition, among others. TWM takes advantage of the final desired computer vision task to compose video frames only with the necessary data. The video frames are compressed and can be stored or transmitted to powerful servers where extensive and time-consuming tasks can be performed. We experimentally present the trade-offs between distortion and bitrate for a wide range of compression levels, and the impact generated by compression artifacts on the accuracy of the desired vision task. We show that, for one selected computer vision task, it is possible to dramatically reduce the amount of required data to be stored or transmitted, without compromising accuracy.',\n",
       " 'A navigation aid system provides additional help to a visually impaired person in navigation. It helps to move in a given environment with certain level of confidence in the absence of own visual ability. Using cane, or pet dog are conventional ways of navigational aid, Researchers have recently developed navigational aid systems based on various computer technologies. For example Radio-Frequency Identification tags, Global Positioning System, Geographical Information System, and Vision-to-Sound systems using sensory substitution. These innovative approaches provide beneficial systems but are tied to some limitation. Dependence upon installed hardware devices, remote information server, GPS signal are some of the limitations faced by the existing contemporany research in navigational aid systems. We propose to develop a computer vision based system that would use camera vision input and text-to-speech synthesized output to provide navigation aid. It will be independent from any remote factors and will have the inherent characteristic of improvement with further advancements in computer vision.',\n",
       " 'The utilization of the key frames in computer vision will economize the storage capacity and also reduce the search amount of image information. In addition, visual processes can be recovered via the key frames. The content-based key frame in computer vision is defined, and a content-based approach for deciding the key frames in computer vision is proposed in this paper. Level-Set-based Geodesic active contours method is adopted in this approach to acquiring the accurate contours of the moving object. The normalized contours of the moving objects are transformed with wavelet. The whole shapes of the contours are described by scale coefficients of wavelet, and details of the contours are particularly described by the parameters of wavelet coefficients Gaussian density distribution. The contents of the vision are understood from the whole and details. The experiments show that the approach is effective and practicable.',\n",
       " 'This article proposes a distinctive approach to processing of graphic images in intelligent computer vision systems on railway transport based on the use of fuzzy models with multistep prediction algorithm.',\n",
       " \"Chronic wound assessment and wound healing are important for diagnostic, follow up and wound treatment. However, this growing disease affecting nearly 2 thousand million and 5.7 million people in the USA and Europe, costing around $20 billion and $8 thousand million USD per year, still relies on subjective human assessment of wounds. A scoping review allowed us to identify 109 articles that map the literature on the topic of computer vision for chronic wound assessment and healing. These results were carefully analyzed and mapped into relevant clinical challenges associated with this field, identifying the maturity of each different computer vision challenge that needs addressing. Results show that wound size and tissue type classification already have interesting work, but various other clinical areas are in need of larger datasets and computer vision research efforts for achieving a relevant impact in today's clinical routine.\",\n",
       " 'Remote patient monitoring can improve the quality of life of elderly and impaired people, while reducing the costs. Among the most interesting technologies being investigated, computer vision has proved to be very effective in several important scenarios in which conventional sensors fail or are impractical. We propose a computer vision-based wireless sensor system for people remote tracking and monitoring based on low-cost embedded systems able to visually track the patient and detect critical motion and posture patterns, associated with dangerous situations. Motivation for the work and experimental results are provided, showing the effectiveness and the validity of the presented approach.',\n",
       " 'We describe the problem of automated steering using computer vision, focusing the analysis and design on appropriate lateral controllers. We investigate various static feedback strategies where the measurements obtained from vision, namely offset from the centerline at some lookahead distance and the angle between the road tangent and the orientation of the vehicle at some lookahead distance, are directly used for control. Within this setting we explore the role of lookahead, its relation to the vision processing delay, the longitudinal velocity and road geometry. Results from ongoing experiments with our autonomous vehicle system are presented along with simulation results.',\n",
       " \"A vision based system is presented for controlling multiple robot platforms in real time using imagery from a top view video camera. This article is a system paper that demonstrates the advantages of computer vision for robot control. A control loop using vision feedback allows creation of a robust working system while greatly reducing the complexity of the rest of the system. Planar marker pattern marker detection is an example of computer vision that is robust enough with today's technology for industry, and is employed for locating the robots with passive vision. Image processing in software obviates the need for complex systems for odometry, sonar etc normally found in robotics. The control of two robotic platforms is achieved with the simplest possible hardware, with computer vision as the only feedback in the system. A system is shown and described where a user can choose desired locations for the robots, which are guided around obstacles to their destinations using ARToolkit marker patterns detected in imagery from a stationary digital video camera.\",\n",
       " 'This article presents a project to design a robust robotic arm which can perform multifunctional tasks. The controller of the manipulator is based on Arduino mega 2560 Microcontroller. The aim of the project is to focus all axes of manipulator to lift, carry and unload the objects at a desired location. This requires a precise drive motion control that incorporate electric motors as a drive system. Further experiments are done to implement a camera based 3D vision system integrated with a computer vision algorithm to recognize object deformation and spatial coordination to control the deviation from the original training. The 3D visualization systems are able to detect the objects as well as their distance from the End-effecter and transmit the signals to the drive system. The vision system requires a separate computing hardware capable of processing complex vision algorithms. We utilize Raspberry pi microcontroller for processing the vision data, separately making the vision system capable of recognizing the specified object as per program commands.',\n",
       " 'After nearly half a century of computer vision research, application-specific systems are common but the goal of developing a robust, general-purpose computer vision system remains out of reach. Rather than focus on the strengths and weaknesses of current computer vision approaches, this paper will enumerate and investigate the challenges that must be overcome before this goal can be achieved. Key challenges include handling variations in environment or acquisition parameters such as lighting, view angle, distance, and image quality; recognizing naturally occurring as well as intentionally deceptive variations in object appearance; providing robust general-purpose image segmentation and co-registration; generating 3D representations from 2D images; developing useful object representations; providing required knowledge that is not represented in the image itself, and managing computational complexity. Each of these challenges, along with their relevance to solving the vision problem, will be discussed. Understanding these challenges as a whole may provide insight into underlying mechanisms that will provide the backbone of a robust general-purpose computer vision system.',\n",
       " 'The creation of a workflow for solving computer vision problems is a complex task. The current practice largely rely on domain experts to achieve this. The search space for creating a suitable solution using available algorithms for a given goal is large. This exploratory work of solution building is time-, effort- and intellect-intensive endeavor. To address these issues, we propose a structured and generalized goal-driven algorithm selection approach for building computer vision workflows on the fly. It generates workflows depending on initial conditions and goal conditions by combining various image processing algorithms. Symbolic AI planning is aided by Reinforcement Learning to recommend optimal workflows that are robust and adaptive to changes in the environment. Experimental results show that our proposed framework gives significantly better workflows as compared to the template-based systems.',\n",
       " 'Organizations of the agro-industrial sector, are now increasingly investing in the development of technological systems that allow the computerization of all its processes. Recently the methods and techniques of computer vision have been widely used for monitoring and inspection during the production and harvesting, allowing detect problems early and thus, improve the quality of products. In the field of mushroom production one of the most important aspects, and perhaps most prevalent, is to be able to predict its production. To this end it is proposed an Intelligent System Mushroom Harvest Forecast (SIPCC), based on techniques and methods of computer vision and Artificial Neural Networks (ANN). This paper presents an architecture of a SIPCC functional and technical level, complemented with the analysis and presentation of data demonstrating its viability.',\n",
       " 'In this study, a computer vision system was proposed for the seed images classification. The classification process was performed using uniform local binary patterns obtained from digital seed images. In this study, 240 (120 training and 120 test) images of the seed were used. First, the average uniform histograms of each type of seed (seed type classes) was obtained for the training set. Then the uniform LBP histogram of each seed in the test set were produced and compared with histograms of classes by using nearest neighbor. The Euclidean distance, sum square error, histogram intersection and Chi-square statistics were used to calculate the distance between seed samples. 95.83%. of seed images has been diagnosed properly with the proposed. As a result, the surface shape of the seeds include important information patterns to determine the taxonomic relationships, it is is expected that the computer vision systems provide significant advantages to identify the type of seed.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'This survey summarizes many theories and methods applied for vision and speech improvement for cerebral palsy babies. Some research team focus on vision and speech stimulation for cerebral palsy with different techniques using computer aided system and electrical devices. Electrical electrode devices are complexity to use and reliability are inadequate for providing vision and speech stimulation which lead to recover partial output. The material of the survey is divided into four section. 1) Overview of vision development 2) vision stimulation techniques. 3) low vision 4) overview of speech stimulation and techniques. The survey taken from the research field of computer science, neural networks, electrical engineering and psychology. It contains useful guidelines for the construction of new vision and speech stimulation tool which support for young and visually inattentive children.',\n",
       " '5G is designed to be an essential enabler and a leading infrastructure provider in the communication technology industry by supporting the demand for the growing data traffic and a variety of services with distinct requirements. The use of deep learning and computer vision tools has the means to increase the environmental awareness of the network with information from visual data. Information extracted via computer vision tools such as user position, movement direction, and speed can be promptly available for the network. However, the network must have a mechanism to match the identity of a user in both visual and radio systems. This mechanism is absent in the present literature. Therefore, we propose a framework to match the information from both visual and radio domains. This is an essential step to practical applications of computer vision tools in communications. We detail the proposed framework training and deployment phases for a presented setup. We carried out practical experiments using data collected in different types of environments. The work compares the use of Deep Neural Network and Random Forest classifiers and shows that the former performed better across all experiments, achieving classification accuracy greater than 99%.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " \"Industry tends to optimize accuracy and time efficiency of every process analyzing its constraints and limits. Several tasks requiring high precision and reproducibility must be automated. In the context of secure Integrated Circuits (ICs) characterization, tasks such as power analysis are commonly automated. However, very few automations exist for tools calibration, while recent characterization schemes encounter mechanical constraints. Computer vision, flexible tool used in various fields, gives opportunities to address these constraints. In the case of laser fault injections, several positioning adjustments are required to ensure a maximal energy transmission in a targeted point. An accurate focalization of the laser beam is reached by using an autofocus system. Such a system is obtained by analyzing the camera view of the IC in the time-frequency domain. Whatever the method to disturb an IC, every secure characterization should be reproducible. By exploiting computer vision assistance, fault injection can be automated by mixing vision techniques to build a full or partial view of an IC and automatically identify the targeted IC and focus the perturbation on a chosen pattern in the image. A reliable pattern detection is implemented by studying spatial consistency of image's remarkable features represented by graphs. This paper presents several computer vision techniques to address above problems.\",\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'We propose an XML-based Computer Vision Markup Language for use in Cognitive Vision, to enable separate research groups to collaborate with each other as well as making their research results more available to other areas of science and industry, without having to reveal any proprietary ideas, algorithms or even software. The Computer Vision Markup Language can communicate any type and amount of information, making unavailable functionality accessible to anyone. In this paper we introduce the language and describe how we have implemented it in a very large cognitive vision project. We provide a free open source library for working with this language, which can easily be implemented into existing code providing seamless network communication abilities and multi-platform support. Last we describe the future of CVML and how it might evolve to include other areas of research.',\n",
       " 'nan',\n",
       " \"User-computer dialogues are typically one-sided, with the benefits from computer to user far greater than that from user to computer. The movement of a user's eyes can provide a convenient, natural, and high-bandwidth source of additional user input, to help redress this one-sidedness. We, therefore investigate the introduction of eye movements as a computer input medium. Our emphasis is on the study of interaction techniques that incorporate eye movements into the user-computer dialogue. In this paper, an advanced approach to man machine interaction is proposed, in which computer vision techniques are used for interpreting user actions.\",\n",
       " 'A computer vision system for person-independent recognition of hand postures against complex backgrounds is presented. The system Is based on Elastic Graph Matching (EGM), which was extended to allow for combinations of different feature types at the graph nodes.',\n",
       " 'Tracking of moving objects in a video stream captured from a camera is a very important step in computer vision based applications. This paper provides a novel framework for tracking moving objects in a closed environment using a composite array of inexpensive, off the shelf cameras.',\n",
       " '3D face modeling based on real images is one of the important subject of Computer Vision that is studied recently. In this paper the study that eve contucted in our Computer Vision and Intelligent Systems Research Laboratory on 3D face model generation using uncalibrated multiple still images is explained.',\n",
       " 'Compared with the CPUs and GPUs, the AI accelerators are able to achieve higher performance and energy efficiency for accelerating the DNNs. However, besides the DNNs, the computer vision also involves other tasks such as conventional image filtering and stereo matching. These tasks are not supported by the AI accelerators. In addition, the newly proposed DNN structures are not supported by the existing AI accelerators, making them difficult to catch up with the ever-evolving AI algorithms. To address this challenge, the Google has proposed the Pixel Visual Core (PVC) processor with a flexible architecture to accelerate diverse computer vision tasks including the DNNs while achieving higher efficiency. However, the architecture of the PVC is not well optimized, leading to limited energy efficiency. In this brief, we have proposed a flexible and efficient processor architecture (named NVP) with several design techniques to address the limitations of the PVC. The NVP is able to accelerate diverse computer vision tasks including DNN structures, conventional image filtering and stereo matching, while achieving significantly improved energy efficiency than the PVC and comparable energy efficiency with the AI accelerators.',\n",
       " \"in today's scenarios world is struggling to develop new technologies for handicap people, so that they can also compete in the modern world. The Main existing solutions are prosthetic devices, voice recognition programs, screen readers, and hand gestures recognition, but developing new technologies faces various problems like time, cost and portability. This paper proposes a new perspective by enhancing the existing technology to make it compatible for handicap people (mainly wrist less). This paper adds a new ingredient in field of Human Computer Interaction [1] (HCI) by reshaping the existing computer technology with computer vision and embedded system. The Key notion of our solution is to detect the motion of a wearable tag to operate an entire computer using aggregation and processing of various data.\",\n",
       " 'nan',\n",
       " \"With current development universally in computing, now a day's user interaction approaches with mouse, keyboard, touch-pens etc. are not sufficient. Directly using of hands or hand gestures as an input device is a method to attract people with providing the applications, through Machine Learning and Computer Vision. Human-computer interaction application in which you can simply draw different shapes, fill the colors, moving the folder from one place to another place and rotating your image with rotating your hand gesture - all this will be without touching your device only. In this paper Machine Learning based hand gestures recognition is presented, with the use of Computer Vision different types of gesture applications have been created.\",\n",
       " 'In this paper, an applied method based on DSP and computer vision for the detection of vehicle flux is presented. The system uses video image processing technology, processes the traffic scene image series taken by the high-speed camera. According to the variation of the grey level of the pixel and the condition of the edges monitored on the video detection area, it can identify the number of the vehicles. So it could accomplish the detection of vehicle flux on a rigid roadway rapidly.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'This paper describes an in-depth investigation and implementation of interleaved memory for pixel lookup operations in computer vision. Pixel lookup, mapping between coordinates and pixels, is a common operation in computer vision, but is also a potential bottleneck due to formidable bandwidth requirements for real-time operation. We focus on the acceleration of pixel lookup operations through parallelizing memory banks by interleaving. The key to applying interleaving for pixel lookup is 2D block data partitioning and support for unaligned access. With this optimization of interleaving, pixel lookup operations can output a block of pixels at once without major overhead for unaligned access. An example implementation of our optimized interleaved memory for affine motion tracking shows that the pixel lookup operations can achieve 12.8 Gbps for random lookup of a 4x4 size block, of 8-bit pixels under 100 MHz operation. Interleaving can be a cost-effective solution for fast pixel lookup in embedded computer vision.',\n",
       " 'nan',\n",
       " 'This paper reports a novel method for Nucleus and Micro Nucleuses segmentation. These biological structures are very handy to biologists for relieving structural chromosome aberration. The adopted method consists into a pipeline of advanced computer vision algorithms some of them specifically tailored for the current segmentation problem. Starting from weak hypotheses on size, shape and colour of Micro Nucleuses it is possible to efficiently segment all the image features by computer vision approach. Experimental results on real images are reported',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Recently, there has been an increasing demand for computer-vision-based inspection and/or measurement system as a part of factory automation equipment. In general, it is almost impossible to check the fault of all parts; coming from part-feeding system, with only manual inspection because of time limitation. Therefore, most of manual inspection is applied to specific samples,. not all coming parts, and manual inspection neither guarantee consistent measuring accuracy nor decrease working time. Thus, in order to improve the measuring speed and accuracy of the inspection, a computer-aided, measuring and analysis method is highly needed. In this paper, a computer-vision-based pipe inspection system is proposed, where the front and side-view profiles of three different kinds of pipes, coming from a forming line, are acquired by. processed by using computer vision. And the edge detection is Laplace operator. To reduce the vision processing time, modified Hough transform is used with clustering method for straight line detection. And the center points and diameters of inner and outer circle are found to determine eccentricity of the parts. Also, an inspection system has been built so that the data and images of faulted parts are stored as files and transferred to the server.',\n",
       " 'In many problems of computer vision we have to estimate parameters in the presence of nuisance parameters increasing with the amount of data. It is known that unlike in the cases without nuisance parameters, maximum likelihood estimation (MLE) is not optimal in the presence of nuisance parameters. By optimal we mean that the resulting estimate is unbiased and its variance attains the theoretical lower bound in an asymptotic sense. Thus, naive application of MLE to computer vision have a potential problem. This applies to a wide range of problems from conic fitting to bundle adjustment. For this nuisance parameter problem, studies have been conducted in statistics for a long time, whereas they have been little known in computer vision community. We cast light to the methods developed in statistics for obtaining optimal estimates and explores the possibility of applying them to computer vision problems. In this paper we focus on the cases where data and nuisance parameters are linearly connected. As examples, optical flow estimation and affine structure and motion problems are considered. Through experiments, we show that the estimation accuracy is improved in several cases.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'this article describes method of automated quality monitoring of the produced fabric. The monitoring system consists of a web camera and micro computer, which allows to determine the geometric dimensions of the spacing between the transverse threads of the fabric. The management of these sizes provides the possibility to control the density of the finished fabric. The described system allows to automate the control of fabric quality. Considered algorithm calculating the distance between the transverse threads based on the microcomputer Raspberry pi and the Python programming language.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " \"This paper describes the Video Convolution Processor (VCP) which meets the stringent requirements of a Computer Vision algorithm for depth estimation from stereo image pairs. Use of the VCP allows the acceleration of this process to near real-time rates. In particular, the VCP is designed to perform a high precision image transformation to force image pairs to satisfy an epipolar constraint (image rectification) and the high level of mathematical integrity required for this operation is not provided by commercial devices. Additionally, the generality and scalability of the VCP allows it to be used in a wide range of Computer Vision and Image Processing applications. A prototype device has been fabricated using a 2LM, 1 mu m, CMOS process and the device operates at a frequency of 30MHz. Under this condition the VCP can 'rectify', decimate, and filter a 256x256 image at 25 frames.s(-1) forming a very versatile 'front-end' to Computer Vision problems.\",\n",
       " \"Computer Vision and Image Analysis are used in researches with an objective of extracting information from a set of scenarios. Multiple researches with varying objectives like vehicle speed detection, traffic density estimation, vehicle counting, or in general, observation of behaviors of multiple objects, have been applying Computer Vision. This research paper is about utilizing Computer Vision for obstruction detection by observing temporal state of vehicles situated in a pedestrian crossing lane. The researchers gathered data by taking videos of real traffic in a road containing a pedestrian crossing lane (PCL). The method starts with a pre-processing phase wherein the image was de-noised, converted to grayscale and derived the Image Binarization, and establishment of the PCL for region of interest (ROI). Connected components are extracted then assigned its own structure with corresponding properties called 'track'. Tracks are monitored by using Kalman Filter and Hungarian Algorithm. Then, Ray-Casting algorithm is applied to determine if an object violates the traffic rule. For violators, a snapshot will be taken and determine the license plate. Based on the result, True Positive Rate of 65.28% and True Negative Rate of 98.26% were obtained.\",\n",
       " 'Autonomous capabilities have emerged as a ubiquitous technology in cars, robots, drones and many other useful devices that are destined to transform our society. Many of the more advanced autonomous devices use computer vision to track and recognize objects, while others may use LIDAR or SONAR or other forms of radar. Since many of these devices are positioned to be disruptive technologies in our society, the fidelity of their abilities is paramount. In this paper, we introduce our black box approach, which can accurately infer, characterize, and break native device autonomy. We demonstrate this approach by using the tightly guarded autonomy code in the immensely popular computer vision driven autonomous DJI drones (i.e., ActiveTrack Mode) as the target native autonomy. We illustrate that this method allows us to quickly infer detailed information about the computer vision tracking algorithm, characterize the autonomous capability, and identify targets that defy these algorithms. We posit that this approach can be extended to other computer vision driven autonomous systems and is a necessary step in developing nondestructive privacy preserving mechanisms that foil the presumed tracking process.',\n",
       " 'Recently, a growing community of researchers has used reconfigurable hardware systems to solve computatianally intensive problems. Reconfigurability provides optimized processors for systems on chip designs, and makes easy to import technology to a new system through reusable modules. The main objective of this work is the investigation of a reconfigurable computer system targeted for real-time computer vision applications. The system is intended to circumvent the inherent computational load of most window-based computer vision algorithms. It aims to build a system for such tasks by providing an FPGA-based hardware architecture. Some preliminary results are presented and discussed.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Techniques from sparse signal representation are beginning to see significant impact in computer vision, often on nontraditional applications where the goal is not just to obtain a compact high-fidelity representation of the observed signal, but also to extract semantic information. The choice of dictionary plays a key role in bridging this gap: unconventional dictionaries consisting of, or learned from, the training samples themselves provide the key to obtaining state-of-the-art results and to attaching semantic meaning to sparse signal representations. Understanding the good performance of such unconventional dictionaries in turn demands new algorithmic and analytical techniques. This review paper highlights a few representative examples of how the interaction between sparse signal representation and computer vision can enrich both fields, and raises a number of open questions for further study.',\n",
       " 'nan',\n",
       " 'The presence of cameras and powerful computers on modern mobile devices gives rise to the hope that they can perform computer vision tasks as we walk around. However, the computational demand and energy consumption of computer vision tasks such as object detection, recognition and tracking make this challenging. At the same time, a fixed vision hard core on the SoC contained in a mobile chip may not have the flexibility needed to adapt to new situations, or evolve as new algorithms are discovered. This may mean that computer vision on a mobile device is the killer application for FPGAs, and could motivate the inclusion of FPGAs, in some form, within modern smartphones. In this paper we present a novel hardware architecture for object detection, that is bit-for-bit compatible with the object classifiers in the widely-used open source OpenCV computer vision software. The architecture is novel, compared to prior work in this area, in two ways: its memory architecture, and its particular SIMD-type of processing. The implementation, which consists of the full system, not simply the kernel, outperforms a same-generation technology mobile processor by a factor of 59 times, and is 13.5 times more energy-efficient.',\n",
       " 'This paper presents an automated computer vision algorithm for estimating contact angles that a droplet of probe liquid forms on hydrophobic fibers. A specially designed microrobotic platform is utilized in manipulating the microscopic fibers, shooting droplets in the scale of tens of nanoliters on the fibers and capturing images of the experiments. The images are then processed with the automated computer vision algorithm. The algorithm is proven to be reliable and repeatable with totally 29 experiments on five different bio-based fiber samples.',\n",
       " 'This article analyzes the research progress in field of Convolutional Neural Networks (CNNs) using the bibliometric method. Literature samples of CNNs are analyzed by a basic statistic and co-citation network. Experimental results show that CNNs are being utilized in many computer vision applications, such as fault and image recognition diagnosis, seismic detection, positioning, and automatic detection of cracks and signals, image classification and image segmentation. In addition, there is systematic research on unbalanced problems in CNNs. Quantitative experimental research, extensive application fields, and market research informatization will be the three vital research tendencies in the future. The ideas and conclusions of this article provide insights to the academic research of CNNs and their practical application in the corporate world.',\n",
       " 'Aiming at the appearance quality detection of potato, this paper proposes an image feature detection and recognition algorithm based on computer vision. Firstly, the image of the potato obtained by the computer vision system was analyzed, and the background image of the potato was removed by Otsu method. Then the image processing and recognition were carried out for the green skin, germination and mechanical damage state of the potato. For the green skin state of potato, the normal and green skin areas of potato are discriminated by the similarity of pixel Euclidean distance. For the potato sprouting from the epidermis, the germination area of the potato is detected by the gray difference method of the image G channel. Finally, the edge information and image geometric features are used to determine the damage site of the potato epidermis, and finally the appearance quality of the potato is detected.',\n",
       " 'In this paper, we present a machine intelligent system that can automatically classify sugarcane images into predefined categories. This system is developed in order to facilitate the operation in sugar manufacturing factories and can be beneficial to the sugar industry as a whole. The software system consists of the core computer vision module and other compounds, such as user interfaces and database management. To develop the core module, we apply deep learning models based on convolutional neural networks, which are currently state-of-the-art models for computer vision. The best models trained and evaluated on our sugarcane datasets achieve more than 90% multi-class accuracy in almost all settings. We have incorporated the trained model into the prototype system and successfully installed the system to test operating at one of the major sugar manufacturing facilities in the previous sugarcane harvesting season.',\n",
       " 'Optimization within a layer of a deep-net has emerged as a new direction for deep-net layer design. However, there are two main challenges when applying these layers to computer vision tasks: (a) which optimization problem within a layer is useful?; (b) how to ensure that computation within a layer remains efficient? To study question (a), in this work, we propose total variation (TV) minimization as a layer for computer vision. Motivated by the success of total variation in image processing, we hypothesize that TV as a layer provides useful inductive bias for deep-nets too. We study this hypothesis on five computer vision tasks: image classification, weakly supervised object localization, edge-preserving smoothing, edge detection, and image denoising, improving over existing baselines. To achieve these results we had to address question (b): we developed a GPU-based projectedNewton method which is 37x faster than existing solutions.',\n",
       " \"Computer vision professionals develop systems that monitor endangered fish species, alert car dealerships of potential theft, and track inventory on retailers' shelves, to name a few examples. While their products vary, their day-to-day work practices rarely do. Most work in small, co-located, multi-disciplinary teams and rapidly iterate systems built with algorithms, products, and services provided by similar teams but at different companies. Their examples challenge global software engineering research to look beyond the social and technical coordination work of large, internal software development teams. Their stories, culled from ethnographic interviews with eighty computer vision engineers and research scientists, echo the heady days of 1980s Silicon Valley when Bay Area social networks and the global migration of professional talent fueled the rapid growth of the high technology industry. These engineers, then and now, migrate from task to task, versus the task from engineer to engineer.\",\n",
       " 'The computer vision systems are mainly devoted for production monitoring in quality inspection systems. It is the fastest growing and most popular non-invasive product defects detection method. The productivity of electronic components growth and their prices decline creates favorable conditions for the development of image processing systems for industrial production. The food industry is one of the main industries. Production volumes grow along with human population growth. Containers for food industry are made in very large quantities and demand on quality inspection system plays important role. An automated computer vision system was developed for the control of PET preparation quality. The implementation of the designed system was presented in this article. The system used for image processing algorithms to inspect the lateral and upper parts of the workpiece. The system is designed according to its operating parameters. Reached throughput is 10,000 workpieces per hour.',\n",
       " 'Container image correction is an important step in image identification preprocessing for container terminal automation. This paper proposes a method for image correction of multi-angle containers based on computer vision technology. The method includes two steps, the first stage is the filtering and straight-line detection of the container image. And in the second stage, the corresponding points are obtained according to the detected straight-line, and the perspective transformation is performed to achieve the corrective images, which can make it easy to detect and identify image text areas.',\n",
       " 'The recent emergence of multi-core processors enables a new trend in the usage of computers. Computer vision applications, which require heavy computation and lots of bandwidth, usually cannot run in real-time. Recent multicore processors can potentially serve the needs of such workloads. In addition, more advanced algorithms can be developed utilizing the new computation paradigm. In this paper, we study the performance of an articulated body tracker on multi-core processors. The articulated body tracking workload encapsulates most of the important aspects of a computer vision workload. It takes multiple camera inputs of a scene with a single human object, extracts useful features, and performs statistical inference to find the body pose. We show the importance of properly parallelizing the workload in order to achieve great performance: speedups of 26 on 32 cores. We conclude that: (1) data-domain parallelization is better than function-domain parallelization for computer vision applications; (2) data-domain parallelism by image regions and particles is very effective; (3) reducing serial code in edge detection brings significant performance improvements; (4) domain knowledge about low/mid/high level of vision computation is helpful in parallelizing the workload.',\n",
       " \"A typical computer vision task for surveillance may include the following step; Image Acquisition, Image Preprocessing, Image Enhancement, Image Segmentation, Feature Extraction and Identification of object. Each step in the pipeline is critical since poor performance at any stage may affect the overall result of a computer vision application. Among the steps indicated, the image pre-processing step is crucial for the performance of a model. However, the time taken by the preprocessing algorithms to complete their tasks generally contributes to the overall running time of a computer vision application. Various techniques and algorithms have been proposed to help reduce the time complexities associated with image pre-processing algorithms. Among the many algorithms are the Adaptive Approximated Median Filtering Algorithm for Impulse Noise Reduction, A Robust Median-based Background Updating Algorithm and Fast Generation of Image's Histogram Using Approximation technique for Image Processing Algorithms. These proposed methods used approximation of existing techniques to significantly reduce the time complexity associated with the algorithms. However, their resultant effects in the pipeline of a typical computer vision application had not been evaluated. This paper therefore evaluated the effectiveness of the algorithms on motion detection and movement tracking computer vision application. The experimental results indicate that these pre-processing algorithms can significantly reduce the overall running times for motion detection and scene boundary monitoring, and therefore can easily replace original ones for real-time motion and object tracking.\",\n",
       " 'In public safety scenarios, target objects identification and tracking is an important application, and two positioning methods including wireless and computer vision are respectively used for applications. In this article, we combine the wireless signal and computer vision, and propose a novel object identification and tracking technology. The positioning method based on computer vision helps to improve the accuracy of positioning, and we can easily distinguish different users according to wireless device information. Based on our proposed trajectory association technology, the visual trajectory is accurately matched to the corresponding wireless trajectory, and the identity of the visual trajectory is confirmed. Combined with the analysis of the position change and appearance change of visual objects, wireless positioning results are fused to correct the affected visual trajectory to improve overall system performance. A tracking system was deployed in the real world. The fusion path is proved to be closer to the real path and 90% of the errors were less than 1m. We have also implemented large-scale simulation experiments to evaluate our approach. The results show that our association algorithm has a high matching success rate and is insensitive to synchronization errors.',\n",
       " 'This paper presents a checkers playing robot, developed as a project within an educational framework by a group of students. The checkers playing robot is a multidisciplinary project that integrates technologies such as robots programming, computer vision, and artificial intelligence. In fact, the project is developed around three modules: namely a computer vision system, a checkers game software engine, and robotic manipulator programming. The result is a robot capable of autonomously playing checkers game against a human player. Firstly, the paper focuses on the technical aspects, and then some pedagogical issues are discussed.',\n",
       " 'Extraction of communication signals from noisy spectrograms is a challenging problem which has not been explored extensively from an intelligent signal processing and computer vision based perspective. In this paper we propose a novel technique of extracting the communications signal from a noisy spectrogram using a combination of fuzzy neighborhood thresholding based self organizing neural network and morphological operations. We show that about 98% detection is achieved at 5% false alarm of a particular scenario outperforming traditional energy detection.',\n",
       " 'In this paper a new automated method for sorting and grading of mangos based on computer vision algorithms is presented. The application of this system is to replace the existing manual technique of sorting and grading used in India. The system is developed for Alphonso mangos, the premium variety of mango exported from India. The developed system was able to sort the Alphonso mangos with an accuracy of 83.3% and can identify a defective skin up to an min area of 6.093845x10(-4) sqcm.',\n",
       " \"This paper-presents a model for real-time computer vision applications based on the synchronous data flow (SDF) methodology. To accomplish the needs of computer vision applications (mainly access to pixel neighborhood and access to previous video frames) a new buffer concept called Structured Buffers replaces the FIFO buffers used in the SDF model: The model minimizes the system's latency and memory consumption and allows an execution time analysis which: is an essential prerequisite for real-time applications. Finally a line-based edge vectorization is presented as an example of the usage of the presented model.\",\n",
       " \"Applications to image/signal processing and analysis have been studied since the very early years in the history of Evolutionary Computation up to a degree of popularity which has allowed terms like Evolutionary Computer Vision (ECV) and Evolutionary Image Processing (EIP) to become common among researchers. Within these fields, the role of EC has gone well beyond basic optimization of the parameters of traditional Computer Vision (CV) or Image Processing (IP) algorithms or mere use within those algorithms which comprise an optimization stage anyway. This paper, far from having the pretence of making an exhaustive review, tries to sketch the motivations behind the success of ECV/EIP, the present status of research in such a field, and a personal view of its possible developments in the near future, based on the authors' more than 20-year long direct experience.\",\n",
       " 'The topic Visual Disabilities and Computer Vision are the most researched topics of recent years. Researchers have been trying to combine two topics to create most usable systems to the visually disabled to aid them in their day to day tasks. In this research, we are trying to create an application which is targeting children between the age of 6-14 who suffers from visual disabilities to aid them in their primary learning task of learning to identify objects without a supervision of a third-party. We are trying to achieve this task by combining latest advancements of Computer Vision and Artificial Intelligence technologies by using Deep Region Based Convolutional Networks (R-CNN), Recurrent Neural Networks (RNN) and Speech models to provide an interactive learning experience to such individuals. The paper discusses.',\n",
       " 'Carrying out Food and Agricultural product Quality Inspection manually is a laborious task. Maintaining consistent product quality is not possible due to human errors. Customer expectations are increasing day by day and seek products of high quality. However, in India the Inspection of Food and Agricultural produce is done manually in most of the cases. Work in the Computer Vision area is being extensively carried out these days to provide automated, non-destructive and cost-effective techniques to achieve this. Computer Vision uses Image processing as one of the intermediary tools to achieve the ultimate objective. Cashew is one of the important commercial crops of India. The objective of this research is to provide an image processing method for measuring cashew kernel Size (i.e. length and width/height) with an accurate and smallest relative error.',\n",
       " 'The combination of RFID and computer vision systems is an effective approach to mitigate the limited tag localization capabilities of current RFID deployments. In this paper, we present a hybrid RFID and computer vision system for localization and tracking of RFID tags. The proposed system combines the information from the two complementary sensor modalities in a probabilistic manner and provides a high degree of flexibility. In addition, we introduce a robust data association method which is crucial for the application in practical scenarios. To demonstrate the performance of the proposed system, we conduct a series of experiments in an article surveillance setup. This is a frequent application for RFID systems in retail where previous approaches solely based on RFID localization have difficulties due to false alarms triggered by stationary tags. Our evaluation shows that the fusion of RFID and computer vision provides robustness to false positive observations and allows for a reliable system operation.',\n",
       " 'In this paper we realize an application of a computer vision method to acquire some basic information to control the Automated Guided Vehicles (AGV). Firstly, we design markers refer to some special meanings (such as Go Straight, Left Turn, Right Turn and Stop). Secondly, we use camera to capture the markers, using HOG feature and classify the markers with SVM predictor. After that we calculate the distance and deviation angel from the markers to the AGV. Finally, we send the information to the AGV and control the movement of it.',\n",
       " \"Automatic guided vehicles (AGV) vision system is an important research area in computer vision. In order to recognize the road lane quickly and effectively, this paper presented a algorithm using Fuzzy reasoning based on Hough transform to solve this problem, which improves the entire system's real-time performance. After our tests on the test vehicle, this method can speed up the road lane recognition velocity phenomenally, and it also can improve the stability in driving.\",\n",
       " 'The technological progress in image acquisition, image processing, and improvements in the field of big data in the last decades opens a wide range of possible applications for image processing using analytical methods. The most popular concepts in this field of tension are computer vision and image mining. However, the issue is that there is no clear distinction between these two concepts in the current scientific discussion. To address this need, a structured literature review was carried out. Thereby, the key characteristics as well as the process of image mining could be identified and compared to computer vision. Thus, this research in progress study was a first step towards differentiating these two concepts. In conclusion, a hypothesis could be derived, which will be considered now in our ongoing research.',\n",
       " 'This paper presents a human-centered vision of unmanned but crewed platforms, combining an onshore collaborative hybrid environment and an intelligent offshore oilfield. Computer Science plays a major role in this vision since digital environments are paramount for the job. We discuss the importance of research in the fields of Computer Graphics, Virtual Reality and Computer Supported Cooperative Work in this context and present the basic technologies needed to accomplish the task. We instantiate our vision appointing the results of some initial experiments using Virtual Reality in submarine installation operations.',\n",
       " 'Climbing robots that climb flat structures using suction cups or magnets are commonly described in the literature. However, robots that can autonomously find randomly placed handholds, and then plan and climb a route up the wails using those handholds, have not been described. A low cost robot has been designed by Dartmouth College students to climb a near-vertical indoor climbing wall using computer vision to locate handholds, and force feedback to maintain pressure on handholds. Using a variation of the probabilistic roadmap algorithm, a climbing route up the wall is planned and then executed.',\n",
       " 'This paper presents the implementation of an algorithm based on elementary computer vision techniques that allow an UAV (Unmanned Aerial Vehicle), or robot, to identify obstacles in front of it and quickly decide a better way to move in order to avoid them, without demanding complex software or high computational performance, using only a trivial camera and applying no more than two mathematical treatments on images. We implement and test this solution in a simulation environment, although it can be readily applied to UAVs in real life.',\n",
       " 'Twenty-first century has endorsed for remarkable growth in technical as well as economic sectors, yet there remains the lack of proper waste management techniques in most countries. This paper focuses on implementation of a fully autonomous robotic device as part of the initial stage to ensure a hygienic environment for us to live in. The main idea was to design and implement a robotic device that collects garbage lying on corridors, large halls, or even a house, by recognizing the refuse using a computer vision system and picking up the same using a Robotic arm. This was realizable through expeditious background subtraction algorithms, which was later implemented on the computer vision module of a motor driven robot. The device embodies a tough chassis, a computer vision and video analysis module, microcontroller module, a sensor network and a precise robotic arm with actuators. The algorithms incorporated to the intelligence of the robot adopt the fundamentals of background subtraction algorithms in order to obtain accurate results accompanied with a faster computational speed. The Image processing unit was implemented using a Raspberry Pi 3 and a Pi Camera module.',\n",
       " 'Algorithmic fairness is frequently motivated in terms of a trade-off in which overall performance is decreased so as to improve performance on disadvantaged groups where the algorithm would otherwise be less accurate. Contrary to this, we find that applying existing fairness approaches to computer vision improve fairness by degrading the performance of classifiers across all groups (with increased degradation on the best performing groups). Extending the bias-variance decomposition for classification to fairness, we theoretically explain why the majority of fairness methods designed for low capacity models should not be used in settings involving high-capacity models, a scenario common to computer vision. We corroborate this analysis with extensive experimental support that shows that many of the fairness heuristics used in computer vision also degrade performance on the most disadvantaged groups. Building on these insights, we propose an adaptive augmentation strategy that, uniquely, of all methods tested, improves performance for the disadvantaged groups.',\n",
       " \"Embedded computer vision applications increasingly require the speed and power benefits of single-precision (32 bit) floating point. However, applications which make use of Levenberg-like optimization can lose significant accuracy when reducing to single precision, sometimes unrecoverably so. This accuracy can be regained using solvers based on QR rather than Cholesky decomposition, but the absence of sparse QR solvers for common sparsity patterns found in computer vision means that many applications cannot benefit. We introduce an open-source suite of solvers for Eigen, which efficiently compute the QR decomposition for matrices with some common sparsity patterns (block diagonal, horizontal and vertical concatenation, and banded). For problems with very particular sparsity structures, these elements can be composed together in 'kit' form, hence the name QRkit. We apply our methods to several computer vision problems, showing competitive performance and suitability especially in single precision arithmetic.\",\n",
       " 'The Codification of Argopecten Purpuratus is a process, where the Stem and Coral are classified by their weight in different codes. This process is done manually, therefore is linked to the subjectivity and the fatigue of people involved in the work. The use of computer vision is an alternative to automate this process. The present work proposes a method to classify the Argopecten Purpuratus based on determination of weights by conversion and adjustment factors. These factors use the area of the whole scallop and of the coral to make the estimation. Results of experiments show that the computer vision system achieved an overall acccuracy of 98%',\n",
       " 'This paper reports on recommendations and results from a panel discussion and a workshop devoted to the theme of education in areas that involve image computation, One specific set of contributions is in the area of integrating image computation into required core courses such as Introduction to Computing and Data Structures. Another area of specific contributions is the development of undergraduate elective courses on topics such as robotics and medical image analysis. A third area of contributions is the improvement of traditional image processing and computer vision courses.',\n",
       " 'Inspection of food quality is an important operation in food and agro industries. Nowadays computer vision is frequently used for such operations as it can provide fast, economical, non-invasive, consistent and objective assessment. This paper presents a study on identifying the qualitative grades of rice bran using computer vision. The study is performed using three samples of rice bran collected from rice mills along with their test reports to confirm their qualitative difference. The images of individual samples were captured in a controlled illumination environment. The image features were extracted from the cropped images after the required color conversion. The constructed feature sets were subjected to principle component analysis (PCA) for observing the cluster formation and also the K-Means cluster analysis to derive the cluster centers. The clustering analysis results show the potential of the presented method for identification of rice bran grades.',\n",
       " \"Analyzing human pupillary behavior is a noninvasive and alternative method for assessing neurological activity. Changes in this behavior are correlated with various health conditions, such as Parkinson's, Alzheimer's, autism and diabetes. Examining pupil behavior is a simple, low-cost method that can be used as a complementary diagnosis in comparison with other neurological evaluation methods. This approach is made by recording the pupillary behavior against light stimuli and measuring the pupil diameter through the video. The relation of pupillometry with digital image processing creates a dependency for computer vision based systems. Therefore, this paper presents a systematic review of the literature (SRL) conducted in order to analyze the progress of pupillometry systems based on computer vision. The main goal was to establish the state of art and identify possible gaps.\",\n",
       " 'Roadway gate automatic control system with the use of fuzzy inference and computer vision intellectual technologies is proposed. The paper is dedicated to operating quality amelioration of transport access monitoring and control system based on recognition technology of transport number plates. The factors influencing recognition reliability and operating quality of the system as a whole are determined. The schematic structure of intellectual control system and the algorithms fusion of computer vision and fuzzy logic systems are provided. The optimal criterion, based on a training set, necessary for fuzzy system values adjusting is introduced. The practice-oriented examples of real weather conditions aggravating the image quality and decreasing the numbers recognition level are given, and the reactions of the proposed intellectual control system are presented.',\n",
       " 'Deep architectures with convolution structure have been found highly effective and commonly used in computer vision. With the introduction of Graphics Processing Unit (GPU) for general purpose issues, there has been an increasing attention towards exploiting GPU processing power for deep learning algorithms. Also, large amount of data online has made possible to train deep neural networks efficiently. The aim of this paper is to perform a systematic mapping study, in order to investigate existing research about implementations of computer vision approaches based on deep learning algorithms and Convolutional Neural Networks (CNN). We selected a total of 119 papers, which were classified according to field of interest, network type, learning paradigm, research and contribution type. Our study demonstrates that this field is a promising area for research. We choose human pose estimation in video frames as a possible computer vision task to explore in our research. After careful studying we propose three different research direction related to: improving existing CNN implementations, using Recurrent Neural Networks (RNNs) for human pose estimation and finally relying on unsupervised learning paradigm to train NNs.',\n",
       " 'The existing shadow detection methods have achieved good results on standard shadow datasets such as SBU and UCF. However, in actual large-scale scenes, key objects covered by shadows are often regarded as shadows, which may harm computer vision tasks. In the paper, we are the first to propose the Object-aware Shadow Detection Network (OSD-Net) model for computer vision tasks in complex scenes. It introduces the direction-aware spatial context (DSC) module to detect shadows, uses semantic segmentation with Mask RCNN to extract key objects in the picture, and designs a function to perform mask fusion. Qualitative experiments have been performed to test OSD-Net on three public datasets commonly used in computer vision. Compared with popular shadow detection methods, OSD-Net is able to effectively protect the key targets in the picture from being misjudged as shadows, and ensure shadow detection accuracy.',\n",
       " 'The global crisis of pollution has influenced our lives adversely. Preparing the most salutary in reducing the catastrophe has its dominant necessity. Due to the expanding population, plastics in our neighborhood water bodies have increased for the past ten years, progressing the obligation to clean it up. This work intends towards the analysis of an automatic garbage collection system for a vessel/boat with improved performance. The focus of this paper is the development of a sorting and classification mechanism/model for the collected garbage by the vessel/boat incorporating the application of convolutional neural network (CNN) and computer vision. Using CNN & computer vision, the garbage features can be extracted and can be classified further into biodegradable and non-biodegradable with a prediction accuracy of more than 90%, which can be further increased by increasing the data set quantity for the model.',\n",
       " 'The problem of reconstructing a three-dimensional scene from its projections is one of the most urgent and studied among the problems solved by computer vision methods. Most often, such systems are implemented by creating geographically distributed complexes that require the transfer of a large amount of data between their components, which creates a large load on the transmission lines. Scene reconstruction using projections obtained from several observation points is a special case and is not always possible, especially when used in unmanned aerial vehicles or autonomous vehicles. To resolve these limitations, the possibility of using a computer stereoscopic vision system and a three-dimensional convolutional neural network capable of reconstructing a complete scene from a single image containing subjective characteristics of scene objects, such as color and depth, is considered in this work, creating its three-dimensional representation. The article describes the architecture and principle of operation of a neural network, and also builds a model of a computer stereo vision stand for image registration. An image formation algorithm is proposed for this model.',\n",
       " 'In this paper we provide a technical perspective in support of the view that mobile videoconferencing is evolving towards immersiveness, with a detailed discussion of the technological readiness of the most critical components, namely the integration of depth and computer vision into the traditional video processing engine. Immersive user experiences are achieved by adding new dimensions to the perceptual space of interaction. Adding depth creates a sense of presence, and providing additional points of view creates a visual flexibility associated with real -life interaction. As the quality of service improves along with the computational capabilities of mobile platforms, we expect to see an evolution towards immersive mobile videoconferencing. Depth capture or extraction is the first element that allows 3D experiences as well as view synthesis. Depth maps or 3D models may be obtained by multiple methods but without application -driven post -processing there is no guarantee that the synthesized outputs provided to the end users will provide the expected quality of experience. The necessary convergence between video processing and computer vision also implies a shift from error -based performance metrics commonly used in computer vision, to visual quality metrics of the type used in video processing.',\n",
       " 'Graph Neural Networks (GNNs) are a family of graph networks inspired by mechanisms existing between nodes on a graph. In recent years there has been an increased interest in GNN and their derivatives, i.e., Graph Attention Networks (GAT), Graph Convolutional Networks (GCN), and Graph Recurrent Networks (GRN). An increase in their usability in computer vision is also observed. The number of GNN applications in this field continues to expand; it includes video analysis and understanding, action and behavior recognition, computational photography, image and video synthesis from zero or few shots, and many more. This contribution aims to collect papers published about GNN-based approaches towards computer vision. They are described and summarized from three perspectives. Firstly, we investigate the architectures of Graph Neural Networks and their derivatives used in this area to provide accurate and explainable recommendations for the ensuing investigations. As for the other aspect, we also present datasets used in these works. Finally, using graph analysis, we also examine relations between GNN-based studies in computer vision and potential sources of inspiration identified outside of this field.',\n",
       " 'Computer-vision applications typically rely on graphics processing units (GPUs) to accelerate computations. However, prior work has shown that care must be taken when using GPUs in real-time systems subject to strict timing constraints; without such care, GPU use can easily lead to unexpected delays not only on the GPU device but also on the host CPU. In this paper, a software library is presented that can detect the improper use of GPUs for safety-critical computer-vision applications. This library was used to analyze several GPU-using sample applications available as part of OpenCV, a popular computer-vision library, revealing the presence of issues in all ten applications considered. Additionally, a case study is presented, detailing the response-time improvements to one of the applications when such issues are corrected.',\n",
       " 'In recent years, hand gesture recognition framework as an effective sign language tool has been extensively explored by many researchers. This paper presents an idea of developing a framework using computer vision for hand gesture based sign language recognition from real-time video stream. The proposed system identifies hand-palm in video stream based on skin color and background subtraction scheme as opposed to the conventional techniques of using gloves or markers as interface and thereby makes an effort of exploring the possibility of a suitable computer vision framework for hand gesture recognition. We have also proposed an iterative polygonal shape approximation strategy in fusion with a special chain-coding scheme for shape-similarity matching. This proposed framework considers digits as important symbols of sign language and it successfully recognizes hand gestures corresponding to various numerical digits with acceptable accuracy.',\n",
       " 'The works presented by the authors at MECO-2013-MECO-2015 conferences as well as the other works considered the components of image superimposition techniques in computer vision systems with different degree of detail. In this paper for the first time a complete description of the automatic image superimposition technique is provided. This technique is suitable for the images both of a similar and of a various nature. The results of the implementation of the proposed technique for an image sequence from the aircraft on-board camera are represented. Also for the first time the practical utilization of the complex contour analysis theory to solve the problem of establishing one-to-one correspondence between object sets in the pair of real images for aviation computer vision systems is accomplished.',\n",
       " 'This article analyzes the actual case in practical engineering field of transferring goods from an irregular tray where objects are placed in each section within a relatively large tolerance to a fixing fixture through a robot. Based on the transformation of a Cartesian coordinate system, it can compute the optimized movement of the robot in order to complete the needed rotation and translation in one single operation. Computer vision system and robot motion control are combined. The two-dimensional Cartesian coordinate systems of computer vision and robot need to be matched in order to the let the two systems work together. At the same time calibration and matching of the measurement units of the two systems as well as the compensation of image distortion play a key role in the practical implementation. This solution leads to a precise and time-efficient positioning.',\n",
       " 'this paper presents the design and VLSI implementation of a CVE (Computer Vision Engine) for real-time video analysis. It offloads CPU/GPU for the power-hungry computation for various vision tasks such as face detection, object detection, motion tracking, etc. The design features 22 computation kernels and is divided into three main categories. The proposed CVE is integrated in a smart video surveillance SoC (System on Chip) and fabricated with TSMC 28nm technology. The total hardware costs are 392K gates and 753 KB memory. The measured results show that the design is able to achieve 1920x1080@30fps real-time video analysis when running at 400MHz. The total power consumption is 20mW and 0.32nJ/pixel of energy efficiency.',\n",
       " \"Dental care is important to pupils in elementary school since it affects their physical and mental development significantly. Therefore, dental care education is an important issue for governments around the world. Based on this, this study proposed a computer vision interactive game for supporting dental care education in Taiwan. Different from traditional lecture-based instruction, this study developed an interactive educational game that applied artificial intelligence technology to develop a computer vision service for recognizing pupils' dental photos and thus engaged pupils in playing an interactive game to prevent caries. To evaluate the proposed approach, an experiment was conducted in an elementary school and twenty-one fifth grade students was asked to participant in the experiment. The results indicated that the proposed approach can improve students' learning motivation, learning attitude, and learning achievement.\",\n",
       " 'In this paper a concept for Virtual Machine Vision is proposed using a commercial Computer Aided Robotics software called RobCad. The system utilizes ideal virtual cameras and lights for the simulation of a real vision system. Sensory data is sent to a vision software for data analysis. The Virtual Machine Vision together with the simulation model can be used to offline programming of a vision system. Experiments have been performed by capturing images of a test piece both in the virtual environment and in a physical experimental rig. To evaluate the concept, image analysis has been performed on these images using the same vision software. The results from the vision analysis of both the virtual and the real images are compared and show good agreement. The proposed system seems to be very promising and further development is ongoing.',\n",
       " 'Collaborative navigation is the most promising technique for infrastructure-free indoor navigation for a group of pedestrians, such as rescue personnel. Infrastructure-free navigation means using a system that is able to localize itself independent of any equipment pre-installed to the building using various sensors monitoring the motion of the user. The most feasible navigation sensors are inertial sensors and a camera providing motion information when a computer vision method called visual odometry is used. Collaborative indoor navigation sets challenges to the use of computer vision; navigation environment is often poor of tracked features, other pedestrians in front of the camera interfere with motion detection, and the size and cost constraints prevent the use of best quality cameras resulting in measurement errors. We have developed an improved computer vision based collaborative navigation method addressing these challenges using a depth (RGB-D) camera, a deep learning based detector to avoid using features found from other pedestrians and for controlling the inconsistency of object depth detection, which would degrade the accuracy of the visual odometry solution if not controlled. Our analysis show that our method improves the visual odometry solution using a low-cost RGB-D camera. Finally, we show the result for computing the solution using visual odometry and inertial sensor fusion for the individual and UWB ranging for collaborative navigation.',\n",
       " 'Machine vision has become a key technology in the area of quality control. Vision systems is primarily focused on computer vision in the context of inspection of the products such as food, pharmaceuticals. The system can consist of a number of cameras all capturing, interpreting and signaling individually with a control system related to some predefined algorithms. The analysis of citrus fruits using various assorted parameters revealing the diseases afflicting Citrus fruits and isolation of the same using Image Processing and Data Mining Techniques is the core area discussed here with.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Vision-based driver assistance systems is one of the rapidly growing research areas of ITS, due to various factors such as the increased level of safety requirements in automotive, computational power in embedded systems, and desire to get closer to autonomous driving. It is a cross disciplinary area encompassing specialised fields like computer vision, machine learning, robotic navigation, embedded systems, automotive electronics and safety critical software. In this paper, we survey the list of vision based advanced driver assistance systems with a consistent terminology and propose a taxonomy. We also propose an abstract model in an attempt to formalize a top-down view of application development to scale towards autonomous driving system.',\n",
       " 'Computer vision makes mobile robot to create environment map for outdoor environment in the focus of straight line. The human vision and cognitive based analysis results correspondence error free spatial map. The proposed map building method computes shear angle values for every grid in forward direction. This shear factor value was varies from 44 degrees to 65 degrees and the value is 0.707 to 0.88. This cognitive vision map mainly useful in intelligent navigation with identifying large pits and escaped from wheel slippage in space terrains.',\n",
       " \"Because of the social change and the advanced technologies, artificial intelligence technology has been developed rapidly. Robots are entering the human's living life soon. Biped robots are expected to be employed in some dangerous places. This study presented that the biped robot moved up and down stairs through computer vision. The processes included 1) Gray level co-occurrence matrix detected to move up or down; 2) Canny edge detection operator is adopted for the clear stair edge images; 3) Compute the distance between the stairs and the camera using the Zhang's calibration.\",\n",
       " 'Superquadrics are one of the ideal shape representations for adapting various kinds of primitive shapes with a single equation. This paper revisits the task of representing a 3D human body with multiple superquadrics. As a single superquadric surface can only represent symmetric primitive shapes, we present a method that segments the human body into body parts to estimate their superquadric parameters. Moreover, we propose a novel initial parameter estimation method by using 3D skeleton joints. The results show that superquadric parameters are estimated, which represent human body parts volumetrically.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'In this paper, the hardware setup of an autonomous robotic vehicle is developed. The controller of the vehicle is developed based on the computer vision technology which became more smart due to the application of popular tool open CV. An algorithm based on color detection is developed to navigate the vehicle in different directions. Raspberry Pi camera is used to access images which are the indications of the directions of the movements such as stop, left turn, right turn. The efficacy of the proposed algorithm is verified experimentally.',\n",
       " 'This paper presents a system of obstacle avoidance based on computer vision for a quadrotor in the outdoor environment. The system first acquires video stream from the camera and obtains the 3D information of specific obstacle by Harris Corner detector method. Based on the 3D information of obstacle, we can design the modeling of the flight environment and plan the flight path by the A* algorithm in the 3D space. Finally, simulations and series of experiments are carried out which show that the system is effective in avoiding obstacles for a quadrotor helicopter in the outdoor environment.',\n",
       " \"An important factor of system performance for computer vision applications such as optical tracking systems or industrial inspection systems is the frame rate. Frame rate measurement requires access to either the application software or hardware. In order to be independent of different standards used by the application under test a non-intrusive method for frame rate measurement is proposed. The system consists of a hardware pattern generator which can be used 'stand-alone' as well as in combination with signal processing software to estimate the frame rate of a given application.\",\n",
       " 'The design of robust classifiers, which can contend with the noisy and outlier ridden datasets typical of computer vision, is studied. It is argued that such robustness requires loss functions that penalize both large positive and negative margins. The probability elicitation view of classifier design is adopted, and a set of necessary conditions for the design of such losses is identified. These conditions are used to derive a novel robust Bayes-consistent loss, denoted Tangent loss, and an associated boosting algorithm, denoted TangentBoost. Experiments with data from the computer vision problems of scene classification, object tracking, and multiple instance learning show that TangentBoost consistently outperforms previous boosting algorithms.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'In this paper, we present Adaptive Vision Studio (AVS)-a novel tool for creating image processing and analysis algorithms. AVS has been applied in post-graduate computer vision course for students of Automatic Control and Biotechnology at Silesian University of Technology. This software is a powerful environment with ready-for-use image analysis filters for computer vision experts as well as for engineers, who are beginners in this field. AVS has been published as a freeware version for non-commercial and educational purposes recommended for students and engineers, who want to learn how to develop complex image processing algorithms. Lite version of AVS is freely available at https://adaptive-vision.com.',\n",
       " 'This paper presents a real-time algorithm of accurately identifying helipad and estimating the state information for landing an unmanned aerial helicopter autonomously via computer vision. The algorithm estimates the instantaneous attitude and position parameters of the helicopter relative to the helipad from continuously tracked points using the optical flow method. The vision system, consisting of a calibrated monocular camera, a helipad and an experiment platform, can perform image processing, helipad recognition, feature extraction, target tracking and motion estimation. The experimental results show that the algorithm is accuracy, robust and fast.',\n",
       " 'Computer vision has been widely used in on-line inspection of electronic components. In this paper, we present a computer vision system using structured lighting, which provides us with an efficient solution for solder joint inspection. Our system uses a novel structured-lighting inspection technology to overcome some difficulties that traditional computer vision systems often experience, We developed a slant map surface shape estimation technique for the solder joint. From this technique, a solder joint can be determined to lie a good (concave), bad (convex), bridged solder joint, or solder joint with surplus solder, or lacking solder.',\n",
       " 'This paper presents a general algorithm to track the line on the ground based on computer vision, which utilizes Raspberry Pi 2 to complete the image processing. The algorithm accomplishes image processing firstly, then recognizes the line by edge detection and scanning model. The proposed algorithm can be used in a variety of platforms including intelligent car system, robot platform, UAV platform and other mobile platforms. In this paper, the algorithm is verified by quadrotor platform and achieves great line-tracking effect by tuning parameters. Series of experiments show that the algorithm has great validity, robustness, and generality.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Motion vision can be used to determine world structure from a video sequence. In harvester machine automation, the potential is that trees could be measured from it distance. Based on the measurements, tree cutting could be optimized and harvester automation increased, resulting in higher resource utilization efficiency. However, a natural environment poses challenges to any computer vision task. This paper presents computer vision algorithms that are applied to a forest environment. The results show that dense optical flow can be computed from a real-world forest data accurately enough as to enable instantaneous dense structure estimates of the visible image scene.',\n",
       " 'nan',\n",
       " \"Today's, blanched or roasted hazelnuts are widely used in chocolate and biscuits industry. Partly skin removed hazelnuts kernels are caused to deterioration in flavor and taste of the chocolate. Many of chocolate and biscuits cooperation in Turkey are using traditional manual selection methods. However, manual selection has high error rate, time consuming and costly. In this study, the partly skin removed hazelnut kernel, skin removed and rotten hazelnuts kernels which are obtained from processed kernels are classified by using computer vision approach. In the proposed study, the processed hazelnut kernels are classified with 93.57 % classification accuracy rate.\",\n",
       " \"Vision - based road lane detection and reconstruction is a very common interest in the field of computer vision (CV). It has numerous application ranging from autonomous vehicle to driver assist and support systems technology. These researches are always focusing on both accuracy and complexity of the system's output; however, none of these uses Macro Block (MB) method. This paper introduces the characteristics of MB method used for spatial road lane detection and reconstruction subjected to different environment conditions; different MB size; and different function approximations.\",\n",
       " 'To overcome the data movement bottleneck, near-sensor and in-sensor computing are becoming more and more popular. However, in the existing near-/in-sensor computing architectures for vision tasks, the effect of the image signal processing (ISP) pipeline, which is of great importance to the final vision performance [1], is always ignored. In this work, we propose a synthesized RAW image-based end-to-end computer vision paradigm, taking the effect of ISP pipeline into account. In the proposed approach, a generative adversarial network (GAN)based tool is used to convert the fully processed color images to their corresponding RAW Bayer versions, generating the training data for end-to-end vision models. In the inference stage, RAW images from the sensor are directly fed to the end-to-end model, bypassing the entire ISP pipeline. Experimental results show that by training/tuning the CNN models using synthesized RAW images, it is possible to design an end-to-end (from RAW image to vision task) vision system that directly consumes RAWimage data from the sensor with negligible vision performance degradation. By skipping the ISP pipeline, an image sensor can be directly integrated with the back-end vision processor without a complex image processor in the middle, making near-/in-sensor computing a practical approach.',\n",
       " 'nan',\n",
       " 'With the rapid adoption of laparoscopic robotic surgery, numerous unanticipated safety and reliability challenges have surfaced for teleoperated devices. A large fraction of the procedure failures are accounted due to the sensor depravation, limited field of view, and lack of planning during procedures. This article surveys the benefits of computer vision for preoperative, intraoperative, and postoperative surgical stages to assist with planning; tool detection, identification, pose tracking, and augmented reality; and surgical skill assessment and retrospective analysis of the procedure. The appropriate use of these computer vision techniques has the potential to improve the safety and efficacy of robotic surgery as it becomes more commonplace.',\n",
       " 'Synthetic aperture radar is a popular instrument for high-resolution imaging. Usage of state-of-the-art image and signal processing solutions allows to significantly increase the efficiency of such systems. In the paper two important applications of computer vision algorithms are described. In particular, usage of local feature extraction algorithms for the radar image stitching. This results in the automatical panorama creation without usage of the navigation data. In addition, the road location approach based on the stroke width transform and contour analysis is proposed. Application of the developed methods is illustrated with real SAR data examples.',\n",
       " 'We explore the opportunity to harness electroencephalograph (EEG) signals generated during human visual processing to enhance computer vision systems. We review the challenging task of categorizing objects, such as faces, in images and then describe methods that can be used to combine the complementary competencies of human and machine computation to achieve improved recognition performance. We present the results of several experiments where brain signals, recorded from people examining images, are used to enhance the performance of vision systems on categorization tasks. We find that significant gains in classification accuracy can be achieved with the human-aided vision systems.',\n",
       " 'We present a real-time approximate simulation of some camera errors and the effects these errors have on some common computer vision algorithms for robots. The simulation uses a software framework for real-time post processing of image data. We analyse the performance of some basic algorithms for robotic vision when adding modifications to images due to camera errors. The result of each algorithm / error combination is presented. This simulation is useful to tune robotic algorithms to make them more robust to imperfections of real cameras.',\n",
       " 'The need to have good security, either in the streets, at home or at workplaces, cannot be overemphasized. Due to its significance, security experts continue to improve the mechanism and the tools used to manage security issues. The revolution in computing and information technology has significantly affected how people deal with security. Computer vision application has been developed for security purposes, especially, by improving surveillance systems. Computer vision can manage face detection, motion detection, person identification, tracking, access control, and interpretation of movement. Most of these tasks can be used to improve security surveillance. Computer integrated systems can be used to identify strange behavior and aid security management. Forensic science involves the analysis of images to establish patterns. As a result, strategies aimed at utilizing computer vision focus on the improvement of image computation power of computer systems. Surveillance processes based on the use of cameras involve different phases: environmental design, discovery of movements, analysis and description of behaviors, organizing objects in motion, tracing as well as discovery of individuals. This paper seeks to analyze how computer systems can be trained to identify digital patterns in order to help in surveillance processes including tracking of strange behavior and crime.',\n",
       " 'In this paper, we start a preliminary discussion about the development of software agents developed for Computer Vision. Such agents must be heterogeneous and adaptive in nature to satisfy increasing requests for integration-cooperation capability exhibited by current Computer Vision applications (e.g., biomedical imaging and image retrieval on the Web). We give a brief introduction to the overall architecture design and,ve focus on lowest level agents, called W-bots (Working Software Robots). We define the nature of such agents and describe their ways of interaction, and their adaptive capabilities. Finally, Me present an example of the proposed architecture utilizing VDM++ to specify agents as active objects.',\n",
       " 'nan',\n",
       " 'Rehabilitation robot for upper limbs can help physicians or therapists in conducting repetitive rehabilitation to patients. This robot is constructed by several electromechanical actuators, sensors, controllers, and the main computer. The patient holds a robot gripper in the left and right hands and moves the gripper according to a rehabilitation protocol determined by the physician who is in charge. A computer vision-based position sensor is designed to measure wrist position without using any marker. Moreover, a new algorithm is proposed to filter coordinate values measured by the video camera and extract the position estimate of the wrist. This sensor system estimates positions of the wrist using OpenPose library. The experimental performance was evaluated in terms of precision and speed for several resize values. Experimental results indicate that the pose-based computer vision algorithm can potentially be used for the upper limbs rehabilitation robot. Our finding stimulates further research to find more consistent values of the precision performance index.',\n",
       " 'This paper proposes a method of removing ambiguities in robot tasks by a multimodal human-robot interface consisting of verbal and nonverbal communication. Such ambiguities often arise from failures of the robot vision system, However, it is not easy to solve this problem only by improving computer vision techniques. Thus, our robot asks a human such a question that a natural reply to it will contain helpful information to adapt the vision system for the current situation. We present a robot system if hat can bring the object ordered by a human by verbal and nonverbal behaviors.',\n",
       " 'nan',\n",
       " 'Computer vision technology has been widely used in on-line inspection. In this paper, we present a computer vision system using structured light which provides us with an efficient solution for solder joint inspection. Our system uses a novel inspection technology to overcome some technology bottlenecks that traditional computer vision system experienced. We focus the SMD inspection process on the solder joint defects in this paper. We developed a surface shape estimation technique for solder joint using slant map. From this technique, a solder joint can be determined to be a good (concave), bad (convex), bridged solder joint, or solder joint with surplus solder, or lacking solder.',\n",
       " 'nan',\n",
       " 'The paper deals with the design and new solutions of application software with the aim to detect and recognize objects sensed by a camera. Objects of the sensed scene were determined and recognized after previous digital processing of data delivered by the camera. To this end the computer vision learning neural-based methods of feature extraction were used. The proposed application software may be used in various applications where the tracking of objects and understanding of a real scene is required. The results obtained will be used for teaching and research at the FEI STU in Bratislava.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Just like in humans vision plays a fundamental role in guiding adaptive locomotion, when designing the control strategy for a walking assistive technology, the use of computer vision may substantially improve modulation of the assistance based on the external environment. In this letter, we developed a hip exosuit controller able to distinguish among three different walking terrains through the use of an RGB camera and to adapt the assistance accordingly. The system was tested with seven healthy participants walking throughout an overground path comprising of staircases and level ground. Subjects performed the task with the exosuit disabled (Exo Off), constant assistance profile (Vision Off), and with assistance modulation (Vision On). Our results showed that the controller was able to classify in real-time the path in front of the user with an overall accuracy per class above the 85%, and to perform assistance modulation accordingly. Evaluation related to the effects on the user showed that VisionOn was able to outperform the other two conditions: weobtained significantly higher metabolic savings than Exo Off, with a peak of approximate to-20% when climbing up the staircase and approximate to-16% in the overall path, and than Vision Off when ascending or descending stairs. Such advancements in the field may yield to a step forward for the exploitation of lightweight walking assistive technologies in real-life scenarios.',\n",
       " 'In computer vision, physics plays an important role in several applications. In this work, we teach a machine to detect the mechanical laws of motion of physical objects using video, and show how the results can be useful for computer vision tasks. We assume no prior knowledge of physics, beyond a temporal stream of bounding boxes. The problem is very difficult because a machine must learn not only a governing equation (e.g. projectile motion) but also the existence of governing parameters (e.g. velocities). We evaluate our ability to represent the physical laws of motion in video, such as the movement of a projectile or circular motion, in both real and constructed videos. These elementary tasks have textbook governing equations and enable ground truth verification of our approach. To establish the importance of the proposed method, we show a real-world use case in the domain of object tracking in confounding scenes, where existing state-of-the-art algorithms fail. Incorporating physics into computer vision not only serves the purpose of curiosity-driven research, but also provides an inductive bias for computer vision applications like object tracking.',\n",
       " \"A new variant of multi ocular stereo vision has been developed. The system involves a single camera and two orthogonal planar mirrors. The resulting device is a low-cost, compact sensor, particularly suitable for depth determination in robot vision applications. The motivation for the work is the need for a sensor determining spatial coordinates of a robot's tool and the object to be processed, The system inherently posses fewer calibration parameters and provides a higher accuracy in the depth determination than traditional two-camera stereo systems. A prototype of the new device has been built, and test results are presented. ICPR Topics: Applications, Robotics and Architecture (automation and robotics, smart sensors, range imaging), Computer Vision and Image Analysis (early vision, scene understanding).\",\n",
       " 'With the development of deep learning, computer vision has made great progress. Computer vision training based on deep learning requires a large number of data sets. However, manual obtaining of relevant data sets is costly, and some special samples are not easy to obtain. Therefore, in order to solve the lack of data sets, a virtual training platform is designed. The platform is able to render three-dimensional scene simulation by using Unity 3D, and automatically generates visual training data sets through corresponding script file. Through the interactive interface of the platform, users can select and adjust the model, scene, light and other variables independently according to different needs, and quickly generate multi-angle pictures and corresponding annotation information. Compared with the previous computer vision training data sets obtained by manual obtain of image information and label, it has the advantages of fast speed, low cost and large scale. At the same time, use the object detection network to evaluate the data generated by the platform and the data obtained by the previous way. It can be observed that the virtual data set can achieve certain effects. Therefore, the data sets generated by this platform can replace the traditional ones to a certain extent.',\n",
       " 'We present a nonintrusive system based on computer vision for human-computer interaction in three-dimensional (3-D) environments controlled by hand pointing gestures. Users are allowed to walk around in a room and manipulate information displayed on its walls by using their own hands as pointing devices. Once captured and tracked in real-time using stereo vision, hand pointing gestures are remapped, onto the current point of interest, thus reproducing in an advanced interaction scenario the drag and click behavior of traditional mice. The system, called PointAt (patent pending), enjoys a careful modeling of both user and optical subsystem, and visual algorithms for, self-calibration and adaptation to both user peculiarities and environmental changes. The concluding sections provide an insight into system characteristics, performance, and relevance for real applications.',\n",
       " \"Computer biometrics has gained importance in recent years, both at the industrial and at the research level. Although there is an increasing need for graduates to know the fundamentals of this technology, there are still only few experiences reporting on what contents to include in courses on this discipline, or on how to teach at the master level. This paper reports the experience carried out at one Spanish university when teaching a one-semester computer biometrics course as part of a computer vision master. The related master's courses provide the students multiple synergies with the necessary background on image processing and pattern recognition required to follow the subject. An effort is made in this paper to ensure that the theory, practices and evaluation parts of our experience are set out in sufficient detail to make this course reproducible by other institutions.\",\n",
       " 'Convolutional Neural Networks (CNNs) have proved very accurate in multiple computer vision image classification tasks that required visual inspection in the past (e.g., object recognition, face detection, etc.). Motivated by these astonishing results, researchers have also started using CNNs to cope with image forensic problems (e.g., camera model identification, tampering detection, etc.). However, in computer vision, image classification methods typically rely on visual cues easily detectable by human eyes. Conversely, forensic solutions rely on almost invisible traces that are often very subtle and lie in the fine details of the image under analysis. For this reason, training a CNN to solve a forensic task requires some special care, as common processing operations (e.g., resampling, compression, etc.) can strongly hinder forensic traces. In this work, we focus on the effect that JPEG has on CNN training considering different computer vision and forensic image classification problems. Specifically, we consider the issues that rise from JPEG compression and misalignment of the JPEG grid. We show that it is necessary to consider these effects when generating a training dataset in order to properly train a forensic detector not losing generalization capability, whereas it is almost possible to ignore these effects for computer vision tasks.',\n",
       " \"Visceral Leishmaniasis (VL) is a neglected disease that affects 1 billion people in tropical and subtropical countries. In Brazil, VL causes about 3,500 cases/year. Although this disease is lethal when left untreated, the number of cases is increasing. Thus, it is necessary to study current and safety technologies for VL diagnosis, treatment, and control. Specialized laboratories carry out the LV diagnosis, and this step has great automation power through automatic methods based on computer vision to aid in diagnosis. This work aims to present state-of-the-art research on computer vision techniques to detect VL in humans and provide a theoretical basis for developing computational systems to aid in diagnosing VL. This work's contributions are finding the methodologies and algorithms used in VL automatic detection and listing the gaps in developing those systems. As a result, we find out the lack of image databases and the use of deep learning techniques is still scarce. We conclude that methodologies that use the segmentation procedure perform better in terms of accuracy and that it is possible to develop a CAD system to help diagnose VL in humans.\",\n",
       " 'Probabilistic models have been adopted for many computer vision applications, however inference in high-dimensional spaces remains problematic. As the state-space of a model grows, the dependencies between the dimensions lead to an exponential growth in computation when performing inference. Many common computer vision problems naturally map onto the graphical model frame-work, the representation is a graph where each node contains a portion of the state-space and there is an edge between two nodes only if they are not independent conditional on the other nodes in the graph. When this graph is sparsely connected, belief propagation algorithms can turn an exponential inference computation into one which is linear in the size of the graph. However belief propagation is only applicable when the variables in the nodes are discrete-valued or jointly represented by a single multivariate Gaussian distribution, and this rules out many computer vision applications. This paper combines belief propagation with ideas from particle filtering; the resulting algorithm performs inference on graphs containing both cycles and continuous-valued latent variables with general conditional probability distributions. Such graphical models have wide applicability in the computer vision domain and we test the algorithm on example problems of low-level edge linking and locating jointed structures in clutter.',\n",
       " 'Motivated by the emerging needs to improve the quality of life for the elderly and disabled individuals who rely on wheelchairs for mobility, and who might have limited or no hand functionality at all, we propose an egocentric computer vision based co-robot wheelchair to enhance their mobility without hand usage. The co-robot wheelchair is built upon a typical commercial power wheelchair. The user can access 360 degrees of motion direction as well as a continuous range of speed without the use of hands via the egocentric computer vision based control we developed. The user wears an egocentric camera and collaborates with the robotic wheelchair by conveying the motion commands with head motions. Compared with previous sip-n-puff, chin-control and tongue-operated solutions to hands-free mobility, this egocentric computer vision based control system provides a more natural human robot interface. Our experiments show that this design is of higher usability and users can quickly learn to control and operate the wheelchair. Besides its convenience in manual navigation, the egocentric camera also supports novel user-robot interaction modes by enabling autonomous navigation towards a detected person or object of interest. User studies demonstrate the usability and efficiency of the proposed egocentric computer vision co-robot wheelchair.',\n",
       " \"Computer vision systems allow identifying physical characteristics and product defects in a non-invasive and reliable form. Due to these advantages, computer vision systems have been widely accepted in the agricultural and food industries, since these industries require a high demand for objectivity, consistency and efficiency in the quality control of the product, requirements that can be met by the computer vision systems. This paper proposes a method for automatically evaluate the state of maturation of the perolera variety pineapple (Ananas Comosus) in post-harvest using computer vision techniques. The proposed evaluation procedure is implemented through a digital color-image processing based on the stages of preprocessing, segmentation, feature extraction and statistical classification. For this purpose we use images in the HSV color space, segmentation by automatic thresholding using Otsu's method, the first-order moment of the distributions of the H and S planes as features, and the Modified Basic Sequential Algorithmic Scheme (MBSAS). 1320 images were utilized, which 770 images were used in the process of training and 550 images in the evaluation process. The results of the evaluation procedure proposed in this paper were compared with the value judgment of three experts, showing that this algorithm has efficiency in the assessment close to 96.36%.\",\n",
       " 'X-ray imaging technology has been used for decades in clinical tasks to reveal the internal condition of different organs, and in recent years, it has become more common in other areas such as industry, security, and geography. The recent development of computer vision and machine learning techniques has also made it easier to automatically process X-ray images and several machine learning-based object (anomaly) detection, classification, and segmentation methods have been recently employed in X-ray image analysis. Due to the high potential of deep learning in related image processing applications, it has been used in most of the studies. This survey reviews the recent research on using computer vision and machine learning for X-ray analysis in industrial production and security applications and covers the applications, techniques, evaluation metrics, datasets, and performance comparison of those techniques on publicly available datasets. We also highlight some drawbacks in the published research and give recommendations for future research in computer vision-based X-ray analysis.',\n",
       " 'Overview and investigate time complexity of computer vision algorithms for face recognition. Main article idea is to compare two popular computer vision librarieobjs, they are OpenCV and dlib, explore features, analyze pros and cons each of them and understand in what situation each of them suit the best.Method. The technologies of computer vision, which are used for face recognition was worked out. Research of two popular computer vision libraries was conducted. Their features are analyzed and the advantages and disadvantages of each of them are estimated. Examples of building recognition application based on histogram-oriented gradients for face finding, face landmark estimation for face orientation, and deep convolutional neural network to compare with known faces. The article generalizes the concept of face recognition. The scientific basis for facial recognition and the construction of a complete recognition system was described. The basic principles of the programs for face recognition are formulated. A comparative analysis of the productivity of both libraries in relation to - the time of execution to the number of iterations of the applied algorithms was presented. Also built two simple applications for face recognition based on these libraries and comparing their performance.',\n",
       " 'While deep learning technologies for computer vision have developed rapidly since 2012, modeling of remote sensing systems has remained focused around human vision. In particular, remote sensing systems are usually constructed to optimize sensing cost-quality tradeoffs with respect to human image interpretability. While some recent studies have explored remote sensing system design as a function of simple computer vision algorithm performance, there has been little work relating this design to the state of the art in computer vision: deep learning with convolutional neural networks. We develop experimental systems to conduct this analysis, showing results with modern deep learning algorithms and recent overhead image data. Our results are compared to standard image quality measurements based on human visual perception, and we conclude not only that machine and human interpretability differ significantly but also that computer vision performance is largely self-consistent across a range of disparate conditions. This paper is presented as a cornerstone for a new generation of sensor design systems that focus on computer algorithm performance instead of human visual perception.',\n",
       " 'Driver distraction is a modern issue when operating automotive vehicles. It can lead to impaired driving and potential accidents. Detecting driver distraction most often relies on analyzing a photo or video of the driver being distracted. This involves complex deep learning models which often can only be ran on computers too powerful and expensive to implement into automobiles. This paper presents a method of detecting driver distraction using computer vision methods within an embedded environment. By taking the deep learning architecture SqueezeNet, which is optimized for embedded deployment, and benchmarking it on a Jetson Nano embedded computer, this paper demonstrates a viable method of detecting driver distraction in real time. The method shown here involves making slight modifications to SqueezeNet to be trained on the AUC Distracted Driver Dataset, yielding accuracies as high as 93% when detecting distracted driving.',\n",
       " \"Developments in satellite technology, remote sensors and drone technologies are mushrooming. These developments yield volumes of high quality scene images that require effective processing for intelligent farming applications. The recent deep learning technologies can leverage these opportunities to fuse computer vision and artificial intelligence in farming. This encompasses the big data phenomena and huge volumes of data that are captured, processed and applied for decision-making. This paper aims to give insights on the integration of computer vision for smart farming in-order to attain sustainable agriculture. Using a structured approach, this research proposes a computer vision technique for crop image feature characterization that applies in the determination of the crop's health status. To achieve this, a deep convolutional network is applied for image feature extraction and representation, and then these features are fed to the support vector-learning machine for training and subsequent image interpretation. From the experimental results, it is evident that the proposed technique generates superior visual interpretation results of scene images as compared to other methods in literature. It follows that the Global food security and agricultural sustainability can be attained through ICT enabled solutions that are integrates and works together a phenomenon referred to as smart farming.\",\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Realistic virtual worlds can serve as laboratories for carrying out camera networks research. This unorthodox Virtual Vision paradigm advocates developing visually and behaviorally realistic 3D environments to serve the needs of computer vision. Our work on high-level coordination and control in camera networks is a testament to the suitability of virtual vision paradigm for camera networks research. The prerequisite for carrying out virtual vision research is a virtual vision simulator capable of generating synthetic imagery from simulated real-life scenes. We present a distributed, customizable virtual vision simulator capable of simulating pedestrian traffic in a variety of 3D environments. Virtual cameras deployed in this synthetic environment generate synthetic imagery-boasting realistic lighting effects, shadows, etc.-using the state-of-the-art computer graphics techniques. The synthetic imagery is fed into a real-world vision pipeline that performs visual analysise. g., blob detection and tracking, facial detection, etc.-and returns the results of this analysis to our simulated cameras for subsequent higher level processing. It is important to bear in mind that our vision pipeline is designed to handle real world imagery without any modifications. Consequently, it closely mimics the performance of a vision pipeline that one might deploy on physical cameras. Our virtual vision simulator is realized as a collection of modules that communicate with each other over the network. Consequently, we can deploy our simulator over a network of computers, allowing us to simulate much larger networks and much more complex scenes then is otherwise possible.',\n",
       " \"In following the Signals of Opportunity theme of the NAECON '09 Grand Challenge, we explore using computer-vision techniques for localization and orientation techniques to complement navigation via the pseudo-satellite technique proposed in [3]. The proposed approach is not affected by the strength of the microwave, and is more accurate than the conventional time of arrival approaches. Methods based on limited and varying information of markers is discussed. Simple applications and experimentation based on augmenting a motorized wheelchair for vision applications is discussed.\",\n",
       " 'Performance measurement of computer vision models provides information about their ability to classify objects. However, their performance gets affected in the real-world environment. We propose a modification for the metric called Generalizability, Robustness, and Elasticity score (GRE), which is used to determine the efficiency of the computer vision models. Specifically, we use unaltered Visual Question Answering (VQA) datasets and develop three new datasets for each attribute of the GRE score. The new datasets pass through three novel serial processes designed to enhance the quality of the datasets. The new datasets have a better distribution of feature information of the objects in the original dataset. Their performance is measured by running the datasets on three models specifically modified for our experiment. Two out of three models perform better on our new datasets and provide a better GRE score. We prove that our system works and can provide better results than the conventional method of measuring the performance of computer vision models.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Feature selection (FS) is a key factor for the performance of machine learning algorithms, as not all data and hence features are related to the various tasks. In this paper, we propose a novel scheme for convolutional FS for machine learning algorithms in computer vision. As not all the convolutional features are related to visual tracking, removing the unrelated ones will dramatically reduce the complexity and improve the algorithm performance. However, how to identify and select features related to the visual tracking task is still a challenge for machine learning algorithms. In the proposed scheme, a novel adaptive weights-objective function approach is established to evaluate and select the features. Furthermore, a quadratic programming method is introduced which improves the optimization efficiency. The experimental results demonstrate that our proposed scheme achieves superior performance compared to the state-of-art trackers on the challenging benchmarks in computer vision.',\n",
       " 'Falling is one of the leading health risks to the independent living elder and handicapped people, elder people falling has increased rapidly in past two decades. Previously fall detection methods were based on computer vision. Computer vision based human fall detection is one of the advice, but unsuccessfully such results demand high computational cost in additionally such computer vision based approaches depends on camera and on the lighting conditions. These devices which creates a problem such as privacy. Despite research done in past, the wearable sensor and computer vision techniques which are being used nowadays create a problem such as confidentiality and it is difficult for the elder people to remember to wear the devices, the elder people who are living alone in the home can have certain incidents like falling so to monitor the elder people activities this paper presents the signal fluctuations using RFID technology using this technology the activities of the elder people can identify.',\n",
       " \"In following the Signals of Opportunity theme of the NAECON '09 Grand Challenge, we explore the use of computer-vision techniques for localization and orientation techniques to complement navigation via the pseudo-satellite tecbnique proposed in [3]. The proposed approach is not affected by the strength of the microwave, and is more accurate than the conventional time of arrival approaches. Methods based on limited and varying information of markers is discussed. Simple applications and experimentation based on augmenting a motorized wheelchair for vision applications is discussed.\",\n",
       " 'The article deals with the image crack analysis issues in pavement management systems based on the computer vision technology. The development of pavement management systems is necessary to predict the long-term performance of roads and prioritize maintenance policies. An algorithm for detecting cracks in pavement images is proposed. The algorithm includes the following steps: enhancing the image contrast; the operation of the morphological opening; image conversion to halftone; Bottom-hat transform; threshold conversion; image cleaning from noise by opening operation; the operation of connecting potentially fractured cracks; search for the edges of cracks; crack edges classification; visualization of the found cracks edges. The article also describes the implementation of the proposed algorithm in the form of experimental automated system software. This software is developed in the C ++ using the computer vision library OpenCV and operates on the Jetson TX2 modular supercomputers hardware platform. The results of studies of the proposed approach are given.',\n",
       " 'This paper presents work which integrates computer vision information obtained from calibrated cameras with location events from an office-based ultrasonic location system. Bayesian networks are used to model dependencies and reliabilities of the multi-modal variables and perform fusion. Context is represented using a world model which incorporates aspects of both the static and dynamic environment. Information from the sentient computing system is used to guide and constrain the computer vision components, which in turn enhance the accuracy and capabilities of the world model.',\n",
       " 'Cameras are the primary sensor in automated driving systems. They provide high information density and are optimal for detecting road infrastructure cues laid out for human vision. Surround-view camera systems typically comprise of four fisheye cameras with 190 degrees+ field of view covering the entire 360 degrees around the vehicle focused on near-field sensing. They are the principal sensors for low-speed, high accuracy, and close-range sensing applications, such as automated parking, traffic jam assistance, and low-speed emergency braking. In this work, we provide a detailed survey of such vision systems, setting up the survey in the context of an architecture that can be decomposed into four modular components namely Recognition, Reconstruction, Relocalization, and Reorganization. We jointly call this the 4R Architecture. We discuss how each component accomplishes a specific aspect and provide a positional argument that they can be synergized to form a complete perception system for low-speed automation. We support this argument by presenting results from previous works and by presenting architecture proposals for such a system. Qualitative results are presented in the video at https://youtu.be/ae8bCOF77uY.',\n",
       " 'nan',\n",
       " 'Traffic jam, lack of adequate information of the traffic low used for development of road infrastructure to reduce traffic jam, and possibility of human error in the execution of a heavy traffic survey become the primary background of this study. By using information technology, open up an opportunity for the development of monitoring system computer vision based. This study will calculate the total amount of moving vehicles based on its type with computer vision based with staged: ROI selection, image segmentation with Gaussian Mixture Model method, filtering process, blob detection and tracking, and vehicles classification with Fuzzy Clustering Means. Implementation of an application using visual studio 2010. The output comprises result of classification and total amount vehicles based on its type. The test application divided into a few test scenarios, namely test 1, test 2, test 3 and test 4. The accuracy obtained on each test are 36.27%, 50.47%, 60.75%, and 67.00% respectively.',\n",
       " 'This paper describes the development of a computer vision-based real time displacement measurement system and demonstrated its performance on a large-scale wood truss bridge model. Digital images were captured with a consumer grade video camera. Three common types of computer vision algorithms are compared, including the Lucas-Kanade (LK) template tracking algorithm, inverse compositional (IC) algorithm (an extension of LK algorithm), and Digital Image Correlation (DIC). Application to the model bridge subjected to loading process indicates that the IC algorithm achieves real time displacement measurement. The performance in displacement from computer vision analyses matches the data collected by the conventional displacement sensors, with an average precision of within 1 mm at a distance of 5m away from the structure. The processing speed of the IC algorithm is over 300 faster than the conventional LK algorithm and around 140 times faster than Digital Image Correlation (DIC).',\n",
       " 'The mean field (MF) methods are an energy optimization method for Markov random fields (MRFs). These methods, which have their root in solid state physics, estimate the marginal density of each site of an MRF graph by iterative computation, similarly to loopy belief propagation (LBP). It appears that, being shadowed by LBP, the MF methods have not been seriously considered in the computer vision community. This study investigates whether these methods are useful for practical problems, particularly MPM (Maximum Posterior Marginal) inference, in computer vision. To be specific, we apply the naive MF equations and the TAP (Thouless-Anderson-Palmer) equations to interactive segmentation and stereo matching. In this paper, firstly, we show implementation of these methods for computer vision problems. Next, we discuss advantages of the MF methods to LBP. Finally, we present experimental results that the MF methods are well comparable to LBP in terms of accuracy and global convergence; furthermore, the 3rd-order TAP equation often outperforms LBP in terms of accuracy.',\n",
       " 'Recent advances in artificial intelligence (AI) and machine learning (ML), such as computer vision, are now available as intelligent services and their accessibility and simplicity is compelling. Multiple vendors now offer this technology as cloud services and developers want to leverage these advances to provide value to end-users. However, there is no firm investigation into the maintenance and evolution risks arising from use of these intelligent services; in particular, their behavioural consistency and transparency of their functionality. We evaluated the responses of three different intelligent services (specifically computer vision) over 11 months using 3 different data sets, verifying responses against the respective documentation and assessing evolution risk. We found that there are: (1) inconsistencies in how these services behave; (2) evolution risk in the responses; and (3) a lack of clear communication that documents these risks and inconsistencies. We propose a set of recommendations to both developers and intelligent service providers to inform risk and assist maintainability.',\n",
       " 'A method which computer vision technology is used to detect defect of small ceramic tubes is put forward, and the automatic testing system is developed by technologies of computer, computer vision, machine-electron and so on, and automatic detecting and classifying of small ceramic tubes are realized. The system comprises five parts which are the machine of automatically delivering and picking, PC, control unit, lamp-house, image collecting and processing. In testing, first, small ceramic tube running, and simultaneously, shooting and storing the tube images by CCD video camera; and secondly, image is preprocessed, segmented, feature extracted and sorted; finally, the inferior is eliminated by the manipulator. Taking example for the semitransparent ceramic tube that external diameter is 8 millimeter and inside diameter 6 millimeter, the testing method of small caliber and thin ceramic tube is researched by the automatic system. The outcome of the experimentation accords with the fact, this method is simple and feasible, and is of high efficiency, dependability, and high automation.',\n",
       " \"Computers have become a vital part of our workplaces. Millions of computer users spend at least more than two hours in front of a computer screen daily. This excessive use of computers leads to a disorder known as Computer Vision Syndrome. Most of the computer users suffer from CVS. CVS causes irritation, redness of the user's eye. CVS directly affects productivity of a computer user and user becomes prone to make mistakes and hence, this is a serious issue for people working in industrial sector. In this paper, we have proposed a method based on feature extraction methods such as SURF and Harris corner detection to detect the uncharacteristic behavior of a human eye and prevent CVS by providing alerts. The proposed method provides highly accurate results.\",\n",
       " \"Recently, general-purpose computing on graphics processing units (GPGPU) has been enabled on mobile devices thanks to the emerging heterogeneous programming models such as OpenCL. The capability of GPGPU on mobile devices opens a new era for mobile computing and can enable many computationally demanding computer vision algorithms on mobile devices. As a case study, this paper proposes to accelerate an exemplar-based inpainting algorithm for object removal on a mobile GPU using OpenCL. We discuss the methodology of exploring the parallelism in the algorithm as well as several optimization techniques. Experimental results demonstrate that our optimization strategies for mobile GPUs have significantly reduced the processing time and make computationally intensive computer vision algorithms feasible for a mobile device. To the best of the authors' knowledge, this work is the first published implementation of general-purpose computing using OpenCL on mobile GPUs.\",\n",
       " 'Detection of diseased fish at an early stage is necessary to prevent the spread of the disease. However, detecting fish diseases still uses a manual process and requires a high level of expertise that can be prone to human error. The ability of automatic detection of these fish diseases is much needed to help and to prevent losses of economic in the aquaculture industry. Therefore, this paper aims to detect disease of fish using computer vision and deep convolutional neural network (DCNN) algorithm. One Thousand and Two Hundred fish samples images were selected is namely diseased fish and healthy fish, which is determined by expert of fish diseases according to the specific of characteristics of fish diseases. The fish images went through the DCNN classifier and successfully achieved a satisfying mean average precision (mAP) with 0.237. The result shows that the computer vision integrated with the DCNN algorithm can efficiently predict fish disease.',\n",
       " 'Deep neural networks (DNNs) are successful in many computer vision tasks. However, the most accurate DNNs require millions of parameters and operations, making them energy, computation and memory intensive. This impedes the deployment of large DNNs in low-power devices with limited compute resources. Recent research improves DNN models by reducing the memory requirement, energy consumption, and number of operations without significantly decreasing the accuracy. This paper surveys the progress of low-power deep learning and computer vision, specifically in regards to inference, and discusses the methods for compacting and accelerating DNN models. The techniques can be divided into four major categories: (1) parameter quantization and pruning, (2) compressed convolutional filters and matrix factorization, (3) network architecture search, and (4) knowledge distillation. We analyze the accuracy, advantages, disadvantages, and potential solutions to the problems with the techniques in each category. We also discuss new evaluation metrics as a guideline for future research.',\n",
       " 'Computer Vision based Scoring system can break the monopoly of other automatic scoring systems like Shell Shockwave Amplitude System due to its ease of implementation and cost effectiveness. This paper presents a Computer Vision based Automatic Scoring method for the shooting targets. We perform Morphological processing of the target image to thicken the boundaries of the bullet hits and then segment the target area by Hysteresis thresholding. The impact of illumination variations is handled. by adjustable thresholds. The bulls eye of the target is segmented by the help of Distance transform to calculate the score inside the bulls eye. Thus, our method is capable of scoring inside and outside the bulls eye separately. The bullet hits are labeled after the segmentation of the target area and the overlapping bullets are also scored by defining a threshold pixel area for the bullet hits. The proposed algorithm is tested on 100 target images with varying number of bullets hit, resulting in bullet count accuracy of 98.3%.',\n",
       " \"Fatigue or drowsiness is one of the main reasons of inattention of drivers. Now a day Computer Vision is a very active field to detect specific human physiology and behavior during driving to detect driver's drowsiness. This paper proposes a design of detection system which can detect fatigue in drivers based on computer vision. The system approaches to detect fatigued drivers by first, detecting the face from a video sequence of driver's frontal face followed by locating the eye from the extracted face. Secondly, system finds the existence of eye pupil from detected eye and measures the blink rate simultaneously and by analyzing these parameters, system measures the loss of awareness before driver completely loss his/her attention. The proposed system aims to be more cost effective than industrial installments and therefore all parameters have been analyzed under laboratory environment and frames have been captured with a low cost webcam.\",\n",
       " 'In this paper authors present a new computer vision algorithm developing and testing for vehicle speed estimation. After researching the literature, two methods of calculating speed have been proposed - sampling method and skipping method. Value of main parameter number of measurement points - for each method has been tested and evaluated. The data obtained by these methods was compared with the reference speed data from the IR sensor group, the mean square error was calculated, and the analysis of the accuracy of these methods was carried out for various moving speeds. This paper is devoted to testing the previously developed system computer vision-based sensor for detecting the position and speed of vehicles by using a single camera. The algorithm used by the sensor was verified and its error measured by using a stand with model vehicle driving around a known track. Infrared sensors were used to measure the real speed and compare with output of the developed system.',\n",
       " 'Deep learning techniques have attracted many researchers in computer vision field to solve computer vision problems such as image segmentation and object recognition. This success also led to the implementation of deep learning techniques in 3D reconstruction. 3D reconstruction itself is a classical problem in computer vision that has been approached by many techniques. However, deep learning techniques for 3D reconstruction are still in the early phase, whereas the opportunity of the techniques is large. Hence, to improve the performance of such approaches, it is important to study the research and applications of the approaches through literature review. This paper reviews deep learning-based methods in 3D reconstruction from single or multiple images. The research scope includes single or multiple image sources but excludes RGB-D type input. Several methods and their significance are discussed, also some challenges and research opportunities are proposed for further research directions.',\n",
       " 'The IEEE Computer Society Smart Grid Vision Project (CS-SGVP) was chartered to develop Smart Grid visions looking forward as far as 30 years into the future. At the completion of the project it was realized that to address the complexity of a Smart Grid with vast numbers of intelligent connected devices and systems, computational intelligence techniques must move from top-down to the lowest levels of architectures, with interactive cooperation between smart components, each with a level of autonomy. The CS-SGVP team emphasized creative thought leadership and blue sky thinking to identify future Smart Grid operational visions and the role of computing to achieve these visions. The CS-SGVP team developed its visions using a three-tiered approach. Architectural concepts describe Smart Grid goals and characteristics, general grid types, as well as computing concepts considered common across grid types. Functional concepts describe how the grid will operate. Technological concepts describe the roles of certain technologies within the Smart Grid. The CS-SGVP expects that over the course of many years, various visions will come to fruition.',\n",
       " \"The blind spot of software testing is the assessment of the actual behavior of the system under test in the real, physical world. In this paper we show how this inherent restriction of software testing to the cyber world can be overcome with the use of methods and techniques from computer vision. It augments conventional software testing and allows making observations about states and events in the physical world as well as the system's real-world context. We implemented a demonstration scenario that shows how visual test automation can be combined with computer vision techniques to include observations of the physical properties of a mechatronic system in software testing. The successful application of the approach in a lab setting revealed several benefits and also some limitations. We discuss these benefits and limitations to highlight potential application scenarios in an industry setting and avenues for future research.\",\n",
       " \"Developing vision-based 3D gestures recognition systems requires strong expertise and knowledge in computer vision and machine learning techniques. As human-computer interaction researchers do not generally have a thorough knowledge of these techniques, we developed Gesta. Gesta is a tool that enables non-experts in vision computing and artificial intelligence techniques to rapidly develop a 3D gestures recognition system prototype and to support the gesture design process. This tool works with up to two Microsoft Kinects, and integrates the depth cameras calibration algorithm and the hidden Markov models classifier. The users can manage these complex functions through a simple graphical user interface, even if they do not have any expertise in computer vision and machine learning domains. A usability test with 12 researchers with experience in human-computer interaction has been conducted in order to evaluate the overall usability of this tool. Results demonsfrate that the testers appreciated the Gesta tool which scored 88.9 points out of 100 in the Brooke's system usability scale.\",\n",
       " 'nan',\n",
       " 'Aircraft attitude has always been an important technical parameter during flights, especially for stages such as aircraft take-off and landing. This article presents a monocular off-board vision system for relative attitude measurement of fixed-wing aircraft. In this system, a zoom imaging subsystem is set up to track and shoot the aircraft over a large distance range. Image sequences and corresponding servo data are transmitted to the processing computer for vision algorithms. A model-based attitude tracking algorithm, which requires the computer-aided design model of the aircraft, is performed to measure the relative attitude of the aircraft with respect to the reference platform. A 2-D visual tracking algorithm and a target detection algorithm are also employed to control the vision system. The proposed attitude tracking algorithm achieves competitive results on a publicly available dataset. The full system is validated in a real-flight experiment. The results revealed that the proposed system can run in real time on 1920 x 1080@40 Hz videos with an accuracy better than 0.5 degrees.',\n",
       " 'nan',\n",
       " \"The basic idea that the perception of actual embodied beings, be they animal or robotic, is fundamentally related to their embodiment is generally referred to as Purposive or Animate Vision. Research in this field generally emphasises low-level vision techniques. This paper outlines a philosophical basis for embodied perception, and develops a framework for conceptual embodiment of vision-guided robots. The aim is to facilitate the use of high-level vision through an active perception framework. We argue that the classical computer vision paradigm has problems in high-level vision due to an implicit assumption that objects in the world can be objectively subdivided into categories. Further, that through conceptual embodiment, active perception offers a way forward. We present a mobile robot navigation system based on the principles of conceptual embodiment. The system uses object recognition to guide a robot around known objects. The robot's object model is embodied, and this embodiment yields specific advantages for the robot.\",\n",
       " 'Biological sampling for biomass estimation is a highly labor intensive process that can benefit from automation using computer vision. In this paper two algorithms the Green Layer Extraction (GLE) method and Morphology and Marker Controlled Watershed Transformation (MMCWT) method are proposed for the estimation of frond (leaf) area of an invasive aquatic fern Salvinia molesta. The two algorithms are tested using a computer controlled automatic testbed. The mean accuracy for the GLE was found to be 89.66% for estimating the number of fronds and 86.74% for estimating the frond area. For the MMCWT the respective results are 91.92% and 80.68%. Then the results are subsequently used for developing a mathematical model as growth rate estimation of Salvinia molesta for various concentrations of Sodium di-hydrogen phosphate.',\n",
       " 'nan',\n",
       " 'This paper addresses video tracking, the problem of following moving targets automatically over a video sequence, and brings three main contributions. first, we give a concise introduction to video tracking in computer vision, including design requirements and a review of recent techniques, with some details of selected algorithms. Second, we give an overview of 28 recent papers on subsea video tracking and related motion analysis problems, arguably capturing the state-of-the-art of subsea video tracking. We summarize key features in a comparative, at-a-glance table, and discuss this work in comparison to the state-of-the-art in computer vision. Third, we identify well-proven computer vision techniques not yet embraced by the subsea research community, suggesting useful research directions for the subsea video processing community.',\n",
       " \"This paper presents an FPGA runtime framework that demonstrates the feasibility of using dynamic partial reconfiguration (DPR) for time-sharing an FPGA by multiple realtime computer vision pipelines. The presented time-sharing runtime framework manages an FPGA fabric that can be round-robin time-shared by different pipelines at the time scale of individual frames. In this new use-case, the challenge is to achieve useful performance despite high reconfiguration time. The paper describes the basic runtime support as well as four optimizations necessary to achieve realtime performance given the limitations of DPR on today's FPGAs. The paper provides a characterization of a working runtime framework prototype on a Xilinx ZC706 development board. The paper also reports the performance of streaming vision pipelines when time-shared. (1)\",\n",
       " 'nan',\n",
       " 'A network of real-time vision sensors is described using 100baseT Ethernet. Each vision sensor comprises a camera connected to a dedicated computer to for-in a vision node. Each vision node is capable of capturing and processing images and communicating the results over the network to a master computer in real-time. Deterministic and timely operation using standard Ethernet is accomplished using three basic techniques: suppressing superfluous packets; using the UDP transport protocol; and using an application layer protocol based on master/slave round-robin polling. Deterministic communications also enable the acquisition times of the cameras to be synchronized over the Ethernet network. By synchronizing the vision nodes to interleave their capture times, network contention is avoided and the effective video sample rate is increased. An analysis is also performed to determine the maximum number of vision nodes that can be supported using this technique over 100baseT Ethernet.',\n",
       " 'nan',\n",
       " \"In this paper, we introduce real time image processing techniques using modem programmable Graphic Processing Units (GPU). GPUs are SIMD (Single Instruction, Multiple Data) device that is inherently data-parallel. By utilizing NVIDIA's new GPU programming framework, Compute Unified Device Architecture (CUDA) as a computational resource, we realize significant acceleration in image processing algorithm computations. We show that a range of computer vision algorithms map readily to CUDA with significant performance gains. Specifically, we demonstrate the efficiency of our approach by a parallelization and optimization of Canny's edge detection algorithm, and applying it to a computation and data-intensive video motion tracking algorithm known as Vector Coherence Mapping (VCM). Our results show the promise of using such common low-cost processors for intensive computer vision tasks.\",\n",
       " 'Recent hardware developments have rendered controlled active vision a viable option for a broad range of problems, spanning applications as diverse as Intelligent Vehicle Highway Systems, robotic-assisted surgery, 3D reconstruction, inspection, vision assisted grasping, MEMS microassembly and automated spacecraft docking. However, realizing this potential requires having a framework for synthesizing robust active vision systems, capable of moving beyond carefully controlled environments. Ln addition, in order to fully exploit the capabilities of newly available hardware, the control and computer vision aspects of the problem must be addressed jointly. In this paper we illustrate with a simple example the control-related issues involved in active vision and we show how some very recently developed control and computer vision techniques can be brought to bear on the problem. These results also point out new research directions and possible extensions of currently available techniques.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Computer vision software is complex, involving many tens of thousands of lines of code. Coding mistakes are not uncommon. When the vision algorithms are run on controlled data which meet all the algorithm assumptions, the results are often statistically predictable. This renders it possible to statistically validate the computer vision software and its associated theoretical derivations. In this paper, we review the general theory for some relevant kinds of statistical testing and then illustrate this experimental methodology to validate our building parameter estimation software. This software estimates the 3D positions of buildings vertices based on the input data obtained from multi-image photogrammetric resection calculations and 3D geometric information relating some of the points, lines and planes of the buildings to each other.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'This paper discusses the use of computer vision in pervasive healthcare systems, specifically in the design of a sensing agent for an intelligent environment that assists older adults with dementia during an activity of daily living. An overview of the techniques applied in this particular example is provided, along with results from preliminary trials completed using the new sensing agent. A discussion of the results obtained to date is presented, including technical and social issues that remain for the advancement and acceptance of this type of technology within pervasive healthcare.',\n",
       " 'Lane Perception and vehicle control are critical components of driverless intelligent vehicles. Lane Perception provides the external information by image processing algorithms. After information processing, the car control system issued a manipulation instructions to achieve autonomous driving. In this paper, related literatures of lane perception and automobile control in recent years are summarized, as well as basic steps and methods of lane perception. Advantages and disadvantages of typical information processing methods and vehicle control methods are listed. The research direction of lane perception and vehicle control based on computer vision is expounded and established, which lays the foundation for the follow-up study.',\n",
       " 'Aquarium usually provides the visitors with pictures and some description of the fishes in the exhibition tank. In this paper, an intelligent fish recognition system using computer vision is proposed to help aquarium to educate people about the fishes in the tank. The designed system provides a convenient and friendly interface to help the users to retrieve the related fish information they are interested. The system includes three SUb-systems: (1) fish object detection system (2) fish object tracking system (3) fish object recognition system. Two types of fish indexing system (offline and real time) have been designed and implemented.',\n",
       " 'Due to traditional four-wheel alignment measurement techniques have such problems as need for many sensors, complicated calibration, fussy operation, low speed of inspection and low accuracy etc. Based on computer vision, the mathematical model for vehicle four-wheel alignment is set up. By taking sequence images of the target board fixed on the wheel in motion and calculating homography matrices between the target board and its images and rotation matrices, the directional cosine of rotation axis is obtained and further more the wheel alignment parameters are solved. Finally, the effectiveness of the mathematical model is verified by experiment.',\n",
       " 'The growth in mobile vision applications, coupled with the performance limitations of mobile platforms, has led to a growing need to understand computer vision applications. Computationally intensive mobile vision applications, such as augmented reality or object recognition, place significant performance and power demands on existing embedded platforms, often leading to degraded application quality. With a better understanding of this growing application space, it will be possible to more effectively optimize future embedded platforms. In this work, we introduce and evaluate a custom benchmark suite for mobile embedded vision applications named MEVBench. MEVBench provides a wide range of mobile vision applications such as face detection, feature classification, object tracking and feature extraction. To better understand mobile vision processing characteristics at the architectural level, we analyze single and multithread implementations of many algorithms to evaluate performance, scalability, and memory characteristics. We provide insights into the major areas where architecture can improve the performance of these applications in embedded systems.',\n",
       " \"Nowadays the logistics industry is developing rapidly. This paper proposes and designs a vision-based manipulator express sorting and delivery system for the problem of express stacking. This system combines computer vision, deep learning, manipulator control and motion planning technologies. This paper uses the YOLO algorithm and PCL to determine the position and normal vector of the pick-up point of the package, plans and controls the movement of the manipulator to the target position to suck the package, and uses vision to determine the size of the package and obtain customer information. Compared with RCNN and SDD algorithm, YOLO has a faster detection speed [1], and at the same time, it handles small stacked targets such as express boxes with higher accuracy. Therefore, this system uses the YOLOv3 algorithm to improve the system's extraction accuracy and speed. Finally, the success rate of the system was 75%, and the completion time was about 15s.\",\n",
       " 'Most existing deep learning networks for computer vision attempt to improve the performance of either semantic segmentation or object detection. This study develops a unified network architecture that uses both semantic segmentation and object detection to detect people, cars, and roads simultaneously. To achieve this goal, we create an environment in the Unity engine as our dataset. We train our proposed unified network that combines segmentation and detection approaches with the simulation dataset. The proposed network can perform end-to-end prediction and performs well on the tested dataset. The proposed approach is also efficient, processing each image in about 30 ms on an NVIDIA GTX 1070.',\n",
       " \"This paper considers the construction of a biologically inspired front-end for computer vision based on an artificial retina 'pyramid' with a self-organised pseudo-randomly tessellated receptive field tessellation. The organisation of photoreceptors and receptive fields in biological retinae locally resembles a hexagonal mosaic, whereas globally these are organised with a very densely tessellated central foveal region which seamlessly merges into an increasingly sparsely tessellated periphery. In contrast, conventional computer vision approaches use a rectilinear sampling tessellation which samples the whole field of view with uniform density. Scale-space interest points which are suitable for higher level attention and reasoning tasks are efficiently extracted by our vision front-end by performing hierarchical feature extraction on the pseudo-randomly spaced visual information. All operations were conducted on a geometrically irregular foveated representation (data structure for visual information) which is radically different to the uniform rectilinear arrays used in conventional computer vision.\",\n",
       " 'Traditional cameras field of view (FOV) and resolution predetermine computer vision algorithm performance. These trade-offs decide the range and performance in computer vision algorithms. We present a novel foveating camera whose viewpoint is dynamically modulated by a programmable micro-electromechanical (MEMS) mirror, resulting in a natively high-angular resolution wide-FOV camera capable of densely and simultaneously imaging multiple regions of interest in a scene. We present calibrations, novel MEMS control algorithms, a real-time prototype, and comparisons in remote eye-tracking performance against a traditional smartphone, where high-angular resolution and wide-FOV are necessary, but traditionally unavailable.',\n",
       " 'Combining technologies of computer vision [1-4] and control systems, we designed a vision-based basketball shooting robot. It is a wheeled mobile robot equipped with a web camera and a shooting arm. Using its vision, mobility and special mechanism to adjust shooting angles, the robot can aim for the target and shoot the ball into basket autonomously. In addition, we proposed a visual feedback method [5,6] which uses laser pointers to spot and then modify the shooting angles to improve shooting precision. Besides, the shooting robot is trained with artificial neural networks [7] in order to build the learning capability for both memorizing the training data and generalization.',\n",
       " 'Retinal prostheses for the blind have demonstrated the ability to provide the sensation of light in otherwise blind individuals. However, visual task performance in these patients remains poor relative to someone with normal vision. Computer vision algorithms for navigation and object detection were evaluated for their ability to improve task performance. Blind subjects navigating a mobility course had fewer collisions when using a wearable camera system that guided them on a safe path. Subjects using a retinal prosthesis simulator could locate objects more quickly when an object detection algorithm assisted them. Computer vision algorithms can assist retinal prosthesis patients and low-vision patients in general.',\n",
       " \"In Bangla alphabet, most of the characters share same features and such similarity misleads a recognizer as it makes decision based on measures of absolute difference. A preprocessing step called 'scrambling' and modification of existing grouping scheme are proposed to decrease the inter-character similarity among Bangla characters. The theory behind this proposal is originally inspired by experiments done on human vision and perception by psychologists and vision scientists. Experimental results corroborate theoretical predictions which lead us to propose scrambling and new grouping technique as effective steps in character recognition.\",\n",
       " 'This paper proposes a vision-based target tracking system of a quadrotor. The system consists of a vision-based target detection algorithm using color and image moment of a target candidate. A flight control layer working with the vision layer and the low level quadrotor control layer is designed using the offset of the target from the center of the image frame. The image processing and control algorithms are all implemented on a latest tablet computer, which was capable of running those algorithms in real-time. The proposed system was validated on a commercially available quadrotor platform and demonstrated satisfactory target tracking performance.(1)',\n",
       " 'Wearing face masks is one of the direct measures that can help tackling the spread of the new coronavirus. In this paper we presented an architecture for face mask wearing detection using pre-trained deep learning models for computer vision and implementation of them on embedded hardware platforms. Three object detection models were fine-tuned and optimized to run on 4 different hardware platforms. The fine tuning and optimization of the models resulted in significant reduction of the inference time, thus making the use of this technology in IoT based security systems for real-time automatic monitoring of face masks wearing realisable.',\n",
       " 'This study is devoted to solving the perception problem of autonomous transport system by Convolutional Neural Network (CNN) application. The goal of this paper is to develop the method for single camera based computer vision sensor for intelligent transport systems to detect and recognize objects in real-time, as well as estimate the speed of detected vehicle. This paper aims at evaluating the workability of this sensor in different weather conditions for autonomous vehicle applications. This sensor can be applied for different levels of abstraction like road safety, autonomous vehicle navigation, guiding system for disabled, animal survey, surveillance and other purposes.',\n",
       " 'A methodological study on significance of image processing and its applications in the field of computer vision is carried out here. During an image processing operation the input given is an image and its output is an enhanced high quality image as per the techniques used. Image processing usually referred as digital image processing, but optical and analog image processing also are possible. Our study provides a solid introduction to image processing along with segmentation techniques, computer vision fundamentals and its applied applications that will be of worth to the image processing and computer vision research communities.',\n",
       " 'To overcome hampers in applying computer micro-vision technology on submicron scale measurement, a new image enhancement algorithm is proposed. Combined with image deblur and wiener filter, as well as the snake model counter extraction, the measurement is well developed. Based on illumination-reflectance model and Center/Surround method, the Gaussian scale parameter is reasonably selected through maximizing the contrast average and minimizing the contrast variance among several windows. The enhancement algorithm can automatically balance the dynamic range compression and image information retaining. Experiments show that the measuring method can clearly observe the submicron scale small to 833nm and achieve little relative error for micron',\n",
       " 'Accurate detection and identification of crop diseases plays an important role in effectively controlling and preventing diseases for sustainable agriculture and food security. In this work, we have developed a novel computer vision-based approach for automatically identifying crop diseases based on marker-controlled watershed segmentation, superpixel based feature analysis and classification. The experimental result demonstrates that the proposed approach can accurately detect crop diseases (i.e. Septoria and Yellow rust. Two types of most important and major wheat diseases in UK and across the world) and assess the disease severity with efficient processing speed.',\n",
       " 'This paper exposes how we united computer vision technology and tangible user interfaces to conceive an innovative product targeting children audience. Voxar Puzzle is a digital game in which children have to choose a puzzle from a virtual library and assemble it by manipulating physical blocks. Furthermore, the process used for the development of the game platform had innovation in mind. A Blue Ocean strategy was used to define the functionalities and features of the game platform, comparing it with relevant competitors. The final result was a prototype with a professional interface that was validated with children from a local public school.',\n",
       " 'Recently, vision-based target positioning technology has received considerable attention in the field of computer vision. The positioning technology has many advantages such as faster speed, higher accuracy and more stable and reliable positioning results. Therefore, it plays an important role in the fields of robot technology, military reconnaissance, geographic survey and field measurement. Based on the positioning technology, the paper designs an embedded vision system with DM816x microprocessor (the processing module) and CCD camera equipped with an infrared filter (the acquisition module), which realizes the recognition and positioning of the fire. Moreover, with 200 groups of experiments in a warehouses, the fire detection system has an alarm accuracy rate over 98%, and positioning accuracy of fire is far higher than the national standard of China. In addition, an automatic fire protection system is proposed, which consists of the fire detector and the water cannon, then it can automatically extinguish the fire when a lire is detected.',\n",
       " 'This paper presents an innovative and motivating methodology to learn vision systems using a humanoid robot, NAO robot. Vision systems are an area of growing development and interest of engineering students. This approach to learning was applied in students of Master of Electrical Engineering. The goal is to introduce students the main approaches of visual object recognition and human face recognition using computer vision techniques to be embedded in a social robot and therefore he is able to iteract with human beings. NAO robot as an educational platform easy to learn how to program, and it has a high sensory ability and two cameras that can capture the images for processing.',\n",
       " 'The real-time detection of drought stress has major implications for preventing cash crop yield loss due to variable weather conditions and ongoing climate change. The most widely used indicator of drought sensitivity/tolerance in corn and soybean is the presence or absence of leaf wilting during periods of water stress. We develop a low-cost automated drought detection system using computer vision coupled with machine learning (ML) algorithms that document the drought response in corn and soybeans field crops. Using ML, we predict the drought status of crop plants with more than 80% accuracy relative to expert-derived visual drought ratings.',\n",
       " 'FPGAs are a popular platform for implementation of computer vision applications, due to the inherent parallelism present in the programmable fabric. In addition to hardware acceleration through parallelization, modern FPGAs are also dynamically reconfigurable, thereby adding an additional dimension to the mapping of algorithms to hardware. Among the various uses for run-time reconfiguration, one application is the time multiplexing of limited hardware resources to carry out a considerably complex computation. This paper presents the use of partial reconfiguration for time multiplexing computations in the implementation of a computer vision application - human detection. The results obtained from the implementation of a proof-of-concept prototype on a Xilinx Virtex-4 FPGA are also presented',\n",
       " 'Machine learning algorithms often use data from databases that are mutable; therefore, the data and the results of machine learning cannot be fully trusted. Also, the learning process is often difficult to automate. A unified analytical framework for trusted machine learning has been presented in the literature to address both issues. It is proposed building a trusted machine learning system by using blockchain technology, which can store data in a permanent and immutable way. In addition, smart contracts on blockchain are used to automate the machine learning process. However, in such a blockchain framework, data efficiency is a big concern, because it is very expensive to store a large amount of data on blockchain. On the other hand, machine learning based computer vision systems often rely on a lot of data. Therefore, to fully leverage a blockchain-based machine learning framework for computer vision systems, data efficiency issues must be addressed. This paper investigates how to enhance data efficiency in such a framework to bring computer vision systems to the edge. It presents a three-step approach. First, a lightweight machine learning model is trained on the server layer. Second, the trained model is saved in a special binary data format for data efficiency. Finally, the streaming layer takes these binary data as input and scores incoming new data in an online fashion. Real-time semantic segmentation for autonomous driving is used as an example to demonstrate how this approach works. This paper makes the following contributions. First, it improves the analytical framework for trusted computer vision systems based on blockchain. Second, the real-time semantic segmentation example shows how data-efficient learning for computer vision can be performed on the edge.',\n",
       " 'In this paper, we report how we design a human-robot co-working scheme. We believe that natural language is the best way that human and machine can exchange messages. However, understand voice command is not enough, what object is referred in what location is also very import. To better understand a natural language instruction, a robot also require computer vision to resolve the real-world object referencing problem. In this paper we show how to integrate the Google voice recognition result with the YOLO computer vision software with the KINOVA robot arm and achieve a collaborative robot interaction.',\n",
       " 'This paper addresses the measurement of the location and precision of stationary points in the three dimensional space, commonly termed remote center of motion (RCM), of minimally invasive surgical robotic manipulators. Two-view computer vision is used for its versatility, portability, remote sensing, and cost effectiveness. Geometrical models of cylindrical tools are constructed from camera images. The RCM is computed from the vectors of the cylindrical tool center lines at multiple poses. To verify the approach, a gold standard spherical bearing is first used to measure the RCM location and precision. The same computer vision system and RCM method is then applied to evaluate the RCM of a commercially available laparoscopy surgical robot.',\n",
       " 'Algorithms for Image processing and computer vision are natural candidates for high performance computing systems. This paper presents a reconfigurable parallel architecture prototype for image processing base on large scale FPGA computing. The introduced architecture can cover a wide range of real-time computer vision applications from pre-processing operations to low-level interpretation. In order to reduce the memory accessing time and communication latency, prime memory system for neighborhood operations or other data structures and FPGA-based transfer interconnection networks were designed in the introduced prototype system. This proposed architecture allows the user to program the system in both high-lever (soft-programming) and low-level (hard-programming).',\n",
       " 'We present an integrated system for vision-guided grasping in the real world. By using very limited resources - a standard personal computer and a robot-mounted camera - an inexpensive robot arm stably grasps unknown planar objects in real time by using visual perception and a standard parallel-jaw gripper. In a simple, yet powerful fashion our system integrates computer vision, grasping and vision-guided central. Novel techniques are presented to solve the involved problems under the imposed resource constraints: namely, for information reduction in image processing, strategies for grasp determination and vision-guided control for grasp execution. Particularly, a novel technique called curvature-symmetry fusion is used to help in efficient grasp determination. The system provides the user with a quantitative measure of the degree of stability of the planned grasp. Experimental results are provided. The imposed resource constraints makes it suitable for short-term applications in the real world, such as service or medical.',\n",
       " 'This paper introduces a vision based object tracking robot which is driven by wheels and controlled by a computer along with software. The objective of this project is to design a robot which is automatically controlled by computer to track and follow a colored object. Emphasis is given on precision vision based robotic applications. Image acquisition by the robot is achieved by using a PC-based webcam, then it is send to image processing software for further processing. The overall paper describes a visual sensor system used in the field of robotics for identification and tracking of the object.',\n",
       " 'Digital cultural heritage objects can benefit greatly from the application of Artificial Intelligence such as computer vision based tools to automatically extract valuable information from them. Novel methods and technologies have been used in the last few years to perform image classification, object detection, caption generation, and other techniques on different types of digital objects from different disciplines. In this pilot study, carried out in the context of the Digital Humanities project ChIA, we present an approach for testing different commercial (Clarifai, IBM Watson, Microsoft Cognitive Services, Google Cloud Vision) and open-source (YOLO) computer vision (CV) tools on a set of selected cultural food images from the Europeana collection with regard to producing relevant concepts. The project generally aims at improving access to implicit cultural knowledge contained in images, and increase analysis possibilities for scientific research as well as for content providers and educational purposes. Preliminary results showed that not only quantitative output results are important, but also the quality of concepts generated. Types of digital objects can pose a challenge to CV solutions.',\n",
       " 'Existing methods to put white and yellow marking on the road are based on manual measurement of the road dimensions and manual marking of guide lines. It is a time consuming, cumbersome and labour intensive process. This was the motivation for this work to develop a system which can automatically apply road markings according to the width of the road. In this paper, a stereo-vision based road marking system is presented. Computer stereo-vision has been used to measure the width of the road which helps us decide the number of lanes on the road. To generate the disparity map, a semi-global matching algorithm has been used, incorporating the advantages of both local and global matching algorithms. Efficient algorithms have been used for camera calibration and to estimate the road width. For deciding the number of lanes, the standards laid down by Indian Road Congress (IRC) have been followed..',\n",
       " 'The current animal sperm morphology analysis is mostly achieved through computer vision technology. By using image processing technique, the parameters of each sperm such as ellipticity, elongation of the head, mid-piece angle and percentage of acrosome can be calculated, then the quality of sperm can be evaluated. This system uses K-means algorithim to segment the sperm images, thinning algorithm to divide the mid piece of the tail, active contour model to segment acrosome, image moment to calculate parameters; Eventuality, the system can complete the task of sperm morphology analysis accurately and automatically. it saves a lot of manpower and improves efficiency.',\n",
       " 'The AI era sustains its foundations from the availability of large datasets. Especially geospatial datasets are very interesting from a computer vision perspective, as they enable us to understand the world we live in. Although many application domains arise from analyzing such big data, analysis itself is not enough for impacting lives. As its counter part, synthesis approaches are recently being developed for mimicking real-world data for completing and creating new worlds. In this paper, we will explore not only example analysis methods developed using large public datasets, but also some generative models to propose realistic and impactful solutions for going beyond observations.',\n",
       " 'Passenger flow monitoring plays an important role in elevator intelligent monitoring and elevator operation status analysis. This study gives one of definitions for elevator passenger flow, and proposes a counting method of the passenger flow in elevator based on the fusion of computer vision techniques. This experiment calculates the passenger flow in real-time detection by dividing the task into three parts, which is detecting the door status using the background subtraction method, detecting the current floor with Support Vector Machine (SVM) algorithm and counting passengers by YOLOv3 (You Only Look Once) model. The results show that the proposed method can accurately measure passenger flow even when passengers are heavily obscured.',\n",
       " 'In recent years there has been an increased interest in Human-Computer Interaction Systems allowing for more natural communication with machines. Such systems are especially important for the elderly and disabled persons. The paper presents a vision-based system for detection of long voluntary eye blinks and interpretation of blink patterns for communication between man and machine. The blink-controlled applications developed for this system have been described, i.e. the spelling program and the eye-controlled web browser.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'We document in this article a general introduction to modern miniature unmanned rotorcraft systems. More specifically, we present a fairly detailed overview on the hardware configuration, software integration, aerodynamic modeling, automatic flight control system, and computer vision system involved in forming an unmanned rotorcraft system.',\n",
       " 'On the road to making self-driving cars a reality, academic and industrial researchers are working hard to continue to increase safety while meeting technical and regulatory constraints Understanding the surrounding environment is a fundamental task in self-driving cars. It requires combining complex computer vision algorithms. Although state-of-the-art algorithms achieve good accuracy, their implementations often require powerful computing platforms with high power consumption. In some cases, the processing speed does not meet real-time constraints. FPGA platforms are often used to implement a category of latency-critical algorithms that demand maximum performance and energy efficiency. Since self-driving car computer vision functions fall into this category, one could expect to see a wide adoption of FPGAs in autonomous cars. In this paper, we survey the computer vision FPGA-based works from the literature targeting automotive applications over the last decade. Based on the survey, we identify the strengths and weaknesses of FPGAs in this domain and future research opportunities and challenges.',\n",
       " 'Automatic driving vehicles have been developed to provide more convenient and comfortable driving experiences. However, these vehicles failed in satisfying the variance of human intentions. Recently, the strategy of collaborating brain-computer interface (BCI) controlling and automatic driving receives attention. Since the BCI system remained some limitation in real-time controlling, a fusion method has been proposed to explore and verify the feasibility of human-vehicle collaborative driving in this paper. A hybrid BCI was developed to interpret human intentions. In addition, a computer vision-based automatic driving component was developed to maintain the vehicle on the road. A system for fusing these two kinds of vehicle driving decisions was first proposed in this paper. This system can simultaneously obtain the visual data and the hybrid electroencephalograph (EEG) signals. The hybrid EEG signals consist of steady-state visual evoked potentials and motor imagery. The obtained multisource information can be fused to make the final decision to drive a simulated vehicle. The proposed system was evaluated with different destinations. The experimental results verify the feasibility of fusing both human intention and computer vision. The task success rate reached 91.1% and the information transfer rate was 85.80 bit/min.',\n",
       " 'The aim of the article is to briefly summarize the main challenges on the field of multi-view computer vision, and by proposing available techniques, novel methods may be developed to several fields. After a grouping of systems with multiple cameras is proposed, current challenges are reviewed, and for every field, the disadvantages of current applications are pointed out.',\n",
       " 'The real-time measurement system is composed of two digital cameras, a computer, a coding scale control board and an illumination lamp-house. Digital cameras are set above the glass window of the wind tunnel and focused to the sensor part. Images of the growing ice will be transferred into the control computer continuously and be processed by the software. Horizontal direction extent of growing ice can be measured with one camera and three dimensions length measured with two cameras, which forms a stereo vision system. The ice thickness could be measured from each image., and then the icing speed can be obtained from the image sequence. Experiments show that the measurement accuracy of growing ice thickness can reach 0.1 millimeter in all direction with the developed system.',\n",
       " 'Hand gestures are a powerful way for human communication, with lots of potential applications in the area of human computer interaction. Vision-based hand gesture recognition techniques have many proven advantages compared with traditional devices, giving users a simpler and more natural way to communicate with electronic devices. This work proposes a generic system architecture based in computer vision and machine learning, able to be used with any interface for human-computer interaction. The proposed solution is mainly composed of three modules: a pre-processing and hand segmentation module, a static gesture interface module and a dynamic gesture interface module. The experiments showed that the core of vision-based interaction systems can be the same for all applications and thus facilitate the implementation. In order to test the proposed solutions, three prototypes were implemented. For hand posture recognition, a SVM model was trained and used, able to achieve a final accuracy of 99.4%. For dynamic gestures, an HMM model was trained for each gesture that the system could recognize with a final average accuracy of 93.7%. The proposed solution as the advantage of being generic enough with the trained models able to work in real-time, allowing its application in a wide range of human-machine applications.',\n",
       " 'The image processing and rapid location algorithm was studied to realize a human-computer interaction system based on DSP platform for image processing. Finger recognition and fast location were implemented during the process of acquiring image sequence. There were three key points in this process: finger recognition, fingertip location and coordinate mapping. Consider of the movement characteristics of the finger, algorithm for moving object extraction was designed by combining consecutive frames subtraction and background subtraction. Canny arithmetic operator was used for edge segmentation of the region of interest and the fingertip was recognized through template matching. Finally, fast fingertip location algorithm based on the look-up table method was developed according to the binocular vision theory. Location of the fingertip and mapping between space physical coordinates and computer screen coordinates were consequently implemented.',\n",
       " 'This paper explores the possibility of using computer vision and underwater Remotely Operated Vehicles (ROVs) to detect medical waste, such as masks and gloves in oceans. We use a single-stage detector to train the machine learning approach and then validate the results using the video feed from the tethered ROV.',\n",
       " \"In 2020, the global educational models became fully modified, migrating to a remote modality due to the global COVID-19 pandemic. This presented a massive challenge for educational programs with high technical content. In specific areas such as industrial process automation, new teaching models were adapted to lead the student to efficient learning through automated system simulators in the industrial area. This project aimed to design and develop a redundant robotic system with 7 degrees of freedom and computational vision. We used computer simulators to improve students' learning experience in technical industrial-process-automation classes. To achieve the control and programming of the redundant robot, we used the CoppeliaSim robotic simulator (V-REP), which allows creating, composing, and simulating almost any type of robot with precise physical motors. This simulator facilitates coding robots with the Python programming language. The OpenCV library was also utilized to integrate a computer vision system into the simulator. An automated (simulated) object classification system was designed using a redundant seven-degrees-of-freedom (7DOF) robot and a computational vision system. The vision system was designed to identify and classify PCB electronic boards with a barcode. The simulator allowed users to learn Python to program the robot's movements to fulfill a specific task. Additionally, it allowed users to learn to develop computer programs incorporating computational vision to monitor industrial processes.\",\n",
       " 'nan',\n",
       " 'Human-Computer Interaction (HCI) using intelligent artificial computing interface is a fast emerging and revolutionary field of study of computer vision. This present study is concerned with making computers responsive to human gestures and postures. In this paper a simple alternative method for hand gesture recognition system has been proposed. The system takes various fingers postures and try to recognize them using machine learning. A pattern of gestures is trained and tested to show the results using linear artificial neural network.',\n",
       " 'nan',\n",
       " 'A new model validation approach to motion segmentation problem is proposed. In order to demonstrate the proposed method, we study the motion segmentation problem for a mobile wheeled robot. Experiments were carried out on a Pioneer 3 mobile robot and a stationary camera.',\n",
       " 'An approach to the image superimposition problem solving on basis of the contour analysis methods is discussed; a number of contour analysis algorithms are suggested; the results of the developed algorithms implementation are shown by example of aerial photograph pair processing',\n",
       " 'Robot photographers have appeared in a variety of novelty settings over the past few years and typically have exploited rudimentary image-content-based approaches to identifying potential photographic subjects. These approaches are primarily limited to human subjects and further progress along content-based lines is hamstrung by slow progress on the general computer vision problem.',\n",
       " 'This work presents AFRIFASHION1600, an openly accessible contemporary African fashion image dataset containing 1600 samples labelled into 8 classes representing some African fashion styles. Each sample is coloured and has an image size of 128 x 128. This is a niche dataset that aims to improve visibility, inclusion, and familiarity of African fashion in computer vision tasks.AFRIFASHION1600 dataset is available here.',\n",
       " 'In this paper, we present a computer vision based text and equation editor for LATEX. The user writes text and equations on paper and a camera attached to a computer records actions of the user. In particular, positions of the pen-tip in consecutive image frames are detected. Next, directional and positional information about characters are calculated using these positions. Then, this information is used for on-line character classification. After characters and symbols are found, corresponding LATEX code is generated.',\n",
       " 'nan',\n",
       " 'At present, 3D reconstruction of facial shape is needed in many fields such as computer diagnosis, biometric authentication, and so on. Although many researchers studied various methods for 3D reconstruction in the field of computer vision, there are still some problems and limitations in human face reconstruction. In this paper, we propose the new 3D reconstruction method with the color stripe projection. The results show that our method is valid for the 3D reconstruction of face, which is low load to human and have high resolution.',\n",
       " 'nan',\n",
       " 'Previous research has shown that aggregated predictors improve the performance of non-parametric function approximation techniques. This paper presents the results of applying aggregated predictors to a computer vision problem, and shows that the method of bagging significantly improves performance. In fact, the results are better than those previously reported on other domains. This paper explains this performance in terms of the variance and bias.',\n",
       " 'Independent and automatic image processing is a fundamental objective of the computer vision community. Understanding the role of the eye movement scanpath in human vision is an important step toward the achievement of this objective. This top-down model of higher human vision is a new approach to bottom-up image processing algorithms and provides an important new metric and tool in computer vision. We have demonstrated that a small and manageable collection of image processing algorithms, experimentally selected and then combined together can serve in a task such as predicting human eye fixations identifying geological features. Thus, automatic picture analysis based upon human vision could be an essential element in planetary exploration.',\n",
       " 'In this paper, a robot vision recognition system is developed based on the Robot Operating System (ROS) and the Open Source Computer Vision (Open CV), which mainly implements face recognition, object detection, motion analysis and object segmentation of the robot.',\n",
       " 'Detecting and tracking objects is recently very important problem in computer vision field. It is widely used in areas like surveillance, security, automated vehicle systems and many more. This application is capable of detecting and tracking multiple objects in video sequences captured by a static camera.',\n",
       " 'With the advent of advanced telescopes and high resolution imaging techniques, astronomical data is flowing in faster than ever. Hence classifying galaxy images via human supervision and even crowd-sourcing is not viable anymore. This paper briefly summarizes the latest efforts in Computer Vision, particularly neural networks and its variants which automatically classify galaxy images.',\n",
       " 'Circle detection is a well-known application in computer vision. The Hough transform has been the traditional algorithm applied to detect circular objects in images. In this paper, we are concerned with detecting circular objects for an autonomous underwater robotics application using computer vision. One of the task of the autonomous robot is to identify circular objects underwater. However, circular objects in the image appear in different sizes depending on the depth of the robot. Our experimental studies show that using conventional algorithms have limitations as the environmental light and reflections significantly affect the performance. Furthermore, the algorithm performance depends on parameters used in preprocessing and it has a considerable amount of computational complexity and large memory space requirements. There are various techniques introduced in computer vision literature aiming to reduce computational complexity or to improve its accuracy. Heuristic optimization techniques such as genetic algorithms and simulated annealing are used for large or noisy images for the advantage of less computation time compared to Hough transform. Nevertheless, deep learning algorithms recently become very popular in computer vision due to their remarkable performance in object detection. In this paper, we experimented detecting circular objects in images using latest deep learning algorithms and studied their performance. It showed significant advantage in underwater images compared to conventional algorithms.',\n",
       " 'The paper presents a camera calibration error of pin-plate target study method for rail profile measurement based on computer vision. The camera calibration pin-plate target is designed according to real rail profile. The camera for rail profile vision measurement is calibrated by pin-plate target. Both structured light vision measurement model and 1-D camera projection model are established from the perspective of geometric optics to study the cause and value of camera calibration error based on pin-plate target. The results of the experimental study show that the target inclination and pixel offset of pin-plate target have an effect on the camera calibration for rail profile vision measurement. With the increase of target inclination and pixel offset, the camera calibration error increases continuously. It provides an optimized direction and research aim for fast camera calibration method of rail profile vision measurement.',\n",
       " 'nan',\n",
       " 'Computer vision technology has made great progress in practice in recent years, and it also has broad application prospects in turbidity detection. Turbidity detection plays an important role in water environment science, but popular turbidity detection methods have some limitations in aspects of cost, convenience, and space-time coverage. Based on above reasons, researchers are devoted to developing image-based turbidity detection methods as a complementary or even alternative to the popular turbidity detection method. However, the use of computer vision technology to detect turbidity is affected by many factors such as imaging system, feature extraction, model selection, and so on. Currently, there is no comparison and analysis of these methods in a framework. Therefore, this paper introduces typical turbidity detection methods based on computer vision in detail, with their principle, measurement range, accuracy, technical framework, and comparison. In this paper, existing studies are divided into four types according to different image sources, and seven image features mainly used in these studies are pointed out. The objective of this paper is to reviewthe development status, existing problems, future research directions of image-based turbidity detection methods, and establishment of a unified framework which includes principles, technical framework, and main equipment of imaging systems.',\n",
       " \"Monocular vision tracking, binocular vision tracking and multi-purpose visual tracking are the main current computer vision positioning tracking patterns. Due to the small field of view, the complex system structure and the difficulty of calibration matching, the binocular vision tracking system is being replaced by the monocular vision tracking system in some applications. The monocular tracking system has simple structure, and less calibrations. This paper investigates the recognition and localization of underwater moving targets using monocular vision. The color threshold algorithm is used to detect and extract the target color information. After this, a combination of geometric model method and data fitting method is proposed to detect the moving target and to obtain it's relative distance to underwater camera. Finally, we established an underwater experiment system with a template target, an underwater camera, a lighting system, and a PC computer. The experimental verification in the laboratory demonstrates the good performance of our proposed detecting and locating method.\",\n",
       " \"Many computer users suffer varying degrees of visual impairment, which hinder their interaction with computers. In contrast with available methods of vision correction (spectacles, contact lenses, LASIK, etc.), this paper proposes a vision correction method for computer users based on image pre-compensation. The blurring caused by visual aberration is counteracted through the pre-compensation performed on images displayed on the computer screen. The pre-compensation model used is based on the visual aberration of the user's eye, which can be measured by a wavefront analyzer. However, the aberration measured is associated with one specific pupil size. If the pupil has a different size during viewing of the pre-compensated images, the pre-compensation model should also be modified to sustain appropriate performance. In order to solve this problem, an adjustment of the wavefront function used for pre-compensation is implemented to match the viewing pupil size. The efficiency of these adjustments is evaluated with an artificial eye (high resolution camera). Results indicate that the adjustment used is successful and significantly improves the images perceived and recorded by the artificial eye.\",\n",
       " \"Using cloud-based computer vision services is gaining traction, where developers access AI-powered components through familiar RESTful APIs, not needing to orchestrate large training and inference infrastructures or curate/label training datasets. However, while these APIs seem familiar to use, their non-deterministic run-time behaviour and evolution is not adequately communicated to developers. Therefore, improving these services' API documentation is paramount-more extensive documentation facilitates the development process of intelligent software. In a prior study, we extracted 34 API documentation artefacts from 21 seminal works, devising a taxonomy of five key requirements to produce quality API documentation. We extend this study in two ways. First, by surveying 104 developers of varying experience to understand what API documentation artefacts are of most value to practitioners. Second, identifying which of these highly-valued artefacts are or are not well-documented through a case study in the emerging computer vision service domain. We identify: (i) several gaps in the software engineering literature, where aspects of API documentation understanding is/is not extensively investigated; and (ii) where industry vendors (in contrast) document artefacts to better serve their end-developers. We provide a set of recommendations to enhance intelligent software documentation for both vendors and the wider research community.\",\n",
       " \"In this paper, we look at the societal effects of computer vision technologies from the perspective of the future minds in computer vision: senior year engineering students. Engineering education has traditionally focused on technical skills and knowledge. Nowadays, the need for educating engineers in socio-technical skills and reflective thinking, especially on the bright and dark sides of the technology they develop, is being recognized. We advocate for the integration of social awareness modules into computer vision courses so that the societal effects of technology can be studied together with the technology itself, as opposed to the often more generic 'impact of technology on society' courses. Such modules provide a venue for students to reflect on the real-world consequences of technology in concrete, practical contexts. In this paper, we present qualitative results of an observational study analyzing essays of senior year engineering students, who wrote about societal impacts of computer vision technologies of their choice. Privacy and security issues ranked as the top impact topics discussed by students among 50 topics. Similar social awareness modules would apply well to other advanced technical courses of the engineering curriculum where privacy and security are a major concern, such as big data courses. We believe that such modules are highly likely to enhance the reflective abilities of engineering graduates regarding societal impacts of novel technologies.\",\n",
       " 'With the increasing volume of cars in traffic and the global traffic increasing exponentially, it has become critical to manage traffic as a challenge in the most developed countries. To address this issue, the intelligent traffic control system will use automatic vehicle counting as one of its core tasks to facilitate access, particularly in parking lots. The primary benefit of automatic vehicle counting is that it allows for managing and evaluating traffic conditions in the urban transportation system. The new era of technologies such as the Internet of Things and computer vision has transformed traditional systems into new smart city networks. Because of the proliferation of computer vision, traffic counting from low-cost control cameras may emerge as an appealing candidate for traffic flow control automation. This paper proposed a low-cost embedded car counter system using a Jetson nano card based on computer vision and IoT technologies to implement the offered system. In the proposed system, we apply a combination of background subtraction and counters, trackable objects, centroid tracking, and direction counting. Moreover, we implement the MoG foreground-background subtractor method. The proposed system is connected to the Internet using Telegram API to send notifications to smartphone hourly to analyze traffic congestion. In addition, we compared the performance of Jetson nano with the Raspberry Pi4 platform.',\n",
       " \"Today's computer systems trace their roots to an era of trusted users and highly constrained hardware; thus, their designs fundamentally emphasize performance and discount security. This article presents a vision for how small steps using existing technologies can be combined into one giant leap for computer security.\",\n",
       " 'In this paper we propose a complete framework that enables big-data tools to execute sequential computer vision algorithms in a scalable and parallel mechanism with limited modifications. Our main objective is to parallelize the processing operation in order to speed up the required processing time. Most of the present big-data processing frameworks distribute the input data randomly across the available processing units to utilize them efficiently and preserve working load fairness. Therefore, the current big-data frameworks are not suitable for processing huge video data content due to the existence of interframe dependency. When processing such sequential computer vision algorithms on big-data tools, splitting the video frames and distributing them on the available cores will not yield the correct output and will lead to inefficient usage of underlying processing resources. Our proposed framework divides the input big-data video files into small chunks that can be processed in parallel without affecting the quality of the resulting output. An intelligent data grouping algorithm was developed to distribute these data chunks among the available processing resources and gather the results out of each chunk using Apache Storm. The proposed framework was evaluated against several computer vision algorithms and achieved a speedup from 2.6x up to 8x based on the algorithm.',\n",
       " 'nan',\n",
       " \"In this paper we present a vision-based vehicle detection pipeline for autonomous vehicles. This pipeline is based on Histograms of Oriented Gradients (HOG) and Support Vector Machines (SVM) for classification. Primary motivation to conduct this research is to evaluate the performance of vision intensive tasks on embedded single board computers which is used to process the real time video frames in vehicles. Complete system is implemented on two different embedded boards that is Raspberry Pi 3 and Odroid C2. We evaluated our proposed system using the KITTI Vision Benchmark Suite, a standard in computer vision research. The results show that the implemented pipeline reaches the accuracy up to 70% on Odroid C2 and 65% on Raspberry Pi 3, given the limited training dataset size of 1200 examples. Furthermore, these embedded systems don't have any on board GPU that may improve the system's performance.\",\n",
       " 'With digital computers development possibility for change of certain human intellect based action raised. Because of a need for human vision replacement, needs that require simplicity, speed and low price in robotized mounting systems, usage in objects recognition and their settlement on exact place robot vision systems took place. This paper explains 3D vision systems implementation industrial robotics. Here are given elements of robot vision systems, ways of assembly and implementation examples.',\n",
       " 'Automation in agriculture comes into play to increase productivity, quality and economic growth of the country. Fruit grading is an important process for producers which affects the fruits quality evaluation and export market. Although the grading and sorting can be done by the human, but it is slow, labor intensive, error prone and tedious. Hence, there is a need of an intelligent fruit grading system. In recent years, researchers had developed numerous algorithms for fruit sorting using computer vision. Color, textural and morphological features are the most commonly used to identify the diseases, maturity and class of the fruits. Subsequently, these features are used to train soft computing technique network. In this paper, use of image processing in agriculture has been reviewed so as to provide an insight to the use of vision based systems highlighting their advantages and disadvantages.',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'nan',\n",
       " 'Serial number is a unique number given to a product which can be used for product identification and inventory management. Information about the product may be its manufacturing date, expiry date, place of manufacture and so on. Some companies use serial numbers to keep track on the number of products which a retailer is authorized to sell. Thus serial number identification becomes an important part at industry level. This paper presents an approach to detect and identify serial numbers using computer vision. Webcam is used to provide vision capability and Raspberry Pi which is Linux based system on chip hardware acts as the processing unit for this project making it a compact system. Various algorithms of OpenCV along with C++ language were used to extract numbers from the image. These numbers were later stored in a file to keep track of them.',\n",
       " 'In our paper we present a complete overview of a reliability control module for an electro-mechanical dice gambling machine based on computer vision principles. We particularly discuss the fundamental, heavily utilized function of color difference, which forms a basis for fast, efficient and parameterized chrome-key applications. We also explain the dice location estimation solution and the final template matching phase of number detection. In the conclusion we discuss some issues and problems which are still. left open but should be solved by the project team.',\n",
       " 'This paper presents a system for automated classification of rice variety for rice seed production using computer vision and image processing techniques. Rice seeds of different varieties are visually very similar in color, shape and texture that make the classification of rice seed varieties at high accuracy challenging. We investigated various feature extraction techniques for efficient rice seed image representation. We analyzed the performance of powerful classifiers on the extracted features for finding the robust one. Images of six different rice seed varieties in northern Vietnam were acquired and analyzed. Our experiments have demonstrated that the average accuracy of our classification system can reach 90.54% using Random Forest method with a simple feature extraction technique. This result can be used for developing a computer-aided machine vision system for automated assessment of rice seeds purity.',\n",
       " 'nan',\n",
       " \"A solution based on a Swarm Intelligence metaphor with a prey-predator scheme is proposed for real time object tracking in video sequences, which is a basic process in multiple computer vision tasks. Swarm predator particles fly on a Boid-like fashion over image prey pixels, using combined image features to guide individual movement rules. Object tracking emerges from interaction between predator particles and their environment. The paper includes method's description and experimental evaluations on video streams that illustrate the efficiency of swarm based methods in different vision tasks.\",\n",
       " 'With the advance of image processing techniques and the fast reduction of image sensor costs, embedded computer vision is becoming a reality. This paper presents the design and implementation of an integrated CMOS imaging system based on DM642. The system can acquire VGA resolution image at 100frame/s. The on-board 10Mbps Ethernet connection and the UART as well as digital I/O ports provide convenience for direct interface to other intelligent devices. The on-board software provides image acquisition function, network-based control, network-based data transfer, JPEG compression, image preprocessing functions, edge detection function. Automatic exposure control and automatic white balance are realized in the system. Experiments show that the CMOS imaging system has a good performance in terms of imaging quality, with much potential for being used as intelligent vision system.',\n",
       " 'This paper illustrates the importance of both algorithmic and embedded software techniques for an optimal embedded implementation of an image analysis and computer vision function: the integral image. A naive, straightforward implementation of the integral image on an embedded processor will likely produce an unacceptable execution time. However, by applying recursion and double buffering, one can improve execution time by several orders of magnitude. We compare execution times and memory utilization for each of the optimization techniques applied. These techniques can also be applied to implement other computer vision functions on programmable processor architectures.',\n",
       " \"As computing power grew over the past decades, the bottleneck that remained, and remains even now, is the communication between the user and the computer. Human Computer Interfaces attempt to be more ergonomic as well as try to improve bandwidth. This paper presents three different HCI solutions that use computer vision for different contexts: the first is a prototype customizable 'paper keyboard' which uses stereo vision, statistical foreground detection and optical flow to detect both striking (as on keys) and stroking actions (as on a touchpad) made on a customized keyboard/touchpad surface printed on ordinary paper. This allows the custom use of multilingual, pictorial, gaming-oriented, purely numerical disposable input devices. We next briefly introduce two more systems: an interactive, inexpensive, vandal proof, large public display, the 'e-signboard', and a home appliance control system based E-visual recognition of hand gestures. All three systems have been prototyped successfully.\",\n",
       " 'Achieving computer vision on microscale devices is a challenge. On these platforms, the power and mass constraints are severe enough for even the most common computations (matrix manipulations, convolution, etc.) to be difficult. This paper proposes and analyzes a class of miniature vision sensors that can help overcome these constraints. These sensors reduce power requirements through template-based optical convolution, and they enable a wide field-of-view within a small form through a refractive optical design. We describe the tradeoffs between the field-of-view, volume, and mass of these sensors and we provide analytic tools to navigate the design space. We demonstrate milliscale prototypes for computer vision tasks such as locating edges, tracking targets, and detecting faces. Finally, we utilize photolithographic fabrication tools to further miniaturize the optical designs and demonstrate fiducial detection onboard a small autonomous air vehicle.',\n",
       " 'The objective of this research is to develop a method to hide information inside a binary image by digital halftoning techniques with certain modifications. Two modified digital halftoning techniques, modified ordered dithering and modified multiscale error diffusion, are used in this research. The data is encoded pixel by pixel in the halftone image according to position at the image and sequence of binarization, respectively. The eye model and mean square error are used to measure the image quality. A computer vision method has been developed to recognize the printed binary image. The results show that thousands of binary images similar to human vision but quite distinct from each other by computer vision can be generated. The eye model and computer vision are useful for both binary image quality measurement and data recognition. These new techniques have great potential in printing security documents such as currency, ID card as well as confidential documents.',\n",
       " 'This project aims to apply image processing techniques in computer vision featuring an omnidirectional vision system to agricultural mobile robots (AMR) used for trajectory navigation problems, as well as localization matters. To carry through this task, computational methods based on the JSEG algorithm were used to provide the classification and the characterization of such problems, together with Artificial Neural Networks (ANN) for pattern recognition. Therefore, it was possible to run simulations and carry out analyses of the performance of JSEG image segmentation technique through Matlab/Octave platforms, along with the application of customized Back-propagation algorithm and statistical methods in a Simulink environment. Having the aforementioned procedures been done, it was practicable to classify and also characterize the HSV space color segments, not to mention allow the recognition of patterns in which reasonably accurate results were obtained.',\n",
       " 'In this paper we describe the Open Vision Computer (OVC) which was designed to support high speed, vision guided autonomous drone flight. In particular our aim was to develop a system that would be suitable for relatively small-scale flying platforms where size, weight, power consumption and computational performance were all important considerations. This manuscript describes the primary features of our OVC system and explains how they are used to support fully autonomous indoor and outdoor exploration and navigation operations on our Falcon 250 quadrotor platform.',\n",
       " 'In this paper, we propose a novel interactive integral imaging system using vision-based 3D fingertip interface. This system consists of the real 3D image generation system based on integral imaging technique and the interaction device using a real-time finger detection interface. The proposed system can be used in effective human computer interaction method for real 3D image. To show the usefulness of the proposed system, we carry out the preliminary experiment and the results are presented.',\n",
       " 'Deep Learning is the most widely used tool in the contemporary field of computer vision. Its ability to accurately solve complex problems is employed in vision research to learn deep neural models for a variety of tasks, including security critical applications. However, it is now known that deep learning is vulnerable to adversarial attacks that can manipulate its predictions by introducing visually imperceptible perturbations in images and videos. Since the discovery of this phenomenon in 2013, it has attracted significant attention of researchers from multiple sub-fields of machine intelligence. In 2018, we published the first-ever review of the contributions made by the computer vision community in adversarial attacks on deep learning (and their defenses). Many of those contributions have inspired new directions in this area, which has matured significantly since witnessing the first generation methods. Hence, as a legacy sequel of our first literature survey, this review article focuses on the advances in this area since 2018. We thoroughly discuss the first generation attacks and comprehensively cover the modern attacks and their defenses appearing in the prestigious sources of computer vision and machine learning research. Besides offering the most comprehensive literature review of adversarial attacks and defenses to date, the article also provides concise definitions of technical terminologies for the non-experts. Finally, it discusses challenges and future outlook of this direction based on the literature since the advent of this research direction.',\n",
       " 'The niche where computer vision software meets networked robotics remains unfilled so far. Descending costs, and technological advances in wireless communications, digital video capture, and computing power provides roboticists with powerful yet affordable tools to develop cooperative, distributed, vision-based control tasks. This paper presents a consistent framework for such networked, visually-controlled environments, where the main assumption is that video may be captured, processed, and used for control in different machines across the network. To do so, care has been taken in order to ensure efficiency, scalability, and reusability of software. The presented framework aims to seamlessly integrate rock-solid, fast, existing vision processing libraries within a cross-platform, distributed environment, based on Java and agents. Different applications in real setups are envisioned.',\n",
       " 'Safety of aircraft operations on approach and landing presents a challenging task of air traffic management. Obstacles presence awareness and the runway condition control is extremely required in challenging weather conditions. The paper proposes the obstacles detection method for modern flight vision systems. The topic is relevant to of avionics, computer vision and image processing.',\n",
       " 'nan',\n",
       " \"This paper describes the development of vision-aided navigation (i.e., pose estimation) for a wearable augmented reality system operating in natural outdoor environments. This system combines a novel pose estimation capability, a helmet-mounted see-through display, and a wearable processing unit to accurately overlay geo-registered graphics on the user's view of reality. Accurate pose estimation is achieved through integration of inertial, magnetic, GPS, terrain elevation data, and computer-vision inputs. Specifically, a helmet-mounted forward-looking camera and custom computer vision algorithms are used to provide measurements of absolute orientation (i.e., orientation of the helmet with respect to the Earth). These orientation measurements, which leverage mountainous terrain horizon geometry and/or known landmarks, enable the system to achieve significant improvements in accuracy compared to GPS/INS solutions of similar size, weight, and power, and to operate robustly in the presence of magnetic disturbances. Recent field testing activities, across a variety of environments where these vision-based signals of opportunity are available, indicate that high accuracy (less than 10 mrad) in graphics geo-registration can be achieved. This paper presents the pose estimation process, the methods behind the generation of vision-based measurements, and representative experimental results.\",\n",
       " 'From this paper, we propose a novel methodology to compute a 2D Homography. applying some algorithms of computer algebra. We consider the classical problem of solving (exactly) a linear system of algebraic equations, and we suggest a new algorithm for computer vision, based on homomorphism methods over Z, to solve a system of equations necessary to achieve a 3 x 3 matrix H which lets us to compute the projective transformation which translates coordinates between points in different planes. From this work, we want to show that it is possible to apply a symbolic approach to some crucial issues of computer vision, moreover of the numerical methodology, in order to reduce the complexity-of some algorithms, and to eliminate the problems associated with loss of precision and normalization. We test our technique in a real situation: a parking management system, which creates a pseudo-top-view of a parking area to determine if there are free parking lots or not.',\n",
       " 'nan',\n",
       " 'This article provides the description of the development of a restaurant guest tracking system and an overview of already existing realization methods. The system works due to computer vision algorithms and neural network. The article contains a description of the main principles of neural network with the face recognition functions and research of different ways to implement the tasks. The purpose of the investigation is finding the most rational solution of the problem. The article consists of comparison of various types of face identification algorithms and computer vision tools and data sets management tools. The result of this research is a developed application that maintains restaurant visitors database. As a conclusion research contains a cons and pros of the provided solution.',\n",
       " \"This paper describes the integration of robotics education into an undergraduate Computer Science curriculum. The proposed approach delivers mobile robotics as well as covering the closely related field of Computer Vision and is directly linked to the research conducted at the authors' institution. The paper describes the most relevant details of the module content and assessment strategy, paying particular attention to the practical sessions using Rovio mobile robots. The specific choices are discussed that were made with regard to the mobile platform, software libraries, and lab environment. The paper also presents a detailed qualitative and quantitative analysis of student results, including the correlation between student engagement and performance, and discusses the outcomes of this experience.\",\n",
       " \"Augmented reality has been widely used in many applications because of its ability to offer an amazing way to overlay computer-generated images over the user's real-world view, creating a composite view rooted in real and virtual worlds. Augmented Reality is a realistic, direct or indirect view of the physical reality environment whose elements are enhanced through computer-generated or sensory input such as sound, video, graphics, tactile, or GPS data. In this paper, we present a novel campus navigation APP that uses augmented reality to provide users with a new and interesting way to meet our campus. With advanced augmented reality technologies such as computer vision and object recognition, the information about the campus environment and its objects is overlaid on the real world and becomes interactive. In order to improve the APP efficiency, this paper presents a virtual terrain modeling interface with deep learning to improve the object recognition ability.\",\n",
       " \"Until now there have been few formalized methods for conducting systematic benchmarking aiming at reproducible results when it comes to computer vision algorithms. This is evident from lists of algorithms submitted to prominent datasets, authors of a novel method in many cases primarily state the performance of their algorithms in relation to a shallow description of the hardware system where it was evaluated. There are significant problems linked to this non-systematic approach of reporting performance, especially when comparing different approaches and when it comes to the reproducibility of claimed results. Furthermore how to conduct retrospective performance analysis such as an algorithm's suitability for embedded real-time systems over time with underlying hardware and software changes in place. This paper proposes and demonstrates a systematic way of addressing such challenges by adopting containerization of software aiming at formalization and reproducibility of benchmarks. Our results show maintainers of broadly accepted datasets in the computer vision community to strive for systematic comparison and reproducibility of submissions to increase the value and adoption of computer vision algorithms in the future.\",\n",
       " \"Micro-Aerial Vehicles (MAVs) have gained significant attention lately due to their size advantage. However, there is a drawback of MAVs - its limited payload and size don't allow adding extensive sensors. That explains why incorporating computer vision is of great significance to MAVs. One of the problems that computer-vision-driven MAVs need to overcome is obstacle avoidance, which is very important for autonomic vehicles especially for aerial vehicles as they are more vulnerable to collision compared to ground vehicles. Over the last ten years, several obstacle detection algorithms have been developed to create collision-free maneuver for MAVs. Most of them have promising results inside virtual environment; however, they fail miserably during actual flight tests. In this project, we will investigate the real-life issues affecting obstacle avoidance for MAVs and carry out the project on a physical drone. We take into consideration the limitations of the platform and derive our own obstacle avoidance algorithm by combining several existing ones. Effectiveness of the algorithm will be demonstrated through experimental results on the physical drone.\",\n",
       " 'nan',\n",
       " 'Acquire spatial information of a 3D model is very useful in many applications in the computer science field. A device broadly used as acquisition hardware of models is the 3D scanner. With a 3D scanner is possible to obtain the geometric structure based on points and triangles of a model, and use it on several computing applications. According the hardware and algorithms employed, the scanners can be classified as passive and active vision. The passive vision scanners are lower cost devices which require geometrics algorithms to obtain a 3D reconstruction. In this paper, we present a design and implementation of a simple 3D scanner of lower cost based on passive vision. The scanner consists in the design of hardware and algorithms where two webcam are placed on a controlled environment. Several experiments demonstrate excellent results on the geometric reconstruction of the scanned objects.',\n",
       " 'Parking occupancy in large outdoor parking as well as at street parking slots is a big problem, and solutions for it were proposed in the past with limited success. Recent advances in Computer Vision (CV) algorithms made this technology more powerful by making models smaller and more precise. On the other hand development of hardware components made Edge devices more powerful with AI accelerators as built-in options. Based on this new development, we propose an architecture where the latest Smart Edge devices are used to calculate parking occupancy on the edge. The proposed solution uses newly available data streams to create a 3D model of the parking environment that will identify available on-street parking spaces with high precision.',\n",
       " 'In application of human computer interaction, modeling of human vision is an approach seems appropriate but has not yet been accomplished. The main contribution in this paper is proposing an intuitive system for human computer interaction by modeling human vision using human sight position and human body part position, as well as an evaluation method to estimate the characteristic of the proposed system.',\n",
       " 'Regular inspection of rail valves and engines is an important task to ensure safety and efficiency of railway networks around the globe. Over the past decade, computer vision and pattern recognition based techniques have gained traction for such inspection and defect detection tasks. An automated end-to-end trained system can potentially provide a low-cost, high throughput, and cheap alternative to manual visual inspection of these components. However, such systems require huge amount of defective images for networks to understand complex defects. In this paper, a multi-phase deep learning based technique is proposed to perform accurate fault detection of rail-valves. Our approach uses a two-step method to perform high precision image segmentation of rail-valves resulting in pixel-wise accurate segmentation. Thereafter, a computer vision technique is used to identify faulty valves. We demonstrate that the proposed approach results in improved detection performance when compared to current state-of-the-art techniques used in fault detection.',\n",
       " 'This paper demonstrates the principle of the robot chess system based on computer vision. According to improved Hough transform, a new image recognition method was proposed to settle the problem of the Location in the robot chess system. Application results show that this method has the high performances in robustness, speed and accuracy.',\n",
       " 'Computers with their growing demands have their applicability in every field, one of the applications is the computer vision, where the function of an human eye has been replaced by using various sensors to capture the environment in the similar way a human eye does. We know that capturing the environment through sensors lead to origin of various kinds of images and videos of different shapes and sizes and contain large amount of descriptive information. These features enable us to construct various models by using the geometry, physics, statistics. For completing our actions we make use of camera, cables, connecting devices and computer set. Similarly, the descriptive information needs to be transmitted over a channel to share with others and also for utilizing the storage resources efficiently we make use of data compression algorithms and compare which algorithm is best suited for a given application and measure the level of compression using entropy and compression ratios.',\n",
       " 'Embedded vision processing is currently ingrained into many aspects of modern life, from computer-aided surgeries to navigation of unmanned aerial vehicles. Vision processing can be described using coarse-grained data flow graphs, which were standardized by OpenVX to enable both system and kernel level optimization via separation of concerns. Notably, graph-based specification provides a gateway to a code generation engine, which can produce an optimized, hardware-specific code for deployment. Here we provide an algorithm and JAVA-MVC-based implementation of automated code generation engine for OpenVX-based vision applications, tailored to NVIDIA multiple CUDA Cores SoC Jetson TX. Our algorithm pre-processes the graph, translates it into an ordered layer-oriented data model, and produces C code, which is optimized for the Jetson TX1 and comprised of error checking and iterative execution for real time vision processing.',\n",
       " 'In this paper a novel method for the visual inspection of frying food items using gray level method is presented. The gray levels of the images of the food items are used for recognition of different stages for frying. Mean gray level of 101.9 was obtained as the threshold of complete frying.',\n",
       " \"A well-researched topic dealing with the automotive market concerns the development of innovative devices to improve security and comfort. Along these lines, this paper proposes a fully automatic system based on computer vision that can orient the interior rear view mirror of a car so as to seamlessly provide the driver with a correct rear view. A cheap 2D camera is mounted over a motorized rear view mirror and used to scan the interior car space in order to identify the driver's face and eyes. Detection does occur effectively regardless of the number and position of car occupants and allows the system to compute the correct orientation and adjusts the mirror accordingly.\",\n",
       " \"'UP2U' project has an objective to create awareness of phubbing behavior in today society. 'UP2U' is an interactive installation application which is designed to be set up in an indoor waiting area. The system has an objective to raises people awareness of their phubbing behavior and encourage them to have more interpersonal communication with each other. It was implemented using computer vision and augmented reality techniques. By conducting the experiments in both the lab and the real indoor public waiting area, we found the system demonstrates high accuracy of head tracking, face direction estimation, and gender classification. Moreover, the system has promising social impact to the phubbing society.\",\n",
       " 'Mobile robotics and computer vision two field of artificial intelligence and are important in areas like autonomous navigation and human computer interaction. In this paper, an extremely low cost solution for the vision system of a small mobile robot has been proposed. The robot navigates in its environment by following a pre-drawn track on floor. The vision system of commercial robots requires expensive dedicated hardware and complex algorithms. The proposed architecture of the vision system would require very cheap and available hardware resources like a webcam and a PC. A unique algorithm to extract track information from a sequence of images captured from the camera has been devised. The control system of the mobile robot would use the information provided by the vision system for its lateral control. Based on the proposed architecture and related algorithms, a software system has been developed that reflects the efficiency and effectiveness of the solution.',\n",
       " 'Fish detection, a specific task in computer vision system for fish monitoring, is challenging due to the complex characteristics of the captured images. A proposed approach in tackling this challenging task was to incorporate a multilayer artificial neural network to a computer vision system algorithm, implemented in aquaculture. This computer vision system algorithm captured the images from the aquaculture setup. Then, these captured images were processed. After that, the features out of these processed images were extracted and utilized to develop this multilayer artificial neural network. The best configuration, which is trained with the least learning time and tested with least mean square error and highest accuracy, was determined by adjusting the number of neurons in the two hidden layers. The multilayer artificial neural network with 50 neurons in the first hidden layer and 10 neurons in the second layer was considered the best configuration; it has achieved learning time of 3.374 ms, mean square error of 0.2315, and accuracy of 79.00%, hence, proving the competitiveness of this approach.',\n",
       " 'The AUTOLAND project main objective was the development of solutions that can enable the autonomous landing of a fixed-wing Unmanned Aerial Vehicle (UAV) on a Fast Patrol Boat (FPB). Since we are operating in a military environment where jamming is possible, we have developed Computer Vision (CV) based systems without using any external sensor information. We have developed and tested two different CV approaches: (i) airborne, and (ii) ground-based. In the airborne approach, the UAV uses its camera and external markers to estimate the relative pose to the landing area and automatically calculates the needed landing trajectory. In the ground-based approach, a ground-based monocular vision system obtains the relative pose of the UAV concerning the camera reference frame using its Computer-Aided Design (CAD) model. Then, a Ground Control Station (GCS) calculates the landing trajectory and transmits it to the UAV. The obtained error was compatible with the automatic landing requirements. The future work will focus on real data acquisition to improve the developed algorithms.',\n",
       " 'In recent years, aiming to improve deriving safety and supporting autonomous driving, pedestrian detection has attracted considerable attention from both industry and academic. Moreover, by taking advantage of the powerful computational capacity of GPU and high-level feature learning ability of the deep convolutional neural network, tremendous image/video-based pedestrian detection methods have been proposed. However, most of the existing approaches are designed relying on the computer vision-based target detection techniques. Accordingly, the evaluation criteria they consider in the design are often from the computer vision research field. Therefore, these existing methods tend to focus on the improvement of accuracy and ignore some of the special requirements that need to be considered in the field of autonomous driving. In this paper, we will analyze and summarize the features of the state-of-the-art pedestrian detection methods in detail. Then, by considering the practical application scenarios of autonomous driving techniques, we further discuss the open challenges of designing a practical pedestrian detection method for supporting autonomous deriving.',\n",
       " 'The infected red blood cell pixel count in thin blood smear image plays a vital role in malaria parasite detection analysis. This paper proposes three stage object detection procedure of computer vision with Kernel-based detection and Kalman filtering process to detect malaria parasite. The use of Kernel based detection with exact pixel information makes the proposed procedure capable of accurately detecting and localizing the target infected by malaria parasites in thin blood smear images. The experiment is conducted on several microscopically preliminary screened benchmark gold standard diagnosis datasets of blood smear images, each 300x300 pixels of Plasmodium falciparum in thin blood smear images. The 300x300 size images were split into overlapping patches, each of size 50x50 pixels. The experimental results on the malaria blood smear image datasets demonstrate the effectiveness of the proposed method over the existing computer vision algorithms. The novelty of the work lies in the application of an object detection for malaria parasite identification in computer vision for thin blood smear images.',\n",
       " 'Ground-penetrating radar (GPR) is a powerful and rapidly maturing technology for subsurface threat identification. However, sophisticated processing of GPR data is necessary to reduce false alarms due to naturally occurring subsurface clutter and soil distortions. Most currently fielded GPR-based landmine detection algorithms utilize feature extraction and statistical learning to develop robust classifiers capable of discriminating buried threats from inert subsurface structures. Analysis of these techniques indicates strong underlying similarities between efficient landmine detection algorithms and modern techniques for feature extraction in the computer vision literature. This paper explores the relationship between and application of one modern computer vision feature extraction technique, namely histogram of oriented gradients (HOG), to landmine detection in GPR data. The results presented indicate that HOG features provide a robust tool for target identification for both classification and prescreening and suggest that other techniques from computer vision might also be successfully applied to target detection in GPR data.',\n",
       " 'The key motivating factor for the underlying research work is to tackle the autonomous driving problem. The proposed research work attempts to design a model to effectively assist the drivers of vehicles. It uses computer vision algorithms to detect lanes, deep-learning to identify obstacles like other vehicles and decision trees to take driving decisions. The driver assistance system has the ability to perform in both autonomous and assisting capabilities. The autonomous mode should be deployed on highways and the assisting mode on congested city roads. In the autonomous mode the system controls the car while in the assisting mode the system ensures that the driver practices safe driving by providing valuable prompts.',\n",
       " 'Quality control in industrial food manufacturing can be reliably performed with computer vision systems that operate at high speed. However, most of these inspection stations need to be tuned manually and only perform well on a specific product. This research integrates machine learning techniques in the process to automate the initial tuning of real-time vision-based inspection systems for bakery products. The combination of feature selection techniques with machine learning is assessed in terms of classification performance. A formal automated tuning methodology is introduced and evaluated experimentally with data from industrial inspection stations. The work demonstrates that an inspection system automatically tuned with the proposed technique can systematically achieve 98% correct classification when compared with the classification generated with a manually tuned system.',\n",
       " \"Recently, deep neural networks have achieved state-of-the-art performance in multiple computer vision tasks, and become core parts of computer vision applications. In most of their implementations, a standard input preprocessing component called image scaling is embedded, in order to resize the original data to match the input size of pre-trained neural networks. This article demonstrates content disguising attacks by exploiting the image scaling procedure, which cause machine's extracted content to be dramatically dissimilar with that before scaled. Different fromprevious adversarial attacks, our attacks happen in the data preprocessing stage, and hence they are not subject to specific machine learning models. To achieve a better deceiving and disguising effect, we propose and implement three feasible attack approaches with L-0-, L-2- and L-infinity-norm distance metrics. Wehave conducted a comprehensive evaluation on various image classification applications, including three local demos and two remote proprietary services. Wealso investigate the attack effects on a YOLO-v3 object detection demo. Our experimental results demonstrate successful content disguising against all of them, which validate our approaches are practical.\",\n",
       " 'One of the major reason behind degradation of quality and quantity of rice crop is pest. The lack of technical and scientific knowledge to prevent pest diseases is the main reason for low production of these commodities. This article aims to develop a computer vision based automatic system for the diagnosis of diseases caused by pests in the rice plants. Automatic disease detection using computer vision approach involves three types of feature extraction in this experiment. Diseased area of the leaf, textural descriptors using gray level co-occurrence matrix (GLCM) and color moments are extracted from diseased and non-diseased leaf images resulting in 21-D feature vector. Genetic algorithm based feature selection approach is employed to select relevant features and to discard redundant features, generating a 14-D feature vector that reduces the complexity. Artificial neural network (ANN) and support vector machine (SVM) is used for classification. The proposed algorithm results in classification accuracy of 92.5% using SVM and 87.5% using ANN.',\n",
       " \"The successful orthognathic surgery is directly influenced by more accurate measurement of distance or size of the surgery area. There exist various methods of measurements during the orthognathic surgery. One of the newest methods is to make measurements by using stereo vision system with stereo cameras. The result of stereo vision is affected by many factors, such as captured image colour, glossiness, ambient light, geometry, etc. One of the most important influence factors is ambient light, especially on measuring distances with the value of microns (10-6 m). The stereo vision system's cameras are also influenced by ambient light and the objective of this paper is to investigate more accurate result of stereo vision according to the ambient light of the orthognathic surgery operation room.\",\n",
       " 'nan',\n",
       " 'In a computer vision system, handwritten digits recognition is a complex task that is central to a variety of emerging applications. It has been widely used by machine learning and computer vision researchers for implementing practical applications like computerized bank check numbers reading. In this study, we implemented a multi-layer fully connected neural network with one hidden layer for handwritten digits recognition. The testing has been conducted from publicly available MNIST handwritten database. From the MNIST database, we extracted 28,000 digits images for training and 14,000 digits images for performing the test. Our multi-layer artificial neural network has an accuracy of 99.60% with test performance.',\n",
       " 'Human computer interaction with simple hand gesture recognition system as the interface using computer vision techniques has been developed. Usage of visually interpreted hand gestures as the interface makes it a more natural and intuitive human computer interaction (HCI) system. In this system model, interfacing by traditional methods in virtual environments like a joystick, mouse, and keyboard and even if the modern method like touch screen is also replaced by using hand gestures. Here the computer is able to visually recognize hand gestures from the video obtained using a webcam. In the hand gesture recognition system, the skin colour thresholding model is used for segmentation. Various features are extracted and then classified using an efficient classifier to generate more accurate and better result. Apart from HCI, this vision based hand gesture recognition can also be used for applications like industrial robot control, sign language translation, in the rehabilitation device for people with upper extremity physical impairments etc.',\n",
       " \"The IEEE Low-Power Image Recognition Challenge (LPIRC) is an annual competition started in 2015. The competition identifies the best technologies that can detect objects in images efficiently (short execution time and low energy consumption). This paper summarizes LPIRC in year 2018 by describing the winners' solutions. The paper also discusses the future of low-power computer vision.\",\n",
       " 'Integrating computer vision for industrial automation involves providing computers with the functions typical of human vision. Vision engines are increasing in speed, decreasing in cost, and improving with advances in microprocessor, Digital Signal Processors (DSP) and Field Programmable Gate Arrays (FPGA). Adopting these advances provide increased functionality at reduced cost-of-ownership, together with simpler operation. FPGA technology brings development advantages in that it combines ease of software development environments, together with the option of utilising System Level Design Languages (SLDL), with the speed resulting from parallel execution implemented within hardware. The focus of this research is to develop the capability to effectively compare Personal Computer (PC) performance versus FPGA for image processing tasks. The algorithms are first tested within a PC environment and their performance measured on various PC platforms. From the host of different image processing methods within the PC application, the algorithms with the most promise for parallel execution or speedup are selected and transported into an FPGA environment. The significance of comparing the performance of PC versus FPGA environments for vision applications is not merely for research purposes, but also to transport and apply vision projects to this increasingly popular and versatile platform. The vision algorithms for creation of a vision application capable of selected image processing and analysis functions have been developed. They include storing an image into memory, the application of noise filters, segmenting the image (with particular attention paid to thresholding) and performing connected component analysis.',\n",
       " 'nan',\n",
       " 'Object tracking is one of the major fundamental challenging problems in computer vision applications due to difficulties in tracking of objects can arises due to intrinsic and extrinsic factors like deformation, camera motion, motion blur and occlusion. This paper proposes a literature review on several state-of-the-art object detection and tracking algorithms in order to reduce the tracking drift.',\n",
       " 'In this study, we investigate a positioning-aided scheme using computer vision techniques for image sensor communication (ISC), generally referred to as a visible light communication (VLC) system that utilizes an image sensor (camera). The image sensor in ISC is frequently used as a typical receiver because it has the ability to measure light intensity and distinguish multiple light sources. Additionally, thanks to its ability to detect the angle of arrival of light, the image sensor can estimate its own position using computer vision techniques. The positioning accuracy of ISC is reported to be in sub-meter levels that render visible light positioning (VLP). VLP is one of the most excellent positioning applications in indoor settings as global positioning system cannot provide satisfying indoor positioning services. To apply this significant positioning performance of VLP to our visible light system, we propose a positioning-aided scheme for ISC using computer vision techniques here. In addition, we evaluate the demodulation performance and positioning accuracy of our proposed scheme.',\n",
       " 'Learning structured models using maximum margin techniques has become an indispensable tool for computer vision researchers, as many computer vision applications can be cast naturally as an image labeling problem. Pixel-based or superpixel-based conditional random fields are particularly popular examples. Typically, neighborhood graphs, which contain a large number of cycles, are used. As exact inference in loopy graphs is NP-hard in general, learning these models without approximations is usually deemed infeasible. In this work we show that, despite the theoretical hardness, it is possible to learn loopy models exactly in practical applications. To this end, we analyze the use of multiple approximate inference techniques together with cutting plane training of structural SVMs. We show that our proposed method yields exact solutions with an optimality guarantees in a computer vision application, for little additional computational cost. We also propose a dynamic caching scheme to accelerate training further, yielding runtimes that are comparable with approximate methods. We hope that this insight can lead to a reconsideration of the tractability of loopy models in computer vision.',\n",
       " 'A lot of Research has been developed to improve the reliability of smart parking systems. The utilization of computer vision is more beneficial than using sensors for the detection of parking spaces on smart parking systems. One smart camera can monitor multiple parking spaces according to camera sensing. The use of cameras is more efficient than using sensors, because sensors require expensive installation and maintenance costs. The accuracy and computational time are challenges that must be resolved by applying computer vision to classify the parking spaces. We perform several comparisons of computer vision classifications by utilizing the pre-trained Convolutional Neural Network (CNN). The mAlexnet customization is called by CmAlexnet to improve accuracy in classification. Parking space classification for CNRPark Camera A and B dataset was done. GoogleNet, Alexnet, VGGNet, tnAlexnet, and CtnAlexnet had varied results. From the testing done states CmAlexnet is better in the accuracy of parking space classification. The average accuracy of CmAlexnet outperforms all pre-trained with almost the same training time as mAlexnet. The test result stated that the performance of mAlexnet can be improved.',\n",
       " 'nan',\n",
       " \"This article is the result of an investigation to improve the communication with a case study that suffers Cerebral Palsy through the use of Computer Vision. At present, there is a 15% of the world's population that suffers some form of disability which prevents them from complying with the common activities of a normal person in a social environment. The technology can help to facilitate the implementation of processes that support people with special needs to improve their lifestyle; in this project the main objective is to improve the communication with the patient in order to facilitate the patient care. For conducting the investigation it was necessary the development of a prototype that detects body expressions using the OpenCV library with the Python programming language. The results are promising because the computer vision system is able to detect with high accuracy the following body patterns: headache 77%, happiness 75%, hunger 82%, fear 88% and recreation 77%. Finally, when a body pattern is detected it is communicated to the patient's caregiver through a mobile application.\",\n",
       " 'Volume is an important factor to determine the external quality of a food product. The volume measurement of food product is not a simple process if it is performed manually. For alternative, several volume measurement methods for food products have been proposed using 2D and 3D computer vision. Disk method and frustum cone method have been applied in many 2D computer visions to approximate the volume of axisymmetric food products. These methods were less in accuracy, since it used piecewise linear function to approximate the boundary of the object. This paper aims to propose a new framework for measuring the volume of axisymmetric food product based on cubic spline interpolation. Cubic spline interpolation is employed to construct a piecewise continuous polynomial of the boundary of object from captured image. The polynomial is then integrated to approximate the volume of the object. The simulation result shows that the proposed framework produced accurate volume measurement result.',\n",
       " 'In this demonstration paper, we present an innovative indoor localization architecture, coined Anyplace Computer Vision (AnyplaceCV), which provides an infrastructure-free (or zero infrastructure) method to localize in indoor spaces that lack any infrastructure whatsoever (e.g., Wi-Fi, BLE, UWB, RFID, Sonar, LED). We have developed a complete functional system of AnyplaceCV around the Anyplace open-source architecture we developed over the years and will make our contributions open-source. We will present AnyplaceCV in two modes: (i) Online Mode, where attendees will be able to collect and analyze real CV fingerprints at the conference venue; and (ii) Offline Mode, where attendees will be able to interact with collected measurements through a smartphone and PC.',\n",
       " \"Our group has been investigating the development of BCI systems for improving information delivery to a user, specifically systems for triaging image content based on what captures a user's attention. One of the systems we have developed uses single-trial EEG scores as noisy labels for a computer vision image retrieval system. In this paper we investigate how the noisy nature of the EEG-derived labels affects the resulting accuracy of the computer vision system. Specifically, we consider how the precision of the EEG scores affects the resulting precision of images retrieved by a graph-based transductive learning model designed to propagate image class labels based on image feature similarity and sparse labels.\",\n",
       " 'Three-dimensional (3D) data acquisition and real-time processing is a critical issue in an artificial vision system. The developing time-of-flight (TOF) camera as a real-time vision sensor for obtaining depth images has now received wide attention, due to its great potential in many areas, such as 3D perception, computer vision, robot navigation, human-machine interaction, augmented reality, and so on. This paper survey advances in TOF imaging technology mainly from the last decade. We focus only on recent progress of overcoming limitations such as systematic errors, object boundary ambiguity, multipath error, phase wrapping, and motion blur, and address the theoretical principles and future research trends as well.',\n",
       " 'At present, there is an increase in the quantity of computer vision method applications in the industry, robotics, in systems of virtual and augmented reality, geoinformation systems and so on. This leads to the need for the creation and development of efficient methods and algorithms for solving the problems of computer vision. In the paper, we present an approach to solve the problem of the finding of displacement vectors in the two consecutive video frames of the strain test process video. To analyse video we use the optical flow approach and propose improvements to feature extraction method of establishing of robust point correspondences between pair of key points in video frames.',\n",
       " 'Recently we developed flat facet solar concentrators that have low material cost. Each concentrator contains hundreds of components. To obtain low labor cost it is necessary to develop automated methods of components manufacture and assembly. In this paper we consider the problems of the concentrator manufacture. To make the automation algorithms more efficient we use computer vision methods based on neural networks. We developed these methods for handwritten digits and face recognition. Here we apply them to automated manufacture of solar concentrators. Several years ago we developed an algorithm of automatic placement of a pin in a hole. Later a visual based algorithm of component measurement was developed. In this paper we consider the possibilities of application of computer vision algorithms in solar concentrator manufacture.',\n",
       " 'The computer is an ubiquitous element of modern society, nonetheless, human computer interaction is still rather inflexible. Particularly in local collaborative environments, like office meetings, the property that the mouse and keyboard exhibit of being a gateway for the individual to act upon a workspace, makes local computer mediated collaboration uncomfortable, as users have to time-share their actions upon that workspace. We present in this paper a novel, very fast, vision based interface that allows multiple users to interact simultaneously with a single computer by performing hand gestures, which are filmed by a static video camera. This interface attempts to continuously recognize predefined postures and movements using a view-dependent method. We also present A.C.O, an application which receives input from the vision based interface and allows users around a table to collaborate playing synthesized music instruments by moving their hands.',\n",
       " 'Biometric systems are playing an important role in identifying a person, thus contributing to global security. There are many possible biometrics, for example height, DNA, handwriting etc., but computer vision based biometrics have found an important place in the domain of human identification. Computer vision based biometrics include identification of face, fingerprints, iris etc. and using their abilities to create efficient authentication systems. In this paper, we work on a dataset [1] of iris images and make use of deep learning to identify and verify the iris of a person. Hyperparameter tuning for deep networks and optimization techniques have been taken into account in this system. The proposed system is trained using a combination of Convolutional Neural Networks and Softmax classifier to extract features from localized regions of the input iris images. This is followed by classification into one out of 224 classes of the dataset. From the results, we conclude that the choice of hyperparameters and optimizers affects the efficiency of our proposed system. Our proposed approach outperforms existing approaches by attaining a high accuracy of 98 percent.',\n",
       " 'In recent years, large number of cameras have been installed in freeway and read environments. While the use of some of these cameras is being automated through computer vision, few computer vision systems allow for true wide-area large scaled automation. This paper describes a distributed computing system capable of managing an arbitrarily large sensor network using only common computing and networking platforms. The architecture is capable of handling many common computer vision tasks, as well as the inter-sensor communication necessary for developing new algorithms which employ data from multiple sensors. This system is tested with an algorithm which tracks moving objects through a prototype camera network with non-overlapping fields of view in a college campus environment. This algorithm allows the system to maintain the identity of a tracked objects as it leaves and enters the fields of view of individual sensors. Such an algorithm is necessary for applications which require tracking objects over large distances or over long periods of time in an environment without complete sensor coverage.',\n",
       " 'A reliable traffic flow monitoring and traffic analysis approach using computer vision techniques has been proposed in this paper. The exponential increase in traffic density at urban intersections in the past few decades has raised precious and challenging demands to computer vision algorithms and technological solutions. The focus of this paper is to suggest a statistical based approach to determine the traffic parameters at heavily crowded urban intersections. The algorithm in addition to accurate tracking and counting of freeway traffic also offers high efficiency for determining vehicle count at a high traffic density T-intersection. The system uses Intel Open CV library for image processing. The implementation of algorithm is done using C++. The real time video sequence is obtained from a stationary camera placed atop a high building overlooking the particular T intersection. This paper suggests a dynamic method where each vehicle at a T intersection is passed through a number of detection zones and the final count of vehicles is derived from a statistical equation.',\n",
       " 'The emergence of new wearable technologies, such as action cameras and smart glasses, has increased the interest of computer vision scientists in the first person perspective. Nowadays, this field is attracting attention and investments of companies aiming to develop commercial devices with first person vision (FPV) recording capabilities. Due to this interest, an increasing demand of methods to process these videos, possibly in real time, is expected. The current approaches present a particular combinations of different image features and quantitative methods to accomplish specific objectives like object detection, activity recognition, user-machine interaction, and so on. This paper summarizes the evolution of the state of the art in FPV video analysis between 1997 and 2014, highlighting, among others, the most commonly used features, methods, challenges, and opportunities within the field.',\n",
       " 'Assembly state detection, i.e., object state detection, has a critical meaning in computer vision tasks, especially in AR assisted assembly. Unlike other object detection problems, the visual difference between different object states can be subtle. For the better learning of such subtle appearance difference, we proposed a two-level group attention module (TGA), which consists of inter-group attention and intro-group attention. The relationship between feature groups as well as the representation within each feature group is simultaneously enhanced. We embedded the proposed TGA module in a popular object detector and evaluated it on two new datasets related to object state estimation. The result shows that our proposed attention module outperforms the baseline attention module.',\n",
       " 'We provide a novel hardware and software system for micro air vehicles (MAV) that allows high-speed, low-latency onboard image processing. It uses up to four cameras in parallel on a miniature rotary wing platform. The MAV navigates based on onboard processed computer vision in GPS-denied in-and outdoor environments. It can process in parallel images and inertial measurement information from multiple cameras for multiple purposes (localization, pattern recognition, obstacle avoidance) by distributing the images on a central, low-latency image hub. Furthermore the system can utilize low-bandwith radio links for communication and is designed and optimized to scale to swarm use. Experimental results show successful flight with a range of onboard computer vision algorithms, including localization, obstacle avoidance and pattern recognition.',\n",
       " 'This article presents a computer vision/dead reckoning based monitoring of transit service reliability in urban areas. The traditional dead reckoning and signpost technologies for position determination, which many agencies use, suffers from a number of limitations, including the drift of the dead reckoning system and the incapability of the signpost system to locate a vehicle on a continuous basis. In an urban area a GPS based system also may have problems in positioning; i.e., signal reflect from buildings and other reflective surface or building canyons and overpasses can block some or all of a satellite signal. To overcome these limitations, we suggest an integrated positioning system, consisting of a dead reckoning unit coupled with a computer vision system.',\n",
       " 'Vision-based user interfaces are a feasible and advantageous modality for wearable computers. To substantiate this claim, we present a robust real-time hand gesture recognition system that is capable of being the sole input provider for a demonstration application. It achieves usability and interactivity even when both the head-worn camera and the object of interest are in motion. We describe a set of general gesture-based interaction styles and explore their characteristics in terms of task suitability and the computer vision algorithms required for their recognition. Preliminary evaluation of our prototype system leads to the conclusion that vision-based interfaces have achieved the maturity necessary to help overcome some limitations of more traditional mobile user interfaces.',\n",
       " \"In this paper, we are presenting a proposed system for Smart Home Automation technique with Raspberry Pi using IoT and it is done by integrating cameras and motion sensors into a web application. To design this system, we are using a Raspberry Pi module with Computer Vision techniques. Using this, we can control home appliances connected through a monitor based internet. Raspberry Pi operates and controls motion sensors and video cameras for sensing and surveillance. For instance, it captures intruder's identity and detects its presence using simple Computer Vision Technique (CVT). Whenever motion is detected, the cameras will start recording and Raspberry Pi device alerts the owner through an SMS and alarm call.\",\n",
       " \"We describe a monocular robot vision system which accomplishes accurate 3-DOF dead-reckoning, closed-loop motion control, and precipice and obstacle detection, all in dynamic environments, using a single, consumer-grade webcam and typical laptop computer hardware. Simultaneous translation and rotation are accurately measured, and the camera need not be placed at the robot's center of rotation. The algorithm is straightforward to implement and robust to noisy measurements. The software is based on open source computer vision libraries and is itself open source. It has been tested in a wide variety of real-world environments and on several different mobile robot platforms.\",\n",
       " 'Carrying out effective and sustainable agriculture product has become an important issue in recent years. Agricultural production has to keep up with an ever-increasing population. A key to this is the usage of modern techniques (for precision agriculture) to take advantage of the quality in the market. The paper reviews various quality evaluation and grading techniques of Oryza Sativa L. (rice) in food industry using computer vision and image processing. In this paper basic problem of rice industry for quality assessment is defined which is traditionally done manually by human inspector. Computer Vision provides one alternative for an automated, non-destructive and cost-effective technique. In this paper we quantify the qualities of various rice varieties in Asian subcontinent and figure out features which directly or indirectly affect the quality of the rice. Based on these features a generalized approach of quality is proposed to be used for quality evaluation of any type of rice variety.',\n",
       " 'This article describes a vision-based manipulation process for real-time moving objects tracking and grasping, aiming at industrial manufacturing and assembling applications. The adoption of computer vision techniques for object recognition is implemented by means of a stereoscopic system using color based methods under OpenCV libraries. The visual software is directly coupled with the control software of the robotic arm Katana 6M90G manufactured by Neuronics AG, running under a Linux-based Operating System (OS) distribution Lubuntu, over a low-cost and powerful microprocessor Odroid U3. Experimental studies validate the effectiveness of the implementation, while remarking the advantageous effects of the 3D pose estimation process.',\n",
       " 'We present the design and implementation of a real-time computer vision system for a rotor-craft unmanned aerial vehicle to land onto a known landing target. This vision system consists of customized software and off-the-shelf hardware which perform image processing, segmentation feature. point extraction, camera pan/tilt control, and motion estimation. We introduce the design of a landing target which significantly simplifies the computer vision tasks such as corner detection and correspondence matching. Customized algorithms are developed to allow for realtime computation at a frame rate of 30Hz. Such algorithms include certain linear and nonlinear optimization schemes for model-based camera pose estimation. We present results from an actual flight test which show the vision-based state estimates are accurate to within 5cm in each axis of translation and 5 degrees in each axis Of rotation, making vision a viable sensor to be placed in the control loop of a hierarchical flight management system.',\n",
       " 'Extracting depth information from the stereo image pair is a commonly used method in 3-D computer vision. For robotics and unmanned vehicle applications that require real-time performance, speed is often more important than accuracy. In recent years, Convolutional Neural Networks (CNNs) have shown great success in many computer vision applications including classification, segmentation, object detection, edge detection, and stereo vision estimation. Existing network architectures for stereo vision estimation predict very little information during the forward pass and are only able to calculate the disparity for one pixel at a time. In this paper, we propose a parallel architecture to speed up disparity map computation by simultaneously processing all pixels on one horizontal line. We train and test our network on five Middlebury datasets. Our parallel architecture achieves at least a 10x speedup compared to existing networks. Its accuracy is also very competitive.',\n",
       " 'The fast, non-contact and highly precise shape measurement of objects is a key importance in the scientific-technological area as well as the area of practical measurement technology. A approach is proposed to determines the world coordinates of markers that are attached to the large-scale object measured, which is based on computer vision. The object-point 3-D coordinates of partial views, containing markers, are obtained by other techniques that have been successfully commercialized, such as triangulation or laser scanning, etc. The coordinates of markers is used to transform point clouds of the partial views to the world coordinate system to achieve 3-D shape measurement of large-Scale complex car body.',\n",
       " 'a computer machine vision system that can be used for automatic high-speed fruit sorting and grading is proposed. The objective of the project is to develop the advanced quality control inspection system makes use of distributed network architecture to interface the camera unit to a computer system through GigE LAN environment in a flexible way. The development work activity involves in dynamic capturing of image signal from camera when the objects are moving on the conveyor in real time based on synchronized trigger events. This project planned to do in visual studio using OpenCV. The process involved for estimating the colour information and geometry parameters makes use of sequence of complex library functionality like removing the noise, detecting the edges, smoothing, dilate, erode, filling, deblurring, filtering, histogram, colour values, pixel averaging, etc.',\n",
       " 'A low power smart imager design is proposed for real time machine vision applications. It takes advantages of recent advances in integrated sensing/processing designs, electronic neural networks, and sub-micron VLSI technology. The smart vision system integrates an active pixel camera with a programmable neural computer and an advanced microcomputer. A system-on-a-chip implementation of this smart vision system is shown to be feasible by integrating the whole system into a 3-cm x 3-cm chip design in a 0.18m CMOS technology. The on-chip neural computer provides one tera-operation-per-second computing power for various parallel vision operations and smart sensor functions. Its high performance is due to massively parallel computing structures, high data throughput rates, fast learning capabilities, and system-on-a-chip implementation. This highly integrated smart imager can be used for various scientific missions and other military, industrial or commercial vision applications.',\n",
       " \"Computer vision techniques applied to systems used on road maintenance, which are related either to traffic signs or to the road itself, are playing a major role in many countries because of the higher investment on public works of this kind. These systems are able to collect a wide range of information automatically and quickly, with the aim of improving road safety. In this context, the correct visibility of traffic signs and panels is vital for the safety of drivers. This paper describes an approach to the VISUAL Inspection of Signs and panEls (VISUALISE), which is an automatic inspection system, mounted onboard a vehicle, which performs inspection tasks at conventional driving speeds. VISUALISE allows for an improvement in the awareness of the road signaling state, supporting planning and decision making on the administration's and infrastructure operators' side. A description of the main computer vision techniques and some experimental results obtained from thousands of kilometers are presented. Finally, the conclusions of the system are described.\",\n",
       " 'Reconfigurable computing is emerging as the new paradigm for satisfying the simultaneous demand for application performance and flexibility. The ability to customize the architecture to match the computation and the data flow of the application has demonstrated significant performance benefits compared to general purpose architectures. Computer vision applications are one class of applications that have significant heterogeneity in their computation and communication structures. At the low level, vision algorithms have regular repetitive computations operating on large sets of image data with predictable data dependencies. At the higher level, the computations have irregular dependencies. Computer vision application characteristics have significant overlap with the advantages of reconfigurable architectures. The main focus of the paper is on outlining the methodologies required to realize the potential of reconfigurable architectures for vision applications. After giving a broad introduction to reconfigurable computing, the advantages of utilizing reconfigurable architectures for vision applications are outlined and illustrated using example computations. The paper discusses the development of fundamental configurable computing models that abstract the underlying hardware for high-level application mapping. The Hybrid System Architecture Model and algorithms utilizing the model are illustrated to demonstrate a formal framework. The paper also outlines ongoing research and provides a comprehensive list of references for further reading.',\n",
       " 'In this paper we report a vision-based human-computer interface that enables touch-free communication with a computer by means of head movements and eye-blinks. The developed interface is intended to be used by physically disabled persons. It was shown that image analysis software running on commercial off-theshelf hardware (a mid-range notebook and a web-camera) allows for successful implementation of typical computer interaction tasks, such as: browsing the Internet, viewing image albums and reading pdf files. Test results of the interface with participation of 6 volunteers are summarized. The paper also includes a short review of other up-to-date solutions to human-computer interaction for the disabled, ranging from brain-computer interface to a tongue control system.',\n",
       " 'Mobile tracking robot will play an important role in our future life. Target recognition, distance estimation and motion estimation are three primary functions of this kind of robot, respectively. Consequently, the irrelevant three technologies, pattern recognition, binocular vision and fast block-matching algorithm, make up of a novel approach of computer vision navigation for mobile tracking robot. This novel approach is analyzed in this paper with emphasis on distance estimation and motion estimation. Extended discussion of several interesting observations are also presented. Experimental results show the validity of the approach. Integrating the key components together, a simplified verification frame based on LEGO Mindstorms NXT is designed to check the three technologies combination.',\n",
       " 'The paper presents a compact vision system for efficient contours extraction in high-speed applications. By exploiting the ultra high temporal resolution and the sparse representation of the sensors data in reacting to scene dynamics, the system fosters efficient embedded computer vision for ultra high-speed applications. The results reported in this paper show the sensor output quality for a wide range of object velocity (5-40 m/s), and demonstrate the object data volume independence from the velocity as well as the steadiness of the object quality. The influence of object velocity on high-performance embedded computer vision is also discussed.',\n",
       " 'Automated acquisition of friction data is an interesting approach to more successfully bridge the reality gap in simulation than conventional mathematical models. To advance this area of research, we present a novel inexpensive computer vision platform as a solution for collecting and processing friction data, and we make available the open source software and data sets collected with our vision robotic approach. This paper is focused on gathering data on anisotropic static friction behavior as this is ideal for inexpensive vision approach we propose. The data set and experimental setup provide a solid foundation for a wider robotics simulation community to conduct their own experiments.',\n",
       " 'nan',\n",
       " 'We report development of a hardware accelerator for robot-soccer systems equipped with global vision. An FPGA-embedded preprocessor captures individual video frames and detects colour blobs (corresponding to the ball and the players). The numerical characteristics of the blobs (colour, size and location) are sent to the host computer so that the current game configuration can be immediately identified by the computer. By exploiting parallelism and pipelining techniques available in the FPGA, a real-time performance has been achieved at rates higher than TV standards. The prototype implementation of the module (using Celoxica (TM) RC203 board with Virtex II FPGA) is able to process frames and to transfer results to the host computer at over 50 frames per second rate. A software module (Visual C++, Windows (R) XP platform) run be the host computer is used to interface the vision hardware, and to analyze and visualize the current game configuration.',\n",
       " 'Computer vision has shown promising potential in wearable robotics applications (e.g., human grasping target prediction and context understanding). However, in practice, the performance of computer vision algorithms is challenged by insufficient or biased training, observation noise, cluttered background, etc. By leveraging Bayesian deep learning (BDL), we have developed a novel, reliable vision-based framework to assist upper limb prosthesis grasping during arm reaching. This framework can measure different types of uncertainties from the model and data for grasping target recognition in realistic and challenging scenarios. A probability calibration network was developed to fuse the uncertainty measures into one calibrated probability for online decision making. We formulated the problem as the prediction of grasping target while arm reaching. Specifically, we developed a 3-D simulation platform to simulate and analyze the performance of vision algorithms under several common challenging scenarios in practice. In addition, we integrated our approach into a shared control framework of a prosthetic arm and demonstrated its potential at assisting human participants with fluent target reaching and grasping tasks.',\n",
       " 'Symmetry is a pervasive phenomenon presenting itself in all forms and scales in natural and manmade environments. Its detection plays an essential role at all levels of human as well as machine perception. The recent resurging interest in computational symmetry for computer vision and computer graphics applications has motivated us to conduct a US NSF funded symmetry detection algorithm competition as a workshop affiliated with the Computer Vision and Pattern Recognition (CVPR) Conference, 2013. This competition sets a more complete benchmark for computer vision symmetry detection algorithms. In this report we explain the evaluation metric and the automatic execution of the evaluation workflow. We also present and analyze the algorithms submitted, and show their results on three test sets of real world images depicting reflection, rotation and translation symmetries respectively. This competition establishes a performance baseline for future work on symmetry detection.',\n",
       " 'nan',\n",
       " 'Today, quality control is a nodal point in many industries, and in the glass one in particular; in most cases the human control does not catch up with the pressing market requirements, therefore computer vision inspection systems are preferable to reduce costs and to improve the product quality, but several problems must be solved. In this paper a prototype system, able to reproduce all the functionalities of an automatic glass inspection system, is designed and realized; it guarantees good results and considerable reliability with low incidence on manufacturing costs. The final in-line computer vision system is under development in cooperation with a specialized electronic industry.',\n",
       " 'In this paper, we propose a human action recognition system suitable for embedded computer vision applications in security systems, human-computer interaction and intelligent environments. Our system is suitable for embedded computer vision application based on three reasons. Firstly, the system was based on a linear Support Vector Machine (SVM) classifier where classification progress can be implemented easily and quickly in embedded hardware. Secondly, we use compacted motion features easily obtained from videos. We address the limitations of the well known Motion History Image (MHI) and propose a new Hierarchical Motion History Histogram (HMHH) feature to represent the motion information. HMHH not only provides rich motion information, but also remains computationally inexpensive. Finally, we combine MHI and HMHH together and extract a low dimension feature vector to be used in the SVM classifiers. Experimental results show that our system achieves significant improvement on the recognition performance.',\n",
       " 'nan',\n",
       " 'Living in an information age the whole earth is a small globe in our hands with the advancements of computers, smartphones etc. The usage of computers in our day-to-day activities has increased enormously leading to both positive and negative effects in our lives. The negative effects are related to health problems such as Computer Vision Syndrome (CVS) etc. Prolonged use of computers would lead to a significant reduction of spontaneous eye blink rate due to the high visual demand of the screen and concentration on the work. The proposed system develops a prototype using blink as a solution to prevent CVS. The first part of the work captures video frames using web-camera mounted on the computer or laptop. These frames are processed dynamically by cropping only the eyes. The algorithms performed on the eye-frames are direct pixel count, gradient, Canny edge and Laplacian of Gaussian (LoG). These determine the eye-status based on the threshold value and the proposed idea, the difference between upper and lower eye frames. Various experiments are done and their algorithms are compared and concluded that the proposed algorithm yields 99.95% accuracy.',\n",
       " 'Autonomous dirigibles - aerial robots that are a blimp controlled by computer based on information gathered by sensors - are a new and promising research field in Robotics, offering several original work opportunities. One of them is the study of visual navigation, that would allow an autonomous dirigible to follow precise trajectories based on environmental visual information imaged by on-board cameras. The first step in the direction of the developing of such a capacity is the creation of a Computer Vision system able to supply to the autonomous dirigible its position and orientation in tridimensional space from the acquired images. This project describes such a system, capable of estimating position and orientation from images of visual beacons - objects with known geometrical properties and recognizable by the system. Experimental results showing the correct and efficient functioning of the system are shown and have your implications and future possibilities discussed.',\n",
       " 'nan',\n",
       " 'Stereo vision is a well-known ranging method because it resembles the basic mechanism of the human eye. However, the computational complexity and large amount of data access make real-time processing of stereo vision challenging because of the inherent instruction cycle delay within conventional computers. In order to solve this problem, the past 20 years of research have focused on the use of dedicated hardware architecture for stereo vision. This paper proposes a fully pipelined stereo vision system providing a dense disparity image with additional sub-pixel accuracy in real-time. The entire stereo vision process, such as rectification, stereo matching, and post-processing, is realized using a single field programmable gate array (FPGA) without the necessity of any external devices. The hardware implementation is more than 230 times faster when compared to a software program operating on a conventional computer, and shows stronger performance over previous hardware-related studies.',\n",
       " 'In this paper, the latency problem of computer vision systems is addressed in the framework of autonomous Unmanned Aerial Vehicles. Recent advancements in sensors and embedded electronic boards made it possible to load, even on small size drones, cameras and image processing devices. Here, a navigation system based on computer vision is considered as one of the most popular applications exploiting this technology in substitution of Global Navigation Satellite System solutions. The main issues when working with a video stream are the limited frame rate (i.e., small sampling frequency), and the non negligible computational time for extracting features from the images (i.e., latency). In particular, the latency negatively affects a position controller that exploits data from the computer vision system, preventing its usage for precise positioning applications. In this paper, a possible solution is designed according to this recipe: First, a sensor fusion technique able to compensate the latency is adopted to estimate the velocity using the position of the computer vision system and the accelerations provided by a Inertial Measurement Unit. Then, a controller is developed using two feedback loops, the inner one accounting for the estimated velocity, and the outer one exploiting the delayed position. Test experiments, showing very positive results, are finally reported.',\n",
       " 'In this paper, a virtual assistant for the visually impaired is built using various technologies such as computer vision, object recognition, emotion detection, and text-to-speech using a micro-computer in order to describe the scene around as a narrative to the user. We generate an audio narrative by converting the scenes obtained by visual data in front of user to text which describes the important objects in the scene in audio format. Possible future ventures into involvement of various sensors to perform SLAM based navigation along with barcode reading.',\n",
       " 'nan',\n",
       " 'Artificial intelligence technology is currently a hot spot for research and application, and the new cross-technology formed by artificial intelligence and multiple technologies has been widely used in Internet, finance, security and other industries. This paper first introduces the development of artificial intelligence, and then analyzes the key technologies of artificial intelligence, including computer vision, machine learning, natural language processing, human-computer exchange, virtual reality, biometric recognition, etc. Finally, the development trend of artificial intelligence technology is described.',\n",
       " \"This article describes the simple step by step system, to teach object recognition and tracking in computer vision systems. Methodology is based on object recognition system complexity incrementation. Student doesn't need any knowledge about mathematical principles of computer based object detection. This is achieved using OpenCV and other high level libraries. Article consists from explanation of simple detection methods based on OpenCv, to complex pattern-based detection methods provided by OpenTLD libraries. All library recognition methods are explained in C/C++ language and results are illustrated as graphical output images.\",\n",
       " 'Untangling of structures like ropes and wires by autonomous robots can be useful in areas such as personal robotics, industries and electrical wiring & repairing by robots. This problem can be tackled by using computer vision system in robot. This paper proposes a computer vision based method for analyzing visual data acquired from camera for perceiving the overlap of wires, ropes, hoses i.e. detecting tangles. Information obtained after processing image according to the proposed method comprises of position of tangles in tangled object and which wire passes over which wire. This information can then be used to guide robot to untangle wire/s. Given an image, preprocessing is done to remove noise. Then edges of wire are detected. After that, the image is divided into smaller blocks and each block is checked for wire overlap/s and finding other relevant information. TANGLED-100 dataset was introduced, which consists of images of tangled linear deformable objects. Method discussed in here was tested on the TANGLED-100 dataset. Accuracy achieved during experiments was found to be 74.9%. Robotic simulations were carried out to demonstrate the use of the proposed method in applications of robot. Proposed method is a general method that can be used by robots working in different situations.',\n",
       " 'This paper proposes vision based collision risk estimation method for an unmanned surface vehicle. Vision based target motion analysis was performed to transform vision information of obstacles into motion information. In vision based target motion analysis, camera model and optical flow are adopted. Collision risk was calculated by fuzzy estimator which uses target motion information and vision information as input variables. To validate suggested collision risk estimation method, an unmanned surface vehicle experiment was performed.',\n",
       " 'nan',\n",
       " 'A computer vision assisted semi-automatic virtual reality (VR) calibration technology has been developed that can accurately match a virtual environment of graphically simulated three-dimensional (3-D) models to the video images of the real task environment. In conventional model-based computer vision, camera calibration and object localization are performed sequentially. This sequential update cannot compensate for the inaccuracy in initial camera calibrations. We have developed a new 20-variable weighted least-squares algorithm that updates both camera and object models simultaneously for given two camera views of two mating objects. This simultaneous update enables accurate model matching even with rough, approximate initial camera calibrations. The developed semi-automatic VR calibration supports automated intermediate updates, eliminating nearly all operator interaction except for initial coarse matching. In our quasistatic supervisory telerobotic applications, intermediate VR calibrations are performed intermittently at a few robot stopping poses only, as a cost-effective and safer approach compared to real-time visual servoing. Extensive experimental results comparing alignment errors under various viewing conditions are presented in the paper. Using the VR calibration technology developed,we hare successfully demonstrated an orbital replacement unit (ORU) insertion task within the required +/-1/4 in and +/-3 degrees alignment precision.',\n",
       " 'This article introduces a Computer Aided Instruction platform based on Augmented Reality. The platform can superimpose virtual objects and virtual characters on the classroom. Teachers can interact with the virtual objects and virtual characters by presenting and moving special vision markers. In the platform, machine vision technology is used to implement human-computer seamless interaction, Virtual Reality Modeling Language is used to describe the virtual scene, Artoolkit library is used to acquire video stream and calibrate coordinate, OpenGL library is used to render the virtual scene. The platform has been applied in teaching process and achieved good results.',\n",
       " \"This paper mainly discusses the application of computer visual technology in embedded image recognition system. We propose and realize a new system called automatic recognition system of analog display instruments using computer vision. After the brief introduction of the characteristics of computer visual technology and embedded system, this paper analyzes the principles and the developing status of computer visual technology used in embedded system, proposes the automatic recognition system of analog display instruments using computer vision, which is on the basis of the application characteristics. Its working principles are just like the principles of the visual system of human being, analyzing the images viewed by the system, and obtaining the information needed. This paper designs the software and hardware system through focusing on the digital image processing technology and the application circumstances of the embedded system. The following key problems have been studied: how to realize automatic recognition of analog display instruments, how to solve automatic judgment of different kinds of indicator dial, how to eliminate disturbance cased by light and circumstances while exact result, how to realize the image collecting with high quality while low cost, how to improve the efficiency of data collection and image recognition( speed and accuracy) and how to correspond the each module design in hardware and software to improve the system's performance and stabilization. At the end, this paper designs automatic recognition program of analog display instruments\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = df[\"Abstract\"].astype(str).tolist()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine')\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "ctfidf_model = ClassTfidfTransformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTopic(\n",
    "  embedding_model=embedding_model,        \n",
    "  umap_model=umap_model,               \n",
    "  hdbscan_model=hdbscan_model,              \n",
    "  vectorizer_model=vectorizer_model, \n",
    "  ctfidf_model=ctfidf_model,\n",
    "  nr_topics=50,\n",
    "  n_gram_range=(1,2)       \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics, probabilities = model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>129</td>\n",
       "      <td>0_nan___</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>571</td>\n",
       "      <td>1_vision_computer_based_paper</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Topic  Count                           Name\n",
       "0      0    129                       0_nan___\n",
       "1      1    571  1_vision_computer_based_paper"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_df = model.get_topic_info()\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vision', 0.09829921336888292),\n",
       " ('computer', 0.08904437265725755),\n",
       " ('based', 0.040366597993864724),\n",
       " ('paper', 0.03822021659227216),\n",
       " ('image', 0.035602446182882476),\n",
       " ('using', 0.0302605592985919),\n",
       " ('detection', 0.025884768266297212),\n",
       " ('algorithms', 0.02492948497032062),\n",
       " ('used', 0.02452760440652543),\n",
       " ('processing', 0.02355347387934571)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_topic = model.get_topic\n",
    "get_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(topic_word_df)):\n\u001b[1;32m     25\u001b[0m   \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     topic_word_df[col[j\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m]][i]\u001b[39m=\u001b[39mget_topic(i\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)[j][\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "zero=[]\n",
    "number=[]\n",
    "for i in range(len(topic_df)):\n",
    "  zero.append('0')\n",
    "  number.append(i-1)\n",
    "\n",
    "topic_word_df=pd.DataFrame({\n",
    "    'topic_num':number,\n",
    "    'topic':topic_df['Name'],\n",
    "    'w1':zero,\n",
    "    'w2':zero,\n",
    "    'w3':zero,\n",
    "    'w4':zero,\n",
    "    'w5':zero,\n",
    "    'w6':zero,\n",
    "    'w7':zero,\n",
    "    'w8':zero,\n",
    "    'w9':zero,\n",
    "    'w10':zero\n",
    "})\n",
    "\n",
    "#각 토픽별 10개 단어 정리\n",
    "col = topic_word_df.columns\n",
    "for i in range(len(topic_word_df)):\n",
    "  for j in range(10):\n",
    "    topic_word_df[col[j+2]][i]=get_topic(i-1)[j][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHmin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
