{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "2Btqqy4FAWwu"
      },
      "source": [
        "# Week 6 Topic Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOWARjs4A6zg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gensim\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gXzmbn0aAdep"
      },
      "source": [
        "## Truncated SVD example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXTxNkrE8kX4"
      },
      "outputs": [],
      "source": [
        "# Get TruncatedSVD\n",
        "\n",
        "sample = [[1,0,0],\n",
        "          [1,1,2],\n",
        "          [1,1,0],\n",
        "          [1,1,0],\n",
        "          [1,1,0],\n",
        "          [1,0,0],\n",
        "          [1,0,1],\n",
        "          [0,1,1],\n",
        "          [0,0,1]\n",
        "          ]\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_model_sample = TruncatedSVD(n_components=2, algorithm='randomized', n_iter=15, random_state=None)\n",
        "svd_model_sample.fit_transform(sample)\n",
        "\n",
        "# Generate Sigma\n",
        "sigma = svd_model_sample.singular_values_\n",
        "sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cWbRaBq7xDF"
      },
      "outputs": [],
      "source": [
        "# Generate VT\n",
        "VT = svd_model_sample.components_\n",
        "VT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLWjR_0p9t7S"
      },
      "outputs": [],
      "source": [
        "# Generate U\n",
        "U = svd_model_sample.transform(sample).dot(np.linalg.inv(np.diag(svd_model_sample.singular_values_)))\n",
        "U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm35uxIc-SZh"
      },
      "outputs": [],
      "source": [
        "# Relationship btw words and topics\n",
        "U*sigma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OysGDO-U-XdU"
      },
      "outputs": [],
      "source": [
        "# Relationship btw topics and documents\n",
        "np.diag(sigma).dot(VT)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6QVvL-mbAhFZ"
      },
      "source": [
        "## LSA w/ Fetch_20newsgroups and Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5Udc0yj3Fwd"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
        "documents = dataset.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "07DeEHFF3H2e"
      },
      "outputs": [],
      "source": [
        "# An example of documents\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAHa8ca73Vzw"
      },
      "outputs": [],
      "source": [
        "# Data Cleansing\n",
        "news_df = pd.DataFrame({'document':documents})\n",
        "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
        "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q1joVZx23Zgs"
      },
      "outputs": [],
      "source": [
        "# An example of preprocessed documents\n",
        "news_df['clean_doc'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lFaNTjyH3hk8"
      },
      "outputs": [],
      "source": [
        "# Build TF-IDF Matrix\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(stop_words='english', \n",
        "max_features= 1000,\n",
        "max_df = 0.5, \n",
        "smooth_idf=True)\n",
        "\n",
        "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RtqAZ17N3661"
      },
      "outputs": [],
      "source": [
        "# Get TruncatedSVD\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
        "svd_model.fit(X)\n",
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "\n",
        "get_topics(svd_model.components_,terms)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "7IXYITuXApG_"
      },
      "source": [
        "## LDA w/ Fetch_20newsgroups and Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQCJ5furFJSY"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda_model = LatentDirichletAllocation(n_components=20, learning_method='online', random_state=123, max_iter=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y59abeV7FMZ7"
      },
      "outputs": [],
      "source": [
        "lda_model.fit_transform(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XI4mw9iFV47"
      },
      "outputs": [],
      "source": [
        "print(lda_model.components_)\n",
        "print(lda_model.components_.shape) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUD58KzdFdS7"
      },
      "outputs": [],
      "source": [
        "terms = vectorizer.get_feature_names()\n",
        "\n",
        "def get_topics(components, feature_names, n=5):\n",
        "    for idx, topic in enumerate(components):\n",
        "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(2)) for i in topic.argsort()[:-n - 1:-1]])\n",
        "        \n",
        "get_topics(lda_model.components_,terms)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
