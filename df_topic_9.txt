Document
"human-computer interaction ( hci ) related technology focus implementation interactive computational system . study hci emphasize system use , creation new technique support user activity , access information , ensure seamless communication . use artificial intelligence deep learning-based model extensive across various domain yield state-of-the-art result . present study , crow search-based convolution neural network model implement gesture recognition pertain hci domain . hand gesture dataset use study publicly available one , download kaggle . work , one-hot encoding technique use convert categorical data value binary form . follow implementation crow search algorithm ( csa ) select optimal hyper-parameters training dataset use convolution neural network . irrelevant parameter eliminate consideration , contribute towards enhancement accuracy classify hand gesture . model generate 100 percent training test accuracy justify superiority model traditional state-of-the-art model ."
"mouse one wonderful invention human-computer interaction ( hci ) technology . currently , wireless mouse bluetooth mouse still use device free device completely since use battery power dongle connect pc . propose ai virtual mouse system , limitation overcome employ webcam built-in camera capturing hand gesture hand tip detection use computer vision . algorithm use system make use machine learn algorithm . base hand gesture , computer control virtually perform left click , right click , scroll function , computer cursor function without use physical mouse . algorithm base deep learning detect hand . hence , propose system avoid covid-19 spread eliminate human intervention dependency device control computer ."
"sign language , different form communication language , important large group people society . different sign sign language variability hand shape , motion profile , position hand , face , body part contribute sign . , visual sign language recognition complex research area computer vision . many model propose different researcher significant improvement deep learning approach recent year . survey , review vision base propose model sign language recognition use deep learning approach last five year . overall trend propose model indicate significant improvement recognition accuracy sign language recognition , challenge yet need solve . present taxonomy categorize propose model isolated continuous sign language recognition , discuss application , datasets , hybrid model , complexity , future line research field ."
"hand gesture key component communication since beginning era . hand gesture foundation sign language , visual form communication . paper , deep learning base convolutional neural network ( cnn ) model specifically design recognition gesture-based sign language . model compact representation achieve good classification accuracy number model parameter exist architecture cnn . order evaluate efficacy model , vgg-11 vgg-16 also train test work . evaluate performance , 2 datasets consider . first , work , large collection indian sign language ( isl ) gesture consist 2150 image collect use rgb camera , second , publicly available american sign language ( asl ) dataset use . high accuracy 99.96 % 100 % obtain propose model isl asl datasets respectively . performance proposed system , vgg-11 , vgg-16 experimentally evaluate compare exist state-of-art approach . addition accuracy , efficiency index also use ascertain robustness propose work . finding indicate propose model outperform exist technique potential classify maximum gesture minimal rate error . model also test augment data find invariant rotation scale transformation ."
"sign language recognition consider important challenging application gesture recognition , involve field pattern recognition , machine learning computer vision . mainly due complex visual-gestural nature sign language availability database study relate automatic recognition . work present development validation brazilian sign language ( libras ) public database . record protocol describes ( 1 ) chosen sign , ( 2 ) signaller characteristic , ( 3 ) sensor software use video acquisition , ( 4 ) recording scenario ( 5 ) data structure . provide step well define , database 1000 video 20 libra sign record twelve different people create use rgb-d sensor rgb camera . sign record five time signaller . correspond database 1200 sample following data : ( 1 ) rgb video frame , ( 2 ) depth , ( 3 ) body point ( 4 ) face information . approach use deep learning-based model apply classify sign base 3d 2d convolutional neural network . best result show average accuracy 93.3 % . paper present important contribution research community provide publicly available sign language dataset baseline result comparison ."
"vision base human pose estimation non-invasive technology human-computer interaction ( hci ) . direct use hand input device provide attractive interaction method , need specialized sensing equipment , exoskeleton , glove etc , camera . traditionally , hci employ various application spread area include manufacturing , surgery , entertainment industry architecture , mention . deployment vision base human pose estimation algorithm give breath innovation application . article , present novel convolutional neural network architecture , reinforce self-attention module . propose model deploy embedded system due lightweight nature 1.9 million parameter . source code qualitative result publicly available ."
"hand gesture recognition one sought technology field machine learning computer vision . unprecedented demand application one detect hand sign deaf people people use sign language communicate , thereby detect hand sign correspondingly predict next word recommend word may appropriate , follow produce word deaf people people use sign language communicate want say . article present approach develop system determine appropriate character sign show user person system . enable pattern recognition , various machine learn technique explore use cnn network reliable solution context . creation system involve several convolution layer feature capture layer layer . gathered feature image far use train model . trained model efficiently predict appropriate character response sign expose model . thereafter , predicted character use predict word accord recommendation system use case . proposed system attain prediction accuracy 91.07 % ."
"past decade , touchless interaction object draw increase attention wide range application , entertainment real-time control robot . purpose , many vision-based hand track device leap motion microsoft kinect develop . however , still need improvement successful realization sensor delicate interaction scenario . major concern low precision , reliability , robustness occlusion occur nature utilized sensor . consequently , article , adaptive multisensor fusion methodology propose hand pose estimation two leap motion . registration-based self-calibration algorithm implement find calibration matrix sensor reference frame . two separate kalman filter adopt adaptive sensor fusion palm position orientation . propose adaptive sensor fusion method verify various experiment six degree freedom space . possible see developed adaptive methodology perform stable continuous hand pose estimation real-time , even single sensor unable detect hand . result show significant improvement smoothness pose estimation without affect occlusion one sensor ."
"hand gesture recognition ( hgr ) serve fundamental way communication interaction human . hgr apply human computer interaction ( hci ) facilitate user interaction , also utilize bridge language barrier . instance , hgr utilize recognize sign language , visual language represent hand gesture use deaf mute world primary way communication . hand-crafted approach vision-based hgr typically involve multiple stage specialized processing , hand-crafted feature extraction method , usually design deal particular challenge specifically . hence , effectiveness system ability deal varied challenge across multiple datasets heavily reliant method utilized . contrast , deep learn approach convolutional neural network ( cnn ) , adapt varied challenge via supervised learning . however , attain satisfactory generalization unseen data dependent architecture cnn , also dependent quantity variety training data . therefore , customized network architecture dub enhanced densely connect convolutional neural network ( edensenet ) propose vision-based hand gesture recognition . modified transition layer edensenet strengthen feature propagation , utilize bottleneck layer propagate feature reuse feature map bottleneck manner , following conv layer smooth unwanted feature . difference edensenet densenet discern , performance gain scrutinize ablation study . furthermore , numerous data augmentation technique utilized attenuate effect data scarcity , increase quantity training data , enrich variety improve generalization . experiment carry multiple datasets , namely one nu hand gesture dataset two american sign language ( asl ) datasets . propose edensenet obtain 98.50 % average accuracy without augment data , 99.64 % average accuracy augmented data , outperform deep learn driven instance setting , without augment data ."
"hand gesture recognition popular topic computer vision make human-computer interaction flexible convenient . representation hand gesture critical recognition . paper , propose new method measure similarity hand gesture exploit hand gesture recognition . depth map hand gesture capture via kinect sensor use method , 3d hand shape segment cluttered background . extract pattern salient 3d shape feature , propose new descriptor-3d shape context , 3d hand gesture representation . 3d shape context information 3d point obtain multiple scale local shape context global shape distribution necessary recognition . description 3d point construct hand gesture representation , hand gesture recognition explore via dynamic time warp algorithm . extensive experiment conduct multiple benchmark datasets . experimental result verify propose method robust noise , articulate variation , rigid transformation . method outperform state-of-the-art method comparison accuracy efficiency ."
"paper , comparative experimental assessment computer vision-based method sign language recognition conduct . implement recent deep neural network method field , thorough evaluation multiple publicly available datasets perform . aim present study provide insight sign language recognition , focus map non-segmented video stream gloss . task , two new sequence training criterion , know field speech scene text recognition , introduce . furthermore , plethora pretraining scheme thoroughly discuss . finally , new rgb+d dataset greek sign language create . best knowledge , first sign language dataset three annotation level provide ( individual gloss , sentence spoken language ) set video capture ."
"due high population hear impaired vocal disabled people india , sign language interpretation system become highly important minimize isolation society . paper propose signer independent novel vision-based gesture recognition system capable recognize single hand static dynamic gesture , double-handed static gesture finger spell word indian sign language ( isl ) live video . use zernike moment key frame extraction reduce computation speed large extent . also propose improved method co-articulation elimination fingerspell alphabet . gesture recognition module comprise mainly three step preprocessing , feature extraction , classification . preprocessing phase , sign extract real-time video use skin color segmentation . appropriate feature vector extract gesture sequence co-articulation elimination phase . obtained feature use classification use support vector machine ( svm ) . system successfully recognize finger spell alphabet 91 % accuracy single-handed dynamic word 89 % accuracy . experimental result show system good recognition rate compare exist method . ( c ) 2019 author . production hosting elsevier b.v. behalf king saud university ."
"paper present event aggregation strategy convert output event camera frame processable traditional computer vision algorithm . propose method first generate sequence intermediate binary representation , losslessly transform compact format simply apply binary-to-decimal conversion . strategy allow us encode temporal information directly pixel value , interpret deep learning model . apply strategy , call temporal binary representation , task gesture recognition , obtain state art result popular dvs128 gesture dataset . underline effectiveness propose method compare exist one , also collect extension dataset challenging condition perform experiment ."
"consider workspace restriction importance maintain sterility operating room , presence contact-less user interface paramount importance order establish efficient interaction surgeon surgical robot . paper , vision-based contact-less user interface name leap motion controller study able track position , velocity , orientation surgeon 's hand detect gesture movement finger , transmit data computer . afterward , surgical robot arm control use lm 's receive data classify programming . addition mentioned capability lm , characteristic low price , acceptable accuracy ( 0.1 mm ) , highrate data processing ( 100 fps ) make device utilizable efficient . paper , displacement hand 's palm center set criterion displacement 5-dof surgical robot simulator . moreover , angle form thumb index finger use command open close laparoscopic grasper . conduct robotic experiment control robot follow hand path use lm , produce error maximum percentage 6.19 % , effect system input noise minimize implementation kalman filter ."
"communication gap deaf public concern parent government bhutan . deaf school urge people learn bhutanese sign language ( bsl ) learn sign language ( sl ) difficult . paper present bsl digits recognition system use convolutional neural network ( cnn ) first-ever bsl dataset 20,000 sign image 10 static digit collect different volunteer . different sl model evaluate compare propose cnn model . proposed system achieve 97.62 % training accuracy . system also evaluate precision , recall , f1-score . ( c ) 2021 korean institute communication information science ( kics ) . publishing service elsevier b.v ."
"sign recognition difficult task due complexity composition use sign different level , word , facial expression , body posture finger-spelling convey meaning . development recent technology , kinect sensor , new opportunity emerge field human computer interaction sign language , allow capture rgb depth ( rgb-d ) information . regard feature extraction , traditional method process rgb depth image independently . paper , propose robust static finger-spelling sign language recognition system adopt quaternion algebra provide robust holistical representation , base fuse rgb image depth information simultaneously . indeed , propose , first time , new set quaternion krawtchouk moment ( qkms ) explicit quaternion krawtchouk moment invariant ( eqkmis ) . proposed system evaluate three well-known finger-spelling datasets , demonstrate performance novel method compare method use literature , geometrical distortion , noisy condition complex background , indicate could highly effective many computer vision application ."
"hand pose estimation 3d space single rgb image highly challenging problem due self-geometric ambiguity , diverse texture , viewpoint , self-occlusions . exist work prove network structure multi-scale resolution subnets , fuse parallel effectively show spatial accuracy 2d pose estimation . nevertheless , feature extract traditional convolutional neural network efficiently express unique topological structure hand key point base discrete correlated property . application hand pose estimation base traditional convolutional neural network demonstrate structural similarity graph hand key point improve accuracy 3d hand pose regression . paper , design implement end-to-end network predict 3d hand pose single rgb image . first extract multiple feature map different resolution make parallel feature fusion , model graph-based convolutional neural network module predict initial 3d hand key point . next , use 2d spatial relationship 3d geometric knowledge build self-supervised module eliminate domain gap 2d 3d space . finally , final 3d hand pose calculate average 3d hand pose gcn output self-supervised module output . evaluate propose method two challenge benchmark datasets 3d hand pose estimation . experimental result show effectiveness propose method achieve state-of-the-art performance benchmark datasets ."
"robust detection hand image different scale , especially , small-sized hand , remain challenge computer vision . work , design multi-scale deep learning algorithm detect hand unconstrained scenario well frame drive video . carefully craft deep learning model achieve improvement detection accuracy several widely use benchmark datasets . show set shallow parallel faster-rcnns lead high accuracy one deep faster-rcnn since deep layer cause loss fine feature due large stride . achieve 77.1 % , 86.53 % , 91.43 % , 74.43 % average precision oxford hand , viva , cvrr icd datasets , respectively . furthermore , propose approach detect hand small 15x15 pixel , possible previous work . analysis show different context module ( human skin ) benefit detection result reduce false positive . purpose , several approach segmentation use dilate convolution adversarial learning propose isolate skin region faster accurately . skin detection accuracy obtain use propose algorithm ibtd , pratheepan , uchile hrg datasets 94.52 % , 96.49 % , 90.74 % , 98.86 % , respectively ."
"one factor hinder progress area sign language recognition , translation , production absence large annotated datasets . towards end , introduce how2sign , multimodal multiview continuous american sign language ( asl ) dataset , consist parallel corpus 80 hour sign language video set correspond modality include speech , english transcript , depth . three-hour subset far record panoptic studio enable detailed 3d pose estimation . evaluate potential how2sign real-world impact , conduct study asl signer show synthesize video use dataset indeed understood . study give insight challenge computer vision address order make progress field ."
"purpose paper investigate effect training state-of-the-art convolution neural network ( cnn ) millimeter-wave radar-based hand gesture recognition ( mr-hgr ) . focus small training dataset problem mr-hgr , paper first propose transfer knowledge cnn model computer vision mr-hgr fine-tune model radar data sample . meanwhile , different data modality mr-hgr , parameterized representation temporal space-velocity ( tsv ) spectrogram propose integrate data modality time-evolving hand gesture feature radar echo signal . tsv spectrogram represent six common gesture human-computer interaction ( hci ) nine volunteer use data sample experiment . evaluated model include resnet 50 , 101 , 152 layer , densenet 121 , 161 169 layer , well light-weight mobilenet v2 shufflenet v2 , mostly propose many late publication . experiment , self-testing ( st ) , also persuasive cross-testing ( ct ) , implement evaluate whether fine-tuned model generalize radar data sample . ct result show best fine-tuned model reach average accuracy high 93 % comparable st average accuracy almost 100 % . moreover , order alleviate problem cause private gesture habit , auxiliary test perform augment four shot gesture heavy misclassifications training set . enrich test similar scenario tablet react new user . result two different volunteer enrich test show average accuracy enriched gesture improve 55.59 % 65.58 % 90.66 % 95.95 % respectively . compare baseline work mr-hgr , investigation paper beneficial promote mr-hgr future industry application consumer electronic design ."
"recent year , gesture recognition video sequence arouse grow interest field computer vision behavioral understanding , example control robot video game , field video surveillance , automatic video indexing content-based video retrieval . process large-scale continuous gesture data in-depth , grayscale input video remain primary challenge academic researcher . wide range recognition model propose solve problem prove great performance . main contribution article address problem segment sequence continuous gesture isolated gesture , use average velocity information calculate basis estimate deep optical flow , extract set relevant descriptor , call characteristic . signature , order characterize different intensity spatial information describe location , speed orientation movement . finally , transmit linear svm characteristic build depth gray scale sequence , isolated segment classification . experimental study carry various standard data collection namely kth , chalearn weizmann , model main model study literature , well analysis result , obtain , clearly show limit study model confirm performance model well efficiency term precision , recall robustness ."
"construction field , common worker rely hand signal communicate express thought due simple effective nature . however , meaning hand signal always capture precisely . result , construction error even accident produce . paper present feasibility study investigate whether hand signal could capture interpret automatically computer vision technology . start literature review exist hand gesture recognition method sign language understanding , human-computer interaction , etc . follow create dataset contain 11 class hand signal construction . performance two state-of-the-art 3d convolutional neural network measure compare . result indicate high classification accuracy ( 93.3 % ) short inference time ( 0.17 s/gesture ) could achieve , illustrate feasibility use computer vision automate hand signal recognition construction ."
"paper , special attention give hand . literature provide solution allow hand gesture recognition and/or object recognition virtual reality , robotic application , . solution rely mainly computer vision data glove . finding , decide develop data glove prototype . data glove exploit recognize common object kitchen person hold ( e.g . , hold fork ) he/she performs basic daily activity drink glass water . propose approach straightforward , cheap ( similar 260 $ usd ) efficient ( similar 100 % ) . moreover , design data glove give easy direct access raw data provide sensor . besides , comparison classical machine learn algorithm ( e.g . , cart , random forest ) deep neural network give . finally , propose prototype describe way researcher reproduce application involve object recognition hand . ( c ) 2021 elsevier b.v. right reserve ."
"head-mounted device-based human-computer interaction often require egocentric recognition hand gesture fingertip detection . paper , unified approach egocentric hand gesture recognition fingertip detection introduce . propose algorithm use single convolutional neural network predict probability finger class position fingertip one forward propagation . instead directly regress position fingertip fully connect layer , ensemble posi-tion fingertip regress fully convolutional network . subsequently , ensemble average take regress final position fingertip . since whole pipeline use single network , sig-nificantly fast computation . experimental result show propose method outperform ex -isting fingertip detection approach include direct regression heatmap-based framework . effectiveness propose method also show in-the-wild scenario well use-case virtual reality . ( c ) 2021 elsevier ltd. right reserve ."
"gesture recognition important direction computer vision research . information hand crucial task . however , current method consistently achieve attention hand region base estimate keypoints , significantly increase time complexity , may lose position information hand due wrong keypoint estimation . moreover , dynamic gesture recognition , enough consider attention spatial dimension . paper propose multi-scale attention 3d convolutional network gesture recognition , fusion multimodal data . propose network achieve attention mechanism locally globally . local attention leverage hand information extract hand detector focus hand region , reduce interference gesture-irrelevant factor . global attention achieve human-posture context channel context dual spatiotemporal attention module . furthermore , make full use difference different modality data , design multimodal fusion scheme fuse feature rgb depth data . propose method evaluate use chalearn lap isolated gesture dataset briareo dataset . experiment two datasets prove effectiveness network show outperforms many state-of-the-art method ."
"paper review sign language research vision-based hand gesture recognition system 2014 2020. objective identify progress need attention . extract total 98 article well-known online database use select keywords . review show vision-based hand gesture recognition research active field research , many study conduct , result dozen article publish annually journal conference proceeding . article focus three critical aspect vision-based hand gesture recognition system , namely : data acquisition , data environment , hand gesture representation . also review performance vision-based hand gesture recognition system term recognition accuracy . signer dependent , recognition accuracy range 69 % 98 % , average 88.8 % among select study . hand , signer independent 's recognition accuracy report selected study range 48 % 97 % , average recognition accuracy 78.2 % . lack progress continuous gesture recognition could indicate work need towards practical vision-based gesture recognition system ."
"automatic sign language recognition lie intersection natural language processing ( nlp ) computer vision . highly successful transformer architecture , base multi-head attention , originate field nlp . video transformer network ( vtn ) adaptation concept task require video understanding , e.g . , action recognition . however , due limited amount label data commonly available train automatic sign ( language ) recognition , vtn reach full potential domain . work , reduce impact data limitation automatically pre-extracting useful information sign language video . approach , different type information offer vtn multi-modal setup . include per-frame human pose keypoints ( extract openpose ) capture body movement hand crop capture ( evolution ) hand shape . evaluate method recently release autsl dataset isolated sign recognition obtain 92.92 % accuracy test set use rgb data . comparison : vtn architecture without hand crop pose flow achieve 82 % accuracy . qualitative inspection model hint potential multi-modal multi-head attention sign language recognition context ."
"hand gesture recognition system massive application mainly utilized robotics computer vision specially control unmanned aerial vehicle ( uav ) . method bypass presence electronic control uavs provide ease operator . paper , present method 3d hand gesture segmentation classification combine mask-rcnn grass hopper optimization . create private 3d rgb hand gesture dataset use intel kinetic intel real sense d435i camera , propose model rgb hand gesture estimate key point use human kinematics , key point later utilize get best degree freedom ( dof ) . grass hopper optimization besides minimum distance function apply achieve fine deep feature 3d hand gesture dataset . resnet50 network use backbone calculate overlap coefficient ( oc ) segmentation resnet50 , resnet101 network calculate classification 3d hand gesture . classification accuracy achieve private dataset 99.05 % 99.29 % public microsoft kinect leap motion dataset oc 88.16 % . 88.19 % respectively ."
"one challenge computer vision model , especially sign language , real-time recognition . work , present simple yet low-complex efficient model , comprise single shot detector , 2d convolutional neural network , singular value decomposition ( svd ) , long short term memory , real-time isolated hand sign language recognition ( ihslr ) rgb video . employ svd method efficient , compact , discriminative feature extractor estimate 3d hand keypoints coordinator . despite previous work employ estimated 3d hand keypoints coordinate raw feature , propose novel revolutionary way apply svd estimate 3d hand keypoints coordinate get discriminative feature . svd method also apply geometric relation consecutive segment finger hand also angle section . perform detailed analysis recognition time accuracy . one contribution first time svd method apply hand pose parameter . result four datasets , rks-persiansign ( 99.5 +/- 0.04 ) , first-person ( 91 +/- 0.06 ) , asvid ( 93 +/- 0.05 ) , isogd ( 86.1 +/- 0.04 ) , confirm efficiency method accuracy ( mean + std ) time recognition . furthermore , model outperforms get competitive result state-of-the-art alternative ihslr hand action recognition ."
"virtual glove ( vg ) low-cost computer vision system utilize two orthogonal leap motion sensor provide detailed 4d hand track real-time . vg find many application field human-system interaction , remote control machine tele-rehabilitation . innovative efficient data-integration strategy , base velocity calculation , select data one leap time , propose vg . position joint hand model , obscure leap , guess tend flicker . since vg us two leap sensor , two spatial representation available moment joint : method consist selection one low velocity time instant . choose smoother trajectory lead vg stabilization precision optimization , reduces occlusion ( part hand handling object obscure hand part ) and/or , sensor see joint , reduce number outlier produce hardware instability . strategy experimentally evaluate , term reduction outlier respect previously use data selection strategy vg , result report discuss . future , objective test set imagine , design , realize , also help external precise positioning equipment , allow also quantitative objective evaluation gain precision , maybe , intrinsic limitation propose strategy . moreover , advanced artificial intelligence-based ( ai-based ) real-time data integration strategy , specific vg , design test resulting dataset ."
"current state-of-the-art hand gesture recognition methodology heavily rely use machine learning . however scenario machine learning apply successfully , example situation data scarce . case one-to-one matching require query dataset hand gesture gesture represent unique class . situation learn algorithm train , classic computer vision technique feature extraction use identify similarity object . shape one important feature extract image , however accurate shape match algorithm tend computationally inefficient real-time application . work present novel shape match methodology real-time hand gesture recognition . extensive experiment carry compare method shape match method respect accuracy computational complexity . method outperform method provide good combination accuracy computational efficiency real-time application ."
"recognition human gesture crucial improve quality human-robot cooperation . article present system compose pepper robot mount rgb-d camera inertial device call senshand . system acquire data twenty people perform five daily living activity ( i.e . , lunch , personal hygiene , work , house cleaning , relax ) . activity compose least two basic gesture total 10 gesture . data acquisition perform two camera position laterally frontally mimic real condition . acquire data off-line classified consider different combination sensor evaluate sensor fusion approach improve recognition ability . specifically , article present experimental study evaluate four algorithm often use computer vision , i.e . , three classical machine learning one belonging field deep learning , namely support vector machine , random forest , k-nearest neighbor long short-term memory recurrent neural network . comparative analysis result show significant improvement accuracy fuse camera sensor data , i.e . , 0.81 whole system configuration robot frontal position respect user ( 0.79 consider index finger sensor ) equal 0.75 robot lateral position . interestingly , system perform well recognise transition gesture present one , common event real-life often neglect previous study ."
"hand gesture natural interaction method , hand gesture recognition familiar human-computer interaction . yet , variation , well complexity hand gesture self-structural characteristic , view , illumination , make hand gesture recognition challenging task . nowadays , human-computer interaction area enhancement lead put interest dynamic hand gesture segmentation base gesture recognition system . apart lengthy clinical success , dynamic hand gesture segmentation webcam vision seem challenge due light effect , partial occlusion , complicate environment . hence , segment entire hand gesture region enhance segmentation accuracy , paper develop improved segmentation deep learning-based strategy dynamic hand gesture recognition . data gather isl benchmark dataset consist static well dynamic image . initial process propose model pre-processing , perform grey scale conversion histogram equalization . far , segmentation gesture novel adaptive hough transform ( aht ) , theta angle tune . segmentation gesture , optimized deep convolutional neural network ( deep cnn ) use gesture recognition . learning rate , epoch count , hidden neuron tune heuristic concept . main contribution , segmentation classification enhance hybridization electric fish optimization ( efo ) , whale optimization algorithm ( woa ) call electric fish-based whale optimization algorithm ( e-woa ) . training optimized deep cnn handle dynamic time warp ( dtw ) avoid redundant frame , thus enhance performance dynamic hand gesture . quantitative measurement accomplish evaluate hand gesture segmentation recognition , portray superior behaviour propose model ."
"gesture understanding one challenging problem computer vision . among , traffic hand signal recognition require consideration speed validity commanding signal . lack available datasets also serious problem . classifier approach problem use skeleton target actor image . extract three-dimensional coordinate skeleton simplify depth information accompany image . however , depth camera cost significantly rgb camera . furthermore , extraction skeleton need perform prior . , show hand signal detection algorithm without skeleton . instead skeleton , use simple object detector train acquire hand direction . variance time length gesture mixed random pause noise handle recurrent neural network ( rnn ) . furthermore , develop flag sequence algorithm assess validity commanding signal . whole , computed hand direction send rnn , identify six type hand signal give traffic controller ability distinguish time variation intermittent randomly appear noise . construct hand signal dataset compose 100 thousand rgb image make publicly available . achieve correct recognition hand signal various background 91 % accuracy . process speed 30 fps fhd video stream , 52 % improvement best among previous work , achieve . despite extra burden decide validity hand signal , method surpass method solely use rgb video stream . work capable perform nonstationary viewpoint , take move vehicle . accomplish goal , set high priority speed validity assessment recognize commanding signal . collected dataset make publicly available korean government portal url data.go.kr/data/15075814/filedata.do ."
"hand gesture recognition significant challenge building block different computer vision application control , conversational , manipulative , communicative gesture . several system suggest address hand gesture recognition classification challenge . convolutional neural network ( cnns ) widely use different pattern recognition problem . besides cnns , feature extract use moment-based approach consider effective transparent feature task image recognition classification . however , moment-based approach consider global image feature neglect discriminative property local image feature . paper propose new efficient gesture recognition approach combine cnn feature conventional zernike moment-based feature . two group zernike moment-based feature extract since global zernike moment-based feature sufficient distinguish similar hand posture . hence , global feature supplement local modify zernike moment-based feature improve recognition accuracy extract local pattern information image . furthermore , introduce improved architecture combine feature derive whitening transform zernike moment compute image cnns ' last convolutional layer . finally , library support vector machine ( libsvm ) use classification . propose model recognition accuracy 98.41 % , 94.33 % , 97.27 % , 99.84 % four different standard datasets . performance comparison show propose model well state-of-the-art method ."
"application human-robot interaction recently raise many research interest , hand gesture recognition recognize human gesture video-based problem one . recent decade , deep learning technique prove promising performance field pattern recognition computer vision . study present improved version convolutional neural network combination long short-term memory hand gesture recognition . proposed structure fully consider time-distributed framework effectively train network frame-level classification . hence , employ time-distributed framework , td-cnn-lstm method develop . finally , efficacy propose architecture evaluate recent publicly available grit corpus dataset , also show method outperform recent state art cnn-lstm method ."
"gesture recognition interpretation system broadly implement deep learning architecture facilitate human system interaction , include communication sign language . paper deal automatic interpretation american sign language ( asl ) gesture use computer vision image processing . hybrid squeezenet multi-lane capsnet architecture introduce feature learn squeezenet convolutional architecture relative pose estimation feature carry capsule multiple lane . multi-lane capsnet setup ensure parallel identification distinct feature map use dynamic routing algorithm . transfer learning implement squeezenet model weight pre-training imagenet dataset . ensure optimized initialization weight reduced time convergence training . hybrid architecture train benchmark dataset contain hand gesture pertain english alphabet asl convention . squeezecapsnet model perform high accuracy 91.6 % , false positive rate 0.0033 degree , comparative analysis model present exist state-of-the-art asl gesture recognition model . weight squeezenet model extract present grad-cam-he visualization gesture interpretation system ."
"independent sign language recognition complex visual recognition problem combine several challenge task computer vision due necessity exploit fuse information hand gesture , body feature facial expression . many state-of-the-art work manage deeply elaborate feature independently , best knowledge , work adequately combine three information channel efficiently recognize sign language . work , employ smpl-x , contemporary parametric model enable joint extraction 3d body shape , face hand information single image . use holistic 3d reconstruction slr , demonstrate lead high accuracy recognition raw rgb image optical flow feed state-of-the-art i3d-type network 3d action recognition 2d openpose skeleton feed recurrent neural network . finally , set experiment body , face hand feature show neglect , significantly reduce classification accuracy , prove importance jointly model body shape , facial expression hand pose sign language recognition ."
"recent year , demand gesture recognition increase enormously due many application computer game , human-robot interaction , assistance system , sport , sign language interpreter , e-commerce . recognition hand gesture one important gesture recognition method . simple hand gesture , device smart home area ( tv , radio , vacuum clean robot , etc . ) easy operate . method base convolutional neural network , precisely mobilenetv2 . lean fast network , able achieve accuracy 99.96 percent recognition hand gesture , future , able offer application field human-computer interaction interact easily everincreasing number technology everyday life ."
"recognize people face biometrics extensively study computer vision . technique work identify wearer egocentric ( first-person ) camera person rarely ( ever ) appear first-person view . one 's face frequently visible , hand : fact , hand among common object one 's field view . thus natural ask whether appearance motion pattern people 's hand distinctive enough recognize . paper , systematically study possibility egocentric hand identification ( ehi ) unconstrained egocentric hand gesture . explore several different visual cue , include color , shape , skin texture , depth map identify user ' hand . extensive ablation experiment conduct analyze property hand distinctive . finally , show ehi improve generalization task , gesture recognition , train adversarially encourage model ignore difference user ."
"sign language recognition ( slr ) relatively popular research area yet contrary popularity , implementation slr daily basis rare ; due complexity various resource require . literature review , author analyze various technique use implement automated sign-language translator analysis methodology model use make working model sign-language translator various source . purpose study explore various possible way implement artificial intelligence technology improve automated american sign language translator applicable . author identify 22 different research paper within period year 2015 - 2020. analysis show every research study pick achieve respectable result , however , perfect , since research demonstrate unique strength weakness . method might suitable need create applicable sign language translator , use standard video camera obtain data , either convolutional neural network support vector machine use classification . ( c ) 2021 author . publish elsevier b.v ."
"recent emerge technology ar/vr hci draw high demand comprehensive hand shape understanding , require 3d hand skeleton pose also hand shape geometry . paper , propose deep learning framework produce 3d hand shape single depth image . address challenge capture ground truth 3d hand shape training dataset non-trivial , leverage synthetic data construct statistical hand shape model adopt weak supervision widely accessible hand skeleton pose annotation . bridge gap due different hand skeleton definition exist public datasets , propose joint regression network hand pose adaptation . reconstruct hand shape , use chamfer loss predicted hand shape point cloud input depth learn shape reconstruction model weakly-supervised manner . experiment demonstrate model adapts well real data produce accurate hand shape outperform state-of-the-art method qualitatively quantitatively ."
"work address challenge problem unconstrained 3d hand pose estimation use monocular rgb image . exist approach assume prior knowledge hand ( hand location side information ) available 3d hand pose estimation . restrict use unconstrained environment . therefore , present end-to-end framework robustly predict hand prior information accurately infers 3d hand pose learn convnet model use keypoint annotation . enhance hand detector 's robustness , propose novel keypoint-based method simultaneously predict hand region side label , unlike exist method suffer background color confusion cause use segmentation detection-based technology . moreover , inspire human hand 's biological structure , introduce two geometric constraint directly 3d coordinate prediction improve performance . experimental result show propose framework outperform state-of-art method standard benchmark datasets provide robust prediction . crown copyright ( c ) 2021 publish elsevier ltd. right reserve ."
"three dimensional ( 3d ) hand pose estimation task estimate 3d location hand keypoints . recent year , task receive much research attention due diverse application human-computer interaction virtual reality . best knowledge , limited study model self-attention 3d hand pose estimation despite use various computer vision task . hence , propose augment convolution self-attention capture long-range dependency depth image . addition , motivate recent work use anchor point set depth image , extend anchor point depth dimension regress 3d hand joint location . validation experiment use propose approach perform various hand pose datasets , obtain performance comparable state-of-the-art method . result demonstrate potential approach hand-based recognition system ."
"object classification recognition important research area widely use computer vision machine learning . use deep learn method field object recognition , important development recent year . object recognition sub-branches face recognition , motion recognition , hand gesture recognition use effectively device use daily life . hand sign classification recognition area researcher work try develop human-computer interaction . study , hybrid model create use capsule network algorithm convolutional neural network object classification . dataset , name hg14 , contain 14 different hand gesture create . measure success propose model object recognition , training carry hg14 , fashionmnist , cifar-10 datasets . also , vgg16 , resnet50 , densenet , capsnet model use classify image hg14 , fashionmnist , cifar-10 datasets . result training compare evaluate . propose hybrid model achieve high accuracy rate 90 % hg14 dataset , 93.88 % fashionmnist dataset , 81.42 % cifar-10 dataset . proposed model find successful study compare model ."
"neural sign language translation ( slt ) , important cross-modal task bridge communication gap deaf hearing people , attract great attention field artificial intelligence , computer vision , multimedia , etc . although great progress achieve recently , current neural slt model still suffer translation error cause under-consideration non-manual feature facial expression , carry critical information communication among deaf . paper aim enhance traditional neural slt model highlight facial expression information cnn-based sign video represent part . two novel scheme propose . first scheme base multi-stream architecture , extract represent facial expression information additional stream aggregate information main stream . second scheme pre-trained scheme base region interest ( roi ) , first train multi-region detection module recognize feature face body transfer pre-trained parameter module slt model . validate propose model , conduct experiment upon publicly available slt benchmark dataset : rwth-phoenix-weather-2014t . experimental result show above-mentioned scheme improve performance slt model . especially , rois-based scheme achieve improve-ment 1.6+ bleu-4 score gain , multi-stream scheme quantitatively analyze impor-tance face mainly flexible component , provide sufficient theoretical basis rois-based scheme . ( c ) 2021 elsevier b.v. right reserve ."
"product life-cycles get shorter limited availability natural resource , paradigm shift towards circular economy impulsed . domain , successful adoption remanufacturing key . however , associated process efficiency date limit give high flexibility requirement product disassembly . emergence industry 4.0 , natural human-robot interaction expect provide numerous benefit term ( ) manufacturing efficiency cost . regard , vision-based wearable-based approach extended come establish gesture-based interaction interface . work , experimental comparison two different movement-estimation systems- ( ) position data collect microsoft kinect rgb-d camera ( ii ) acceleration data collect inertial measurement unit ( imu ) -is address . result point imu-based proposal , operable , recognition accuracy rate 8.5 time high microsoft kinect , prove dependent movement 's execution plane , subject 's posture , focal distance ."
"combination gesture recognition aerospace exploration robot realize efficient non-contact control robot . harsh aerospace environment , captured gesture image usually blur damage inevitably . motion blur image cause part transmitted information lose , also affect effect neural network training late stage . improve speed accuracy motion blur gesture recognition , algorithm yolov4 ( look , vision 4 ) study two aspect motion blur image processing model optimization . deblurganv2 employ remove motion blur gesture yolov4 network input picture . term model structure , k-means++ algorithm use cluster priori box obtain appropriate size parameter priori box . cbam attention mechanism spp ( spatial pyramid pool layer ) structure add yolov4 model improve efficiency network learning . dataset network training design human-computer interaction aerospace space . reduce redundant feature captured image enhance effect model training , wiener filter bilateral filter superimpose blurred image dataset simply remove motion blur . augmentation model execute imitate different environment . yolov4-gesture model build , collaborate k-means++ algorithm , cbam spp mechanism . deblurganv2 model build process input image yolov4 target recognition . yolov4-motion-blur-gesture model compose yolov4-gesture deblurganv2 . augmented enhanced gesture data set use simulate model training . experimental result demonstrate yolov4-motion-blur-gesture model relatively good performance . propose model high inclusiveness accuracy recognition effect real-time interaction motion blur gesture , improve network train speed 30 % , target detection accuracy 10 % , value map 10 % . constructed yolov4-motion-blur-gesture model stable performance . meet real-time human-computer interaction aerospace space real-time complex condition , also apply application environment complex background require real-time detection ."
"natural gesticulation in-air multifaceted , define gesture 's pattern/size/speed stroke information difficult . unavailability vision-based technique distinguish ground truth stroke segment intentional movement ( self co-articulation ) result capture everything trajectory . recognize gesture pattern ( a-z , a-z , 0-9 , 04 operator , 29 symbol ) along self co-articulations result rise misclassification . hence , gesture separate 3 set base physical structure use artificial neural network . , set specific pre-processing model propose remove self co-articulations . result , mean error rate 0.0112 ( ground truth segment remove ) 5.63 % self co-articulations present gesture pattern obtain . relative improvement 22 % ( accuracy - 94.17 % ) exist model achieve . , gesture cluster two group mamdani fuzzy interference system use prior stroke information . use transfer learning , separate pre-trained alexnet model utilize recognize gesture pattern fall group average accuracy 97.05 % ( precision - 0.9680 , recall - 0.9656 ; f-score - 0.9668 ) . relative improvement similar 17 % exist state-of-the-art model gesture recognition . ( c ) 2021 elsevier b.v. right reserve ."
"hearing-impaired people use sign language express thought emotion reinforce information deliver daily conversation . though make significant percentage population , majority people ca n't interact due limited knowledge sign language . sign language recognition aim detect significant motion human body , especially hand , analyze understand . system may become life-saving hearing-challenged people desperate situation like heart attack , accident , etc . present study , deep learning-based hand gesture recognition model develop accurately predict emergency sign indian sign language ( isl ) . dataset use contain video eight different emergency situation . several frame extract video feed three different model . two model design classification , one object detection model , apply annotate frame . first model consist three-dimensional convolutional neural network ( 3d cnn ) , second comprises pre-trained vgg-16 recurrent neural network long short-term memory ( rnn-lstm ) scheme . last model base yolo ( look ) v5 , advanced object detection algorithm . prediction accuracy classification model 82 % 98 % , respectively . yolo base model outperform rest achieve impressive mean average precision 99.6 % ."
"sign language critical convey meaning use visual-manual modality primary mean communication deaf hard hear family member society . advance computer graphic , computer vision , neural network , introduction new powerful hardware , research sign language show new potential . novel technology help people learn , communicate , interpret , translate , visualize , document , develop various sign language related skill . paper review technological advancement apply sign language recognition , visualization , synthesis . define multiple research question identify underlying technological driver strive improve challenge domain . study design accordance prisma methodology . search article publish 2010 2021 multiple digital library ( i.e . , elsevier , springer , ieee , pubmed , mdpi ) . automate initial step prisma identify potentially relevant article , duplicate removal basic screening , utilize natural language processing toolkit . , perform synthesis exist body knowledge identify different study achieve significant advancement sign language recognition , visualization , synthesis . identified trend base analysis almost 2000 paper clearly show technology development , especially image processing deep learning , drive new application tool improve various performance metric sign language-related task . finally , identify technique device contribute result common thread gap would open new research direction field ."
"3d hand pose estimation single depth image essential topic computer vision human-computer interaction . although rising deep learning boost accuracy lot , problem still hard solve due complex structure human hand . two exist type method deep learning , i.e . regression-based detection-based method , either lose spatial information hand structure lack direct supervision joint coordinate . paper , propose novel differentiable spatial regression method combine advantage two type method overcome 's shortcoming . method use spatial-form representation ( sfr ) maintain spatial information differentiable decoder establish direct supervision . follow procedure suggest method , particular model name srnet design use combination 2d heatmaps local offset map sfrs . two module name plane regression depth regression design differentiable decoder regress plane coordinate depth coordinate respectively . ablation study demonstrate superiority method two combine method since differentiable decoder lead well sfrs learn network human design . extensive experiment four public datasets demonstrate srnet comparable state-of-the-art model ."
"computer vision system commonly use design touch-less human-computer interface ( hci ) base dynamic hand gesture recognition ( hgr ) system , wide range application several domain , , game , multimedia , automotive , home automation . however , automatic hgr still challenging task , mostly diversity people perform gesture . addition , number publicly available hand gesture datasets scarce , often gesture acquire sufficient image quality , gesture correctly perform . data article , propose dataset 27 dynamic hand gesture type acquire full hd resolution 21 different subject , carefully instruct perform gesture monitor perform gesture ; subject repeat movement case perform hand gesture correct , i.e . , author paper observe gesture find correspond exact expect movement and/or camera record viewpoint allow plain visualizing gesture . subject perform 3 time 27 hand gesture total 1701 video collect correspond 204,120 video frame ."
"people hearing speaking disability face significant hurdle communication . knowledge sign language help mitigate hurdle , people without disability , include relative , friend , care provider , understand sign language . availability automated tool allow people disability around communicate ubiquitously variety situation non-signers . currently two main approach recognize sign language gesture . first hardware-based approach , involve glove hardware track hand position determine gesture . second software-based approach , video take hand gesture classify use computer vision technique . however , hardware , phone 's internal sensor device worn arm track muscle data , less accurate , wear cumbersome uncomfortable . software-based approach , hand , dependent lighting condition contrast hand background . propose hybrid approach take advantage low-cost sensory hardware combine smart sign-recognition algorithm goal develop efficient gesture recognition system . myo band-based approach use support vector machine method achieve accuracy 49 % . software-based approach use convolutional neural network ( cnn ) recurrent neural network ( rnn ) method train myo-based module achieve accuracy 80 % experiment . method combine two approach show potential improvement . experiment dataset nine gesture generate multiple video , repeat five time total 45 trial software-based hardware-based module . apart show performance approach , result show improved hardware module , accuracy combined approach significantly improve ."
"gesture recognition one popular technique field computer vision today . recent year , many algorithm gesture recognition propose , good balance recognition efficiency accuracy . therefore , propose dynamic gesture recognition algorithm balance efficiency accuracy still meaningful work . currently , commonly use dynamic gesture recognition algorithm base 3d convolutional neural network . although 3d convolutional neural network consider spatial temporal feature , network complex , main reason low efficiency algorithm . improve problem , propose recognition method base strategy combine 2d convolutional neural network feature fusion . original keyframes optical flow keyframes use represent spatial temporal feature respectively , send 2d convolutional neural network feature fusion final recognition . ensure quality extracted optical flow graph without increase complexity network , use fractional-order method extract optical flow graph , creatively combine fractional calculus deep learning . finally , use cambridge hand gesture dataset northwestern university hand gesture dataset verify effectiveness algorithm . experimental result show algorithm high accuracy ensure low network complexity ."
"recent year , several technology utilize bridge communication gap person hearing speaking impairment n't . paper present development novel sign language recognition system translate ethiopian sign language ( ethsl ) amharic alphabet use computer vision technology deep convolutional neural network ( cnn ) . system accept sign language image input give amharic text desired output . proposed system comprises three main stage : preprocessing , feature extraction , recognition . methodology employ involve data acquisition , preprocessing acquire data , background normalization , image resizing , region interest ( roi ) identification , noise removal , brightness adjustment , feature extraction , deep convolutional neural network ( cnn ) use end-to-end classification . data use study acquire student hear impairment debre markos teach college iphone 6s phone resolution 3024 x 4020. image jpeg file format collect controlled environment . proposed system implement use kera 's ( tensorflow2.3.0 backend ) python test use image dataset collect debre markos teach college graduate student 2012. result show running time minimize adjust image suitable size color . addition , result show improved recognition accuracy compare previous work . propose model achieve 98.5 % training , 95.59 % validation , 98.3 % test accuracy recognition ."
"behavior recognition video sequence interesting task computer vision field . human action recognition , allow machine analyze comprehend several human activity input data source , sensor , multimedia content . widely apply public security , video surveillance , robotics video game control , industrial automation field . however , application , gesture form complex temporal sequence motion . representation impose many problem variation action/gesture performer style , body shape size even way signer realize motion , various problem relate scene environment like illumination change background complexity . moreover , process large-scale continuous gesture data make problem challenge due unknown boundary limit gesture embed continuous sequence . work , present novel deep architecture isolated action large scale continuous hand gesture recognition use rgb , depth , gray-scale video sequence . propose approach , call end-to-end deep gesture recognition process ( e2e-dgrp ) , consider new representation data extract gesture characteristic . follow analysis sequence detect recognize corresponding action . firstly , segmentation module spot start end gesture boundary segment continuous gesture sequence several isolated gesture base motion indicator method . , set discriminate feature generated characterize motion location , motion velocity motion orientation use propose method name deep signature . generate feature feed additional neural network layer form softmax classifier output layer gesture classification purpose . high performance approach obtain experiment carry three well-known datasets kth weizman isolated action rgb grayscale modality chalearn lap congd dataset continuous gesture depth . first experimental study show , recognition result obtain method outperform obtain previously publish approach achieve 97.5 98.7 % term accuracy kth weizmann datasets , respectively . second experimental study , conduct use chalearn lap congd data set , show strategy outperform method term precision reach 0.6661 mean jaccard index ."
"chalearn large-scale gesture recognition challenge run twice two workshop conjunction international conference pattern recognition ( icpr ) 2016 international conference computer vision ( iccv ) 2017 , attract 200 team around world . challenge two track , focus isolated continuous gesture recognition , respectively . describe creation benchmark datasets analyze advance large-scale gesture recognition base two datasets . article , discuss challenge collect large-scale ground-truth annotation gesture recognition provide detailed analysis current method large-scale isolated continuous gesture recognition . addition recognition rate mean jaccard index ( mji ) evaluation metric use previous challenge , introduce correct segmentation rate ( csr ) metric evaluate performance temporal segmentation continuous gesture recognition . furthermore , propose bidirectional long short-term memory ( bi-lstm ) method , determine video division point base skeleton point . experiment show propose bi-lstm outperform state-of-the-art method absolute improvement 8.1 % ( 0.8917 0.9639 ) csr ."
"automatic sign language recognition challenge task machine learning computer vision . work focus recognize sign language use hand gesture . however , body motion facial gesture play essential role sign language interaction . take account , introduce automatic sign language recognition system base multiple gesture , include hand , body , face . use depth camera ( oak-d ) obtain 3d coordinate motion recurrent neural network classification . compare multiple model architecture base recurrent network long short-term memory ( lstm ) gate recurrent unit ( gru ) develop noise-robust approach . work , collect dataset 3000 sample 30 different sign mexican sign language ( msl ) containing feature coordinate face , body , hand 3d spatial coordinate . extensive evaluation ablation study , best model obtain accuracy 97 % clean test data 90 % highly noisy data ."
"gesture recognition foremost need build intelligent human-computer interaction system solve many day-to-day problem simplify human life digital world . traditional machine learning ( ml ) algorithm try capture specific handcraft feature , fail miserably real-world environment . deep learning ( dl ) technique become sensation among researcher recent year , make traditional ml approach quite obsolete . however , exist review consider datasets dl algorithm apply , categorization dl algorithm vague review . study provide precise categorization dl algorithm considers around 15 gesture datasets technique apply . study also provide brief overview numerous challenging dataset available among research community insight various challenge limitation dl algorithm vision-based dynamic gesture recognition ."
"gesture recognition study within field computer vision pattern recognition . gesture define meaningful physical movement finger , hand , arm , part body purpose convey information environment interaction . instance , hand gesture recognition ( hgr ) use recognize sign language primary mean communication deaf mute . vision-based hgr critical application ; however , challenge need overcome variation background , illumination , hand orientation size similarity among gesture . traditional machine learn approach widely use vision-based hgr recent year complexity processing major challenge-especially handcrafted feature extraction . effectiveness handcrafted feature extraction technique prove across various datasets comparison deep learning technique . therefore , hybrid network architecture dub lightweight vgg16 random forest ( lightweight vgg16-rf ) propose vision-based hand gesture recognition . propose model adopt feature extraction technique via convolutional neural network ( cnn ) use machine learn method perform classification . experiment carry publicly available datasets american sign language ( asl ) , asl digit nus hand posture dataset . experimental result demonstrate propose model , combination lightweight vgg16 random forest , outperform method ."
"3d hand pose estimation monocular rgb image highly challenging task due self-occlusion , diverse appearance , inherent depth ambiguity within monocular image . previous method first employ deep neural network fit 2d joint location map , combine implicit explicit pose-aware feature directly regress 3d hand joint position use design network structure . however , skeleton position correspond skeleton-aware content information locate latent space invariably ignore . skeleton-aware content effectively bridge gap hand joint hand skeleton information associate relationship different hand joint feature hand skeleton position distribution 2d space . address issue , propose simple yet efficient deep neural network directly recover reliable 3d hand pose monocular rgb image fast estimation process . purpose reduction model computational complexity maintain high precision performance . therefore , design novel feature chat block ( fcb ) complete feature boosting , enable intuitively enhanced interaction joint skeleton feature . first , fcb module update joint feature effectively base semantic graph convolutional neural network multi-head self-attention mechanism . gcn-based structure focus physical hand joint include binary adjacency matrix self-attention part pay attention hand joint locate complementary matrix . , fcb module employ query key mechanism respectively represent joint skeleton feature implement feature interaction . set fcb module , model update fused feature coarse-to-fine manner finally output predicted 3d hand pose . conduct comprehensive set ablation experiment interhand2.6m dataset validate effectiveness significance propose method . additionally , experimental result rendered hand dataset , stereo hand datasets , first-person hand action dataset freihand dataset show model surpass state-of-the-art method fast inference speed ."
"dynamic hand gesture recognition still interesting topic computer vision community . set feature vector represent hand gesture . recurrent neural network ( rnn ) recognize feature vector hand gesture analyze temporal contextual information gesture sequence . thus , propose hybrid deep learning framework recognize dynamic hand gesture . hybrid model googlenet pipelined bidirectional gru unit recognize dynamic hand gesture . dynamic hand gesture consist many frame , feature frame need extract get temporal dynamic information performed gesture . rnn take input sequence feature vector , extract feature video use pretrained googlenet . gated recurrent unit one variant rnn classify sequential data , create feature vector correspond video pass bidirectional gru ( bgru ) network classify gesture . evaluate model four publicly available hand gesture datasets . propose method performs well comparable exist method . instance , achieve 98.6 % accuracy northwestern university hand gesture ( nwuhg ) , 99.6 % skig , 99.4 % cambridge hand gesture ( chg ) datasets respectively . perform experiment dhg14/28 dataset achieve accuracy 97.8 % 14-gesture class 92.1 % 28-gesture class . dhg14/28 dataset contain skeleton depth data , propose model use depth data achieve comparable accuracy ."
"grow number deaf hearing-impaired population evolution vision-based application device present enormous opportunity research modelling domain gesture classification recognition . since hand gesture recognition crucial sign language interpretation , efficient gesture recognition system consider maximum sign character recognition . work , robust hand gesture recognition model propose employ deep ensemble neural network . initially , pre-trained network design adopt vgg16 architecture self-attention layer embed vgg16 architecture . self-attention module enable learn potentially distinguishing image feature good differentiability among gesture category . , weighted ensemble model introduce , use complementary information contribute base model magnify overall network performance . detailed experimental investigation individual pre-trained model weighted ensemble model conduct validate efficacy propose network . moreover , work show propose model also employ weakly supervise object segmentation . additionally , performance suggested methodology evaluate publicly available two complex hand gesture datasets obtain accuracy 99.76 % 95.10 % , relatively outperform state-of-the-art approach ."
"audio-visual speech recognition ( avsr ) one promising solution reliable speech recognition , particularly audio corrupt noise . additional visual information use automatic lip-reading gesture recognition . hand gesture form non-verbal communication use important part modern human-computer interaction system . currently , audio video modality easily accessible sensor mobile device . however , out-of-the-box solution automatic audio-visual speech gesture recognition . study introduce two deep neural network-based model architecture : one avsr one gesture recognition . main novelty regard audio-visual speech recognition lie fine-tuning strategy visual acoustic feature propose end-to-end model , consider three modality fusion approach : prediction-level , feature-level , model-level . main novelty gesture recognition lie unique set spatio-temporal feature , include consider lip articulation information . available datasets combined task , evaluate method two different large-scale corpora-lrw autsl-and outperform exist method audio-visual speech recognition gesture recognition task . achieve avsr accuracy lrw dataset equal 98.76 % gesture recognition rate autsl dataset equal 98.56 % . result obtain demonstrate high performance propose methodology , also fundamental possibility recognize audio-visual speech gesture sensor mobile device ."
"hand pose estimation , predict spatial location hand joint , fundamental task vr/ar application . although exist method recover hand pose competently , tremor issue occur hand motion completely solve . tremor involuntary motion accompany desired gesture hand motion , lead hand pose deviate user 's intention . consider characteristic tremor motion , present novel graph neural network stable 3d hand pose estimation . input depth image . constraint adjacency matrix devise graph neural network dynamically adjust topology hand graph message passing aggregation . firstly , since rich potential constraint among hand joint , utilize constraint adjacency matrix mine suitable topology , model spatial-temporal constraint joint output precise tremor hand pose pre-estimation result . , obtain stable hand pose , provide tremor compensation module base constraint adjacency matrix , exploit constraint control point tremor hand pose . concretely , control point represent voluntary motion employ constraint edit tremor hand pose . extensive quantitative qualitative experiment show propose method achieve decent performance 3d tremor hand pose estimation ."
"background : finger mobility play crucial role everyday living leading indicator hand rehabilitation assistance task . depth-based hand pose estimation potentially low-cost solution clinical home-based measurement symptom limited human finger motion . objective : purpose study achieve contactless measurement finger motion base depth-based hand pose estimation use azure kinect depth camera transfer learning , evaluate accuracy comparison three-dimensional motion analysis ( 3dma ) system . method : thirty participant perform series task hand motion measure concurrently use azure kinect 3dma system . propose simple effective approach achieve real-time hand pose estimation single depth image use ensemble convolutional neural network train transfer learn strategy . algorithm calculate finger joint motion angle present track location 24 hand joint . demonstrate potential , azure-kinect-based 3d finger motion measurement system algorithm experimentally verify comparison camera-based 3dma system , gold standard . result : result reveal azure-kinect-based hand pose estimation system produce highly correlated measurement hand joint coordinate . method achieve excellent performance term fraction good frame ( > 80 % ) error threshold large approximately 2 cm , range mean error distance 0.23 - - 1.05 cm . joint angle , azure kinect 3dma system comparable inter-trial reliability ( icc2,1 range 0.89 0.97 ) excellent concurrent validity , pearsons r-values > 0.90 measurement ( range : 0.88 - - 0.97 ) . 95 % blandaltman limit agreement narrow enough azure kinect consider valid tool measurement report joint angle index finger thumb pinch . moreover , method run real time 45 fps . conclusion : result study suggest propose method capacity measure performance fine motor skill ."
"machine translation sign language critical task computer vision . work , propose use 3d motion capture technology sign capture graph matching sign recognition . two problem relate 3d sign match address work : ( 1 ) identify sign different number motion frame ( 2 ) sign extraction clutter non-sign hand motion . two problem make 2d 3d sign language machine translation challenging task . propose graph match early estimation model address problem two phase . first phase consist intra graph match motion frame extraction , retain motion intensive frame database query 3d video . second phase applies inter graph match early estimation model motion extract query dataset 3d video . proposed model increase speed graph match algorithm estimate sign frame . test graph match model , record 350 word indian sign language 3d motion capture technology . test 4 variation per sign capture sign 5 different signer , slow , faster hand speed sign mixed cluttered hand motion . early estimation graph match model test accuracy efficiency classify 3d sign two induced real time constraint . addition 3d sign language dataset , propose method validate five benchmark datasets state-of-the-art graph match method . ( c ) 2018 author . production hosting elsevier b.v. behalf king saud university . open access article cc by-nc-nd license ( http : //creativecommons.org/licenses/by-nc-nd/4.0/ ) ."
"use gesture one main form human machine interaction ( hmi ) many field , advanced robotics industrial setup , multimedia device home . almost every gesture detection system use computer vision fundamental technology , already well-known problem image processing : change light condition , partial occlusion , variation color , among others . solve potential issue , deep learning technique prove effective . research propose hand gesture recognition system base convolutional neural network color image robust environmental variation , real time performance embedded system , solve principal problem present previous paragraph . new cnn network specifically design small architecture term number layer total number neuron use computationally limited device . obtained result achieve percentage success 96.92 % average , good score obtain previous algorithm discuss state art ."
"main objective work develop intelligent system able take hand gesture ( sign language ) , recognize interpret automatically use image processing computer vision technique . several technique available literature , however , improve accuracy exist recognition system , unique method adopt use histogram gradient base algorithm feature extraction & svm base feature classifier . see average recognition rate high 96.3 % 10 alphabet ( a-j ) 95.05 % 10 numeral ( 0-9 ) use technique . proposed system develop python-3.6 opencv 3.0 platform ."
"large diffusion low-cost computer vision ( cv ) hand track sensor use hand gesture recognition , allow development precise low cost touchless track system . main problem cv solution cope occlusion , frequent hand grasp tool , self-occlusions occur joint obscures . case occlusion solve use synchronized multiple stereo sensor . virtual glove ( vg ) one cv-based system use two orthogonal leap sensor integrate single system . vg system drive personal computer master operate system ( ) virtual machine instal order drive two sensor ( one sensor time drive single instance ) . strong limitation vg run powerful pc , thus result properly low-cost portable solution . propose vg architecture base three raspberry pi ( rp ) , consisting cheap single board computer linux . proposed architecture assign rpi leap third rp collect data two . third rp merges , real time , data single hand model make available , api , render web application inside virtual reality ( vr ) interface . detailed design propose , architecture implement experimental benchmark measurement , demonstrate rpi-based vg real-time behaviour contain cost power consumption , present discuss . proposed architecture could open way develop modular hand track system base two leap , associate one rp , order improve robustness ."
"currently , communication deaf hearing people still face great problem research level application level . also , practical automatic sign language interpretation method become relatively large demand . paper , recognize video sign language base computer vision capture skeletal key point information use multi-head attention mechanism long short-term memory mechanism . construct database chinese video sign language consist 30 word , total data volume 1200. meanwhile , experiment propose framework dataset achieve 85 % accuracy rate . experimental result show propose method characteristic high accuracy light weight problem chinese sign language recognition ."
"accurate reliable gesture recognition central problem human-computer interaction ( hci ) . many application make use gesture recognition call mobile device reduced power consumption , weight form factor . recent advance computer vision particularly bring deep neural network come cost high computational complexity hinder employment mobile device . study , evaluate usability low-cost raspberry pi 4b amend coral usb accelerator , neural compute stick 2 , respectively , low power real-time gesture recognition . end evaluate accuracy , inference time power consumption two different deep neural network-based recognition model compare result computer system available standard . experiment show combination raspberry pi 4b coral usb accelerator allow hand gesture recognition frame rate 30 frame per second power consumption less 5 watt ."
"hand emerge quite often egocentric video , structure disposition reveal lot individual engage show interest . early research hand detection , hand , depend significant effect operate limited situation , lab setting minimal social interaction . work investigate recently develop yolo architectural object recognition egocentric video , offer good balance precision performance . perform experiment assessment utilize various different architecture configuration train method . experiment lead sort conclusion cost - effective methodology strategic objective , lead us research enhance detection smart glass video . test approach exist dataset contain 48 first individual footage people interact situation real life well bounding box pixel level 15,000 hand gesture ."
"video game among popular form entertainment modern world . however , many gamers physical disability impede traditional controller . accessory foot pedal enlarged button exist , many accessible game setup end cost hundred dollar . solution problem must obtainable gamers demographic , ideally , incorporate item people already . hand gesture recognition system game come rescue . laptop computer many desktop come equip webcam , naturally , would starting point . user would able perform various hand gesture , map set button combination virtual gamepad . gesture detection would real-time , fast computer vision library opencv need process image . establish basis research , program deploy computer webcam instantly create gamepad thin air . final program feature intuitive user interface customizable game profile save load storage . program capture webcam input 60 time per second , perform multiple level processing image . use technique thresholding , gaussian blurring , grayscale conversion , ideal image feed opencv 's contour detection algorithm . calculate angle contour , number finger hold determine . gesture detect , program communicate kernel-mode driver send controller input directly game . result low latency controller real-world application . research work design enable user customization allows xinput compatible game control . allow implementation race game drive simulator , also first-person shoot side-scrolling platformers ."
"last year , increase demand develop efficient solution computer vision-related task fpga hardware due quick protolyping compute capability . therefore , work aim implement low precision binarized neural network ( bnn ) use python framework xilinx pynq-z2 embed platform tackle challenging problem sign language recognie . specifically , finn framework adopt bnn topology modify adapt large resolution ( i.e 64x64 ) perform classification proposed indian sign language ( isl ) gesture correspond number . addition , data augmentation technique also apply improve overall performance neural network . furthermore , hardware/software co-verification bnn topology perform validate accuracy implement onto hardware . extensive experimental result show achieve classification rate 843.8 frame per second ( fps ) pynq-z2 , fpga deliver high performance compare previous work . also , post-implementation result analyze term resource utilizal power consumption ."
"recent year , lot research con-ducted interpret bangladeshi sign language ( bdsl ) mean general people communicate people hearing impairment reduce verbal gap . computer vision play vital role regard develop sustainable system understand sign machine translation . obtain optimal performance , along state-of-the-art cnn model , requirement high-quality sign language dataset foreseen . paper , introduce new image dataset okkhornama fingerspelled bangladeshi sign language include 46 sign image 12k . image , bound box carefully annotate label . okkhornama contain image high resolution , good quality , adequate variation make ideal train object detection localization algorithm would perform well real-world application . okkhornama dataset compare datasets okkhornama significantly outperform datasets number trained model performance . dataset publicly available future research development ."
"many advancement computer vision machine learning show potential significantly improve life people disability . particular , recent research demonstrate deep neural network model could use bridge gap deaf use sign language hearing people . major impediment advance model lack high-quality large-scale training data . moreover , previously release sign language datasets include interrogative sentence compare declarative sentence . paper , introduce new publicly available large-scale korean sign language ( ksl ) dataset-ksl-guide-that include declarative sentence comparable interrogative sentence , require model achieve high performance real-world interactive task deploy service application . dataset contain total 121k sign language video sample feature sentence word speak native ksl speaker extensive annotation ( e.g . , gloss , translation , keypoints , timestamps ) . exploit multi-camera system produce 3d human pose keypoints well 2d keypoints multi-view rgb . experiment quantitatively demonstrate inclusion interrogative sentence training sign language recognition translation task greatly improve performance . furthermore , empirically show qualitative result develop prototype application use dataset , provide interactive guide service help lower communication barrier sign language speaker hear people ."
"sign language recognition ( slr ) witness boost recent year , particularly surge deep learning technique . however , exist method exploit concept attention mechanism , despite success several computer vision task . paper , propose novel method isolated slr utilize spatial attention focus processing informative , discriminate part input . particularly important slr , since rgb image contain several distract information background signer 's clothes , irrelevant task . investigate three way incorporate spatial attention : ) pre-focused attention , use optical-flow-based motion prior b ) learn attention , network learn focus training , c ) hybrid attention , combine approach initialize attention layer learned attention motion-based attention mask use pre-focused attention . show , first , three approach outperform state-of-the-art method one large isolated slr datasets , validate effectiveness attention mechanism slr task , second , best performing approach hybrid attention , combine idea ."
"increase smartphone 's storage capacity mean increase important data save exchange within smartphone well , thus ensure secure authentication smartphones crucial . nowadays text password physical biometrics become easy hack due predictable nature bypass brute force . hand , hand-gesture recognition require user perform hand gesture know identical . work carry gain good understanding gesture recognition authentication especially field computer-based gesture recognition specifically field hand track . still room improvement achieve good authentication smartphones , one technique employ hand- tracking gesture recognition ( ht-gr ) . hand-tracking gesture recognition ( ht-gr ) utilize vision framework apple gain good hand-gesture recognition . able perform well harsh environment , lidar sensor apply assist tracking process . significance paper overcome current limitation authentication system smartphones . technique gesture recognition work well harsh environment , dark environment . unlike current approach , lidar sensor study improve current weakness ."
"system , help convert sign language voice hand gesture understand capture motion hand . many sign language natural language , differ construction oral language use proximity , employ mainly deaf people order communicate . base raspberry pi camera module program python open-source computer vision ( open cv ) library backend . built-in image processing algorithm raspberry pi name gesture track object ( finger ) feature pull . main purpose gesture recognition system establish connection human computer control system . camera use system capture various gesture hand . various algorithm take place processing image . first preprocessing image take place . finally , sign identify use tensor flow algorithm outcome result voice use tts algorithm . open cv python implement system . various library use system ."
"inspire recent spatio-temporal convolutional neural network computer vision field , propose olt-c3d ( online long-term convolutional 3d ) , new architecture base 3d convolutional neural network ( 3d cnn ) address complex task early recognition 2d handwritten gesture real time . input signal gesture translate image sequence along time trajectory history . image sequence pass 3d cnn olt-c3d give prediction new frame . olt-c3d couple integrated temporal reject system postpone decision time information need . moreover system end-to-end trainable , olt-c3d temporal reject system jointly train optimize earliness decision . approach achieve superior performance two complementary freely available datasets : ilgdb mtgsetb ."
"even though virtual reality ( vr ) industry experience rapid growth ever-expanding demand today , vr application yet provide fully immersive experience . insufficient resolution vr head-mounted display ( hmd ) hinder user immersion virtual world . work , attempt enhance immersive experience improve perceptual resolution vr hmds . employ efficient neural-network-based approach propose temporal integration loss function . take temporal integration mechanism human visual system ( hvs ) account , network learn perception process human eye , temporally upsamples sequence turn improve perceived resolution . specifically , discuss possible scenario deploy approach vr system equip eye-tracking technology , could save 75 % computational load . compare state-of-the-art term inference time analysis user experiment , show approach run around 1.89x faster produce favorable result ."
"paper , concern problem automatically extract step compose real-life hand activity . key competence towards processing , monitoring provide video guidance mixed reality system . use egocentric vision observe hand-object interaction real-world task automatically decompose video constituent step . approach combine hand-object interaction ( hoi ) detection , object similarity measurement finite state machine ( fsm ) representation automatically edit video step . use combination convolutional neural network ( cnns ) fsm discover , edit cut merge segment observe real hand activity . evaluate quantitatively qualitatively algorithm two datasets : gtea [ 19 ] , new dataset introduce chinese tea making . result show method able segment hand-object interaction video key step segment high level precision ."
"3d hand reconstruction popular research topic recent year , great potential vr/ar application . however , due limited computational resource vr/ar equipment , reconstruction algorithm must balance accuracy efficiency make user good experience . nevertheless , current method well balance accuracy efficiency . therefore , paper propose novel framework achieve fast accurate 3d hand reconstruction . framework relies three essential module , include spatial-aware initial graph building ( saigb ) , graph convolutional network ( gcn ) base belief map regression ( gbbmr ) , pose-guided refinement ( pgr ) . first , give image feature map extract convolutional neural network , saigb build spatial-aware compact initial feature graph . node graph represent vertex mesh vertex-specific spatial information helpful accurate efficient regression . , gbbmr first utilizes adaptive-gcn introduce interaction vertex capture short-range long-range dependency vertex efficiently flexibly . , map vertex ' feature belief map model uncertainty prediction accurate prediction . finally , apply pgr compress redundant vertex ' belief map compact joint ' belief map pose guidance use joint ' belief map refine previous prediction well obtain accurate robust reconstruction result . method achieves state-of-the-art performance four public benchmark , freihand , ho-3d , rhd , stb . moreover , method run speed two three time previous state-of-the-art method . code available http : //github.com/zxz267/sar ."
"2d keypoints detection computer vision task applicable several field hand , face , body tracking , provide useful information spatial analytics , gestural interaction , augment reality application . work investigate usage depthwise separable convolution ( optimize convolution operation ) speed inference time largely used architecture 2d keypoints estimation . evaluate impact precision performance optimization hand pose estimation task . also extend evaluation towards simulate challenge scenario defocused lens , motion blur , occlusion , noisy image understand stress situation affect original optimized architecture . show execution time improve average 12.8 % accuracy compromise less 1 pixel ( mean epe ) . experiment challenge scenario reveal model power depthwise separable convolution fit occlusion case noisy environment suffer motion blur simulate scenario ."
"sign language recognition ( slr ) become increasingly popular recent year computer vision . essential extract discriminative spatiotemporal feature model spatial temporal evolution different sign . also , local gesture facial expression representation contribute distinguish sign similar motion pattern different meaning . paper , propose multi-modal sign language recognition framework , rgb representation model , design adaptive spatiotemporal attention module fulfill visual cue definition sign video , adapter design construct auxiliary task , jointly learn slr task enhance performance model . give signing video , spatiotemporal attention-based pseudo-3d residual network ( sta p3d resnet ) use learn spatiotemporal feature mainly area interest key frame . feature extraction , attention-based bidirectional long short-term memory network ( att-blstm ) utilized select significant motion . meanwhile , skeletal data , obtain texture image color encoding construct spatial relation feature , high level representation human posture . learnt skeleton-based feature skeletal data fuse attention-aware video feature provide informative spatiotemporal information slr . experiment carry two large scale sign language datasets . experimental result demonstrate effectiveness propose method ."
"human-computer interaction , head gesture play significant role improve smoothness naturality . however , exist head gesture recognition algorithm disadvantage accuracy generalization ability . deal problem , paper address two-stream dynamic head gesture recognition method slowfast pathway call 3dsfi ( 3d slowfast inception ) . slowfast pathway design reduce parameter computational cost . meanwhile , two-stream structure efficiently capture motion feature video dense optical flow . besides , inception block inceptionv3 expand 3d convolutional kernel space-time serve feature extractor . finally , 3dsfi apply robot pepper order evaluate realistic performance . experimental result show propose method high accuracy good generalization performance classical c3d ( convolutional 3d ) i3d ( inflated 3d convnet ) method ."
"advancement computer vision technology , learn use sign language communicate deaf mute people become easier . excite research ongoing provide global platform communication different sign language . paper , present deep learning base approach recognize sign perform american sign language capture image input . system predict sign 0 9 digit perform user . utilize image processing convert rgb data grayscale image , efficient reduction achieve storage requirement train time convolutional neural network . objective experiment find mix image processing deep learning architecture less complexity deploy system mobile application embed single board computer . database train scratch use small network lenet-5 alexnet well deep network vgg16 mobilenet v2 . comparison recognition accuracy discuss paper . final select architecture 10 layer include dropout layer boost training accuracy 91.37 % test accuracy 87.5 % ."
"work aim present novel computer vision approach development real-time , web-camera base , british sign language recognition system . literature review focus current ( 1 ) state sign language recognition system ( 2 ) technique use conduct . review process use foundation convolutional neural network ( cnn ) base system design implement . bespoke british sign language dataset contain 11,875 image - perform train test cnn use classification human hand perform gesture . finally , cnn architecture recognize 19 static british sign language gesture , incorporate single double-handed gesture . test , system achieve average recognition accuracy 89 % ."
"estimate hand-object mesh pose challenging computer vision problem many practical application . paper , introduce simple yet efficient hand-object reconstruction algorithm . end , exploit fact pose mesh graphs-based representation hand-object different level detail . allow take advantage powerful graph convolution network ( gcns ) build coarse-to-fine graph-based hand-object reconstruction algorithm . thus , start estimate coarse graph represent 2d hand-object pose . , detail ( e.g . third dimension mesh vertex ) gradually add graph represent dense 3d hand-object mesh . paper also explore problem represent rgbd input different modality ( e.g . voxelized rgbd ) . hence , adopt multi-modal representation input combine 3d representation ( i.e . voxelized rgbd ) 2d representation ( i.e . rgb ) . include intensive experimental evaluation measure ability simple algorithm achieve state-of-the-art accuracy challenging datasets ( i.e . ho-3d fphab ) ."
"development wearable camera new environment emerge , egocentric perspective , computer vision task detect hand disambiguate leave right . order address challenge , use attention network various egocentric hand property make final classification . hand feature inspire egocentric perspective include hand location image , hand size , fact one object hand class probability hand appear image . addition , use yolo object detector tiny version see impact overall performance speed , need wearable device . finally , compare current object hand detection approach ."
"common problem face deaf community people know sign language communication still issue . different brazil , sign language call libra . many technological solution propose gesture recognition video sequence less intrusive one . work , address problem recognize gesture libra video sequence . , first segment body part person video use deep neural network architecture , frame frame fashion . , employ gait energy image encode motion compact feature space . feature feed classifier attribute label video . experiment , achieve 53.90 +/- 5.48 % accuracy use k-nearest neighbor 79.57 +/- 239 % use random forest best configuration across 24 dataset class ."
speech impairment disability affect individual 's ability verbal communication . overcome issue sign language use one organised language . definitely need method application recognize sign language gesture communication possible even someone understand sign language . paper effort towards fill gap differently-abled people like deaf dumb people . image processing combine machine learning help form real-time system . image processing use pre-processing image extract different hand background . image obtain extract background use form data contain 24 alphabet english language . convolutional neural network propose test custom-made dataset also real-time hand gesture perform people different skin tone . accuracy obtain propose algorithm 83 % .
"article describe elaboration low-cost system allow user interaction vr environment model unity view google cardboard . proposed system divide processing client-server architecture vr experience execute client , send picture capture mobile device inside card board server . server process hand pose information send client ."
"paper consider three different classifier stack algorithm : simple stacking , cascade classifier ensemble nonlinear version classifier stack base classifier interaction . classifier interaction express use classifier prediction pairwise matrix ( cppm ) . meta-learner last algorithm convolutional neural network ( cnns ) two classifier stack algorithm ( simple classifier stack cascade classifier ensemble ) apply . allow apply classical stacking cascade-based recursive stacking euclidean riemannian geometry . cascade random forest ( rf ) extra tree ( ets ) consider forest-based alternative deep neural network goal compare accuracy cascade rf cnn-based stacking deep multi-layer perceptrons ( mlps ) different classification problem . use gesture phase dataset uci repository [ 2 ] compare analyze cascade rf extra tree ( ets ) geometry cnn-based version classifier stack . data set select generally motion consider nonlinear process ( pattern lie euclidean vector space ) computer vision application . thus assess good forest-based deep learning riemannian manifold ( r-manifolds ) apply nonlinear process . datasets uci repository use compare aforementioned algorithm well-known classifier stacking-based version geometry . experimental result show classifier stack algorithm riemannian geometry ( r-geometry ) less dependent property individual classifier ( e.g . depth decision tree rf ets ) comparison euclidean geometry . independent individual classifier allow obtain r-manifolds good property classification . generally , accuracy classification use classifier stack r-geometry high euclidean one ."
"human gesture recognition draw much attention area computer vision . however , performance gesture recognition always influence gesture-irrelevant factor like background clothes performer . therefore , focus region hand/arm important gesture recognition . meanwhile , adaptive architecture-searched network structure also perform good block-fixed one like resnet since increase diversity feature different stage network well . paper , propose regional attention architecture-rebuilt 3d network ( raar3dnet ) gesture recognition . replace fixed inception module automatically rebuilt structure network via neural architecture search ( nas ) , owe different shape representation ability feature early , middle , late stage network . enable network capture different level feature representation different layer adaptively . meanwhile , also design stackable regional attention module call dynamic-static attention ( dsa ) , derive gaussian guidance heatmap dynamic motion map highlight hand/arm region motion information spatial temporal domain , respectively . extensive experiment two recent large-scale rgb-d gesture datasets validate effectiveness propose method show outperform state-of-the-art method . code method available : http : //github.com/zhoubenjia/raar3dnet ."
"high-order statistic prove useful framework convolutional neural network ( cnn ) variety computer vision task . paper , propose exploit high-order statistic framework recurrent neural network ( rnn ) skeleton-based hand gesture recognition . method base statistical recurrent unit ( sru ) , un-gated architecture introduce alternative model long-short term memory ( lstm ) gate recurrent unit ( gru ) . sru capture sequential information generate recurrent statistic depend context previously see data compute move average different scale . integration high-order statistic sru significantly improve performance original one , result model competitive state-of-the-art method dynamic hand gesture ( dhg ) dataset , outperform first-person hand action ( fpha ) dataset ."
"deaf & mute people communicate usually sign language . although sign-language well know among people 's quite unknown people . attempt make project bridge communication gap people n't know sign language . sign language use american sign language lexicon video dataset project far extend different language . dataset video format dominant frame extraction algorithm would extract dominant image video use create as-per-requirement dataset . find lot work already finger-spellings recognition since finger-spellings make much complicated actual time think move ahead rather identify word . work complete computer vision 's state-of-the-art model base deep learning help achieve completion task instance level classification make recognition task good identification pixel-level in-turn help achieve good result . attempt make employ late method make computer learn sign language independent person 's complexion , light condition , orientation ."
"signboard important location landmark provide service local community . non-disabled people easily understand meaning signboard base special shape ; however , visually impaired people need assistive system guide destination help understand surroundings . currently , design accurate assistive system remain challenge . computer vision struggle recognize signboard due diverse design combine text image . moreover , lack datasets train best model reach good result . paper , propose novel framework automatically detect recognize signboard logo . addition , utilize google street view collect signboard image taiwan 's street . propose framework consist domain adaptation reduce loss function source-target datasets , also represent important source feature adopt target dataset . model , add nonlocal block attention mechanism call deep attention network achieve best final result . perform extensive experiment dataset public datasets demonstrate superior performance effectiveness propose method . experimental result show propose method outperform state-of-the-art method across evaluation metric ."
"sign language invent help deaf-mute people communicate ordinary people . paper propose novel method recognize vsl ( vietnamese sign language ) base combination gauss distribution correlation coefficient extract single dynamic object video , googlenet ( cnn model ) bil-stm ( bidirectional long short-term memory ) classify video sequence . used dataset include 2700 sample correspond 27 sign vsl . experimental result reach accuracy 9938 % validation set 98.15 % test set ."
"driver ' distraction one leading cause road accident . reduce driver distraction , real-time gesture recognition system ada aim simplify enhance interaction human computer implement vision-based technique allow driver interact vehicle infotainment system function use natural mid-air hand gesture . therefore , paper , propose track recognize human static hand gesture . system process separate five step include image acquisition , background subtraction , hand segmentation , feature extraction , gesture recognition . firstly , image frame capture resize . thereafter , region interest determine minimize required processing increase performance . next , foreground model extract convert hsv color space . , skin filter apply extract skin region . next step , image transform binary image thresholding smoothen apply morphological transformation . contour detection approximation implement . finally , hand feature extract build gesture recognition model hand center , palm radius , fingertip , defect point , hull area , hand area angle finger . experimental result show system able achieve 86.25 % recognition accuracy room environment 80 % recognition accuracy car environment . comparison , average classification accuracy 92.5 % 90 % , room car environment , respectively ."
"indispensable mean communication deaf people sign language . give familiarity lack hear people specific language practice deaf people , establish interpretation system make easy communication deaf people social environment give impression necessary . main challenge system identify sign continuous sign language video . therefore , work present computer vision base system recognize sign continuous sign language video . system base two main phase ; sign word extraction classification . challenging task process separate sign word video sequence . purpose , present new algorithm able detect accurate word boundary continuous sign language video . use hand shape motion feature , algorithm extract isolate sign video show well efficiency compare work present literature . recognition phase , extracted sign classify recognize use hidden markov model ( hmm ) strongly adopt test approach independent bayesian classifier combination ( ibcc ) . system manifest auspicious performance recognition accuracy 95.18 % one gesture 93.87 % two hand gesture . compare system use manual feature , propose framework reach 2.24 % 2.9 % progress one two hand gesture respectively , employ head pose eye gaze feature . result reach base dataset contain 33 isolated sign ."
"bound support gaussian mixture model ( bgmm ) propose data model alternative unbounded support mixture model case data lie bounded support . paper , propose application multivariate bgmm data cluster insightful analysis model . also propose minimum message length ( mml ) criterion model selection data cluster use multivariate bgmm . presented model apply data cluster several speech ( tsp spoken digit ) image database ( mnist fashion mnist ) . also propose application bgmm code-book generation feature extraction phase . inspire success bag visual word approach computer vision , also introduce speech data representation validate experiment present paper . validation model selection criterion , mml apply different medical , speech image datasets . experimental result obtain model selection mml compare seven different model selection criterion . result present paper demonstrate effectiveness bgmm cluster speech image database , code-book generation cluster feature representation model selection ."
"gesture recognition popular research field computer vision application deep neural network greatly improve performance . however , general deep learning method large number parameter prevent practical application resource-limited device . meanwhile , collect large number training sample usually time-consuming difficult . end , propose lightweight 3d inception-resnet extract discriminative feature real-time one-shot learning gesture recognition aim recognize gesture successfully give one training sample new class . efficient extraction gesture feature , firstly extend original 2d inception-resnet 3d version apply two kind separable convolution well design strategy reduce number parameter computation complexity make run real-time even cpu feature extraction . moreover , consumption storage space also greatly reduce . order obtain robust performance one-shot learning recognition , employ evolution mechanism update root sample innovation new sample enhance improve performance near neighbor classifier . meanwhile , propose update strategy dynamic threshold deal problem threshold selection real-world application . order improve robustness recognition performance , conduct artificial data synthesis augment collected dataset . series experiment conduct public datasets collected dataset demonstrate effectiveness approach one-shot learning gesture recognition ."
"despite recent advance 3-d pose estimation human hand , thanks advent convolutional neural network ( cnns ) depth camera , task still far solve uncontrolled setup . mainly due highly non-linear dynamic finger self-occlusions , make hand model train challenging task . study , novel hierarchical tree-like structured cnn exploit , branch train become specialised predefined subset hand joint call local pose . far , local pose feature , extract hierarchical cnn branch , fuse learn high order dependency among joint final pose end-to-end training . lastly , loss function use also define incorporate appearance physical constraint doable hand motion deformation . finally , non-rigid data augmentation approach introduce increase amount train depth data . experimental result suggest feed tree-shaped cnn , specialise local pose , fusion network model joint ' correlation dependency , help increase precision final estimation , show competitive result nyu , msra , hands17 synthetichand datasets ."
"monomr system synthesize pseudo-2.5d content monocular video mixed reality ( mr ) head-mounted display ( hmds ) . unlike conventional system require multiple camera , monomr system use casual end-users generate mr content single camera . order synthesize content , system detect people video sequence via deep neural network , detected person 's pseudo-3d position estimate propose novel algorithm homography matrix . finally , person 's texture extract use background subtraction algorithm place estimate 3d position . synthesized content play mr hmd , user freely change viewpoint content 's position . order evaluate efficiency interactive potential monomr , conduct performance evaluation user study 12 participant . moreover , demonstrate feasibility usability monomr system generate pseudo-2.5d content use three example application scenario ."
"surgeon must intraoperatively view cross-section image sterilization condition . keyboard computer mouse source contamination . computer vision algorithm hand movement pattern analysis technique apply solve problem base surgeon 's behavior . paper propose new method control radiological image viewer operating room . pattern code hand movement grid square guideline use . propose algorithm comprise three step : hand tracking , pattern code area identification , hand movement pattern recognition . first , system feed sequence three-dimensional data . 3d camera capture whole target body . skeleton track algorithm use detect human body . left-hand joint skeleton data set track . second , algorithm support one hand movement , grid square guideline define . hand movement interpret hand path move grid square area . finally , pattern code define feature vector . use feature vector close point classifier , hand movement recognize k-nearest neighbor algorithm . test performance propose algorithm , data twenty subject use . seven command use interface computer workstation control radiological image viewer . accuracy rate 95.72 % . repeatability 1.88. advantage method one hand control image viewer software distance 1.5 satisfactorily without contact computer device . method also need big data set train system ."
"efficient sign language recognition system ( slrs ) recognize gesture sign language ease communication signer non-signer community . paper , computer-vision base slrs use deep learning technique propose . study primary three contribution : first , large dataset indian sign language ( isl ) create use 65 different user uncontrolled environment . second , intra-class variance dataset increase use augmentation improve generalization ability propose work . three additional copy training image generate paper , use three different affine transformation . third , novel robust model use convolutional neural network ( cnn ) propose feature extraction classification isl gesture . performance method evaluate self-collected isl dataset publicly available dataset asl . total three datasets use achieved accuracy 92.43 , 88.01 , 99.52 % . efficiency method also evaluate term precision , recall , f-score , time consume system . result indicate propose method show encourage performance compare exist work ."
"hand pose estimation rgb image always difficult task , owe incompleteness depth information . moon et al . improve accuracy hand pose estimation use new network , internet , unique design . still , network still potential improvement . base architecture mobilenet v3 moga , redesign feature extractor introduce late achievement field computer vision , acon activation function new attention mechanism module , etc . use module effectively network , architecture better extract global feature rgb image hand , lead great performance improvement compare internet similar network ."
"sign language main type communication deaf community . however , people know language , cause communication problem many people . many technological solution propose overcome issue . use concept wearable device , gesture recognition video sequence cheap less intrusive solution . work , address problem gesture recognition video . , employ two-step method feature space mapping classification . first , body part subject video segment deep neural network architecture . , use gait energy image encode motion body part compact feature space . small datasets usually problem type application , lead sparse representation feature space . contour problem , evaluate smote data augmentation technique feature space classical dimensionality reduction technique . evaluate method three challenge brazilian sign language ( libras ) datasets , cefet/rj-libras , minds-libras , libras-ufop , achieve global accuracy 85.40 +/- 3.13 % , 84.66 +/- 1.78 % , 64.91 +/- 3.79 % , respectively , singular value decomposition support vector machine ."
"great extent , immersion virtual environment ( ) depend naturalness interface provide interaction . people commonly exploit gesture communication , therefore interaction base hand-postures enhances degree realism . however , choice select hand posture interaction varies person person . generalize use specific posture particular interaction require considerable computation turn depletes intuition 3d interface . investigate machine learning domain virtual reality ( vr ) , paper present open posture-based approach 3d interaction . technique user-independent rely neither size color hand distance camera posing-position . system work two phases-in first phase , hand-postures learnt , whereas second phase know posture use perform interaction . ordinary camera , scanned image partition equal size non-overlapping tile . four light-weight feature , base binary histogram invariant moment , calculate part portion posture-image . support vector machine classifier train posture-specific knowledge carry accumulatively tile . pose known posture , system extract tile information detect particular hand-posture . successful recognition , appropriate interaction activate design . proposed system implement case-study application ; vision-based open posture interaction use library opencv opengl . system assess three separate evaluation session . result evaluation testify efficacy approach various vr application ."
"full-body tracking virtual reality improve presence , allow interaction via body posture , facilitate well social expression among user . however , full-body track system today require complex setup fxed environment ( e.g . , multiple lighthouses/cameras ) laborious calibration process , go desire make vr system portable integrated . present hybridtrak , provide accurate , real-time full-body tracking augment inside-out1upper-body vr track system single external of-the-shelf rgb web camera . hybridtrak use full-neural solution convert transform user ' 2d full-body pose webcam 3d pose leverage inside-out upper-body tracking data . show hybridtrak accurate rgb depth-based track method mpi-inf-3dhp dataset . also test hybridtrak popular vrchat app show body posture present hybridtrak distinguishable natural solution use rgbd camera ."
